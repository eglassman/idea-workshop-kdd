{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                                   gensim LDA on papers for different K.ipynb  paper_titles_and_abstracts.csv              \u001b[34mvenv\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Paper titles', ''], ['', ''], ['Deep Learning from Temporal Coherence in Video', 'This work proposes a learning method for deep architectures that takes advantage of sequential data, in particular from the temporal coherence that naturally exists in unlabeled video recordings. That is, two successive frames are likely to contain the same object or objects. This coherence is used as a supervisory signal over the unlabeled data, and is used to improve the performance on a supervised task of interest. We demonstrate the effectiveness of this method on some pose invariant object and face recognition tasks'], ['Learning hierarchical invariant spatio-temporal features for action recognitionwith independent subspace analysis', 'Previous work on action recognition has focused on adapting hand-designed local features, such as SIFT or HOG, from static images to the video domain. In this paper, we propose using unsupervised feature learning as a way to learn features directly from video data. More specifically, we present an extension of the Independent Subspace Analysis algorithm to learn invariant spatio-temporal features from unlabeled video data. We discovered that, despite its simplicity, this method performs surprisingly well when combined with deep learning techniques such as stacking and convolution to learn hierarchical representations. By replacing hand-designed features with our learned features, we achieve classification results superior to all previous published results on the Hollywood2, UCF, KTH and YouTube action recognition datasets. On the challenging Hollywood2 and YouTube action datasets we obtain 53.3% and 75.8% respectively, which are approximately 5% better than the current best published results. Further benefits of this method, such as the ease of training and the efficiency of training and prediction, will also be discussed. You can download our code and learned spatio-temporal features here: http://ai.stanford.edu/\\x89\\xf6_wzou/'], ['Deep Learning of Invariant Spatio-Temporal Features from Video', 'We present a novel hierarchical, distributed model for unsupervised learning ofinvariant spatio-temporal features from video. Our approach builds on previousdeep learning methods and uses the convolutional Restricted Boltzmann machine(CRBM) as a basic processing unit. Our model, called the Space-Time Deep BeliefNetwork (ST-DBN), alternates the aggregation of spatial and temporal informationso that higher layers capture longer range statistical dependencies in both spaceand time. Our experiments show that the ST-DBN has superior performance ondiscriminative and generative tasks including action recognition and video denoisingwhen compared to convolutional deep belief networks (CDBNs) appliedon a per-frame basis. Simultaneously, the ST-DBN has superior feature invarianceproperties compared to CDBNs and can integrate information from both space andtime to fill in missing data in video.'], ['Sequential Deep Learningfor Human Action Recognition', 'We propose in this paper a fully automated deep model,which learns to classify human actions without using any prior knowledge.The first step of our scheme, based on the extension of ConvolutionalNeural Networks to 3D, automatically learns spatio-temporalfeatures. A Recurrent Neural Network is then trained to classify eachsequence considering the temporal evolution of the learned features foreach timestep. Experimental results on the KTH dataset show that theproposed approach outperforms existing deep models, and gives comparableresults with the best related works'], ['Convolutional Learningof Spatio-temporal Features', 'We address the problem of learning good features for understandingvideo data. We introduce a model that learns latent representationsof image sequences from pairs of successive images. The convolutionalarchitecture of our model allows it to scale to realistic image sizeswhilst using a compact parametrization. In experiments on the NORBdataset, we show our model extracts latent \\x89\\xdb\\xcfflow fields\\x89\\xdb\\x9d which correspondto the transformation between the pair of input frames. We also use ourmodel to extract low-level motion features in a multi-stage architecturefor action recognition, demonstrating competitive performance on boththe KTH and Hollywood2 datasets.'], ['Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation', 'Learning goal-directed behavior in environments with sparse feedback isa major challenge for reinforcement learning algorithms. The primarydifficulty arises due to insufficient exploration, resulting in an agent beingunable to learn robust value functions. Intrinsically motivated agents canexplore new behavior for its own sake rather than to directly solve problems.Such intrinsic behaviors could eventually help the agent solve tasks posed bythe environment. We present hierarchical-DQN (h-DQN), a framework tointegrate hierarchical value functions, operating at different temporal scales,with intrinsically motivated deep reinforcement learning. A top-level valuefunction learns a policy over intrinsic goals, and a lower-level function learnsa policy over atomic actions to satisfy the given goals. h-DQN allows forflexible goal specifications, such as functions over entities and relations. Thisprovides an efficient space for exploration in complicated environments. Wedemonstrate the strength of our approach on two problems with very sparse,delayed feedback: (1) a complex discrete MDP with stochastic transitions,and (2) the classic ATARI game \\x89\\xdb\\xf7Montezuma\\x89\\xdb\\xaas Revenge\\x89\\xdb\\xaa.'], ['Factored Temporal Sigmoid Belief Networks for Sequence Learning', 'Deep conditional generative models are developedto simultaneously learn the temporal dependenciesof multiple sequences. The model is designedby introducing a three-way weight tensorto capture the multiplicative interactions betweenside information and sequences. The proposedmodel builds on the Temporal Sigmoid BeliefNetwork (TSBN), a sequential stack of SigmoidBelief Networks (SBNs). The transition matricesare further factored to reduce the number of parametersand improve generalization. When sideinformation is not available, a general frameworkfor semi-supervised learning based on the proposedmodel is constituted, allowing robust sequenceclassification. Experimental results showthat the proposed approach achieves state-of-theartpredictive and classification performance onsequential data, and has the capacity to synthesizesequences, with controlled style transitioningand blending.']]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('paper_titles_and_abstracts.csv', 'rb') as f:\n",
    "    data = [row for row in csv.reader(f.read().splitlines())]\n",
    "print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paper titles', '']\n",
      "['', '']\n",
      "['Deep Learning from Temporal Coherence in Video', 'This work proposes a learning method for deep architectures that takes advantage of sequential data, in particular from the temporal coherence that naturally exists in unlabeled video recordings. That is, two successive frames are likely to contain the same object or objects. This coherence is used as a supervisory signal over the unlabeled data, and is used to improve the performance on a supervised task of interest. We demonstrate the effectiveness of this method on some pose invariant object and face recognition tasks']\n",
      "['Learning hierarchical invariant spatio-temporal features for action recognitionwith independent subspace analysis', 'Previous work on action recognition has focused on adapting hand-designed local features, such as SIFT or HOG, from static images to the video domain. In this paper, we propose using unsupervised feature learning as a way to learn features directly from video data. More specifically, we present an extension of the Independent Subspace Analysis algorithm to learn invariant spatio-temporal features from unlabeled video data. We discovered that, despite its simplicity, this method performs surprisingly well when combined with deep learning techniques such as stacking and convolution to learn hierarchical representations. By replacing hand-designed features with our learned features, we achieve classification results superior to all previous published results on the Hollywood2, UCF, KTH and YouTube action recognition datasets. On the challenging Hollywood2 and YouTube action datasets we obtain 53.3% and 75.8% respectively, which are approximately 5% better than the current best published results. Further benefits of this method, such as the ease of training and the efficiency of training and prediction, will also be discussed. You can download our code and learned spatio-temporal features here: http://ai.stanford.edu/\\x89\\xf6_wzou/']\n",
      "['Deep Learning of Invariant Spatio-Temporal Features from Video', 'We present a novel hierarchical, distributed model for unsupervised learning ofinvariant spatio-temporal features from video. Our approach builds on previousdeep learning methods and uses the convolutional Restricted Boltzmann machine(CRBM) as a basic processing unit. Our model, called the Space-Time Deep BeliefNetwork (ST-DBN), alternates the aggregation of spatial and temporal informationso that higher layers capture longer range statistical dependencies in both spaceand time. Our experiments show that the ST-DBN has superior performance ondiscriminative and generative tasks including action recognition and video denoisingwhen compared to convolutional deep belief networks (CDBNs) appliedon a per-frame basis. Simultaneously, the ST-DBN has superior feature invarianceproperties compared to CDBNs and can integrate information from both space andtime to fill in missing data in video.']\n",
      "['Sequential Deep Learningfor Human Action Recognition', 'We propose in this paper a fully automated deep model,which learns to classify human actions without using any prior knowledge.The first step of our scheme, based on the extension of ConvolutionalNeural Networks to 3D, automatically learns spatio-temporalfeatures. A Recurrent Neural Network is then trained to classify eachsequence considering the temporal evolution of the learned features foreach timestep. Experimental results on the KTH dataset show that theproposed approach outperforms existing deep models, and gives comparableresults with the best related works']\n",
      "['Convolutional Learningof Spatio-temporal Features', 'We address the problem of learning good features for understandingvideo data. We introduce a model that learns latent representationsof image sequences from pairs of successive images. The convolutionalarchitecture of our model allows it to scale to realistic image sizeswhilst using a compact parametrization. In experiments on the NORBdataset, we show our model extracts latent \\x89\\xdb\\xcfflow fields\\x89\\xdb\\x9d which correspondto the transformation between the pair of input frames. We also use ourmodel to extract low-level motion features in a multi-stage architecturefor action recognition, demonstrating competitive performance on boththe KTH and Hollywood2 datasets.']\n",
      "['Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation', 'Learning goal-directed behavior in environments with sparse feedback isa major challenge for reinforcement learning algorithms. The primarydifficulty arises due to insufficient exploration, resulting in an agent beingunable to learn robust value functions. Intrinsically motivated agents canexplore new behavior for its own sake rather than to directly solve problems.Such intrinsic behaviors could eventually help the agent solve tasks posed bythe environment. We present hierarchical-DQN (h-DQN), a framework tointegrate hierarchical value functions, operating at different temporal scales,with intrinsically motivated deep reinforcement learning. A top-level valuefunction learns a policy over intrinsic goals, and a lower-level function learnsa policy over atomic actions to satisfy the given goals. h-DQN allows forflexible goal specifications, such as functions over entities and relations. Thisprovides an efficient space for exploration in complicated environments. Wedemonstrate the strength of our approach on two problems with very sparse,delayed feedback: (1) a complex discrete MDP with stochastic transitions,and (2) the classic ATARI game \\x89\\xdb\\xf7Montezuma\\x89\\xdb\\xaas Revenge\\x89\\xdb\\xaa.']\n",
      "['Factored Temporal Sigmoid Belief Networks for Sequence Learning', 'Deep conditional generative models are developedto simultaneously learn the temporal dependenciesof multiple sequences. The model is designedby introducing a three-way weight tensorto capture the multiplicative interactions betweenside information and sequences. The proposedmodel builds on the Temporal Sigmoid BeliefNetwork (TSBN), a sequential stack of SigmoidBelief Networks (SBNs). The transition matricesare further factored to reduce the number of parametersand improve generalization. When sideinformation is not available, a general frameworkfor semi-supervised learning based on the proposedmodel is constituted, allowing robust sequenceclassification. Experimental results showthat the proposed approach achieves state-of-theartpredictive and classification performance onsequential data, and has the capacity to synthesizesequences, with controlled style transitioningand blending.']\n"
     ]
    }
   ],
   "source": [
    "for d in data:\n",
    "    print d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
