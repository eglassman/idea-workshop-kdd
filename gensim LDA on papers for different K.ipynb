{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('Papers.csv', 'rb') as f:\n",
    "    data = [row for row in csv.reader(f.read().splitlines())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'Title', 'EventType', 'PdfName', 'Abstract', 'PaperText']\n",
      "['5677', 'Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing', 'Poster', '5677-double-or-nothing-multiplicative-incentive-mechanisms-for-crowdsourcing.pdf', 'Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural no-free-lunch requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible  mechanisms (that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers.  Interestingly, this unique mechanism takes a multiplicative form. The simplicity of the mechanism is an added benefit.  In preliminary experiments involving over several hundred workers, we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure.', 'Double or Nothing: MultiplicativeIncentive Mechanisms for CrowdsourcingNihar B. ShahUniversity of California, Berkeleynihar@eecs.berkeley.eduDengyong ZhouMicrosoft Researchdengyong.zhou@microsoft.comAbstractCrowdsourcing has gained immense popularity in machine learning applicationsfor obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, butsuffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivizeworkers to answer only the questions that they are sure of and skip the rest. Weshow that surprisingly, under a mild and natural \\xe2\\x80\\x9cno-free-lunch\\xe2\\x80\\x9d requirement, thismechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible mechanisms(that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers. Interestingly, this unique mechanism takes a\\xe2\\x80\\x9cmultiplicative\\xe2\\x80\\x9d form. The simplicity of the mechanism is an added benefit. Inpreliminary experiments involving over several hundred workers, we observe asignificant reduction in the error rates under our unique mechanism for the sameor lower monetary expenditure.1IntroductionComplex machine learning tools such as deep learning are gaining increasing popularity and arebeing applied to a wide variety of problems. These tools, however, require large amounts of labeleddata [HDY+ 12, RYZ+ 10, DDS+ 09, CBW+ 10]. These large labeling tasks are being performed bycoordinating crowds of semi-skilled workers through the Internet. This is known as crowdsourcing.Crowdsourcing as a means of collecting labeled training data has now become indispensable to theengineering of intelligent systems.Most workers in crowdsourcing are not experts. As a consequence, labels obtained from crowdsourcing typically have a significant amount of error [KKKMF11, VdVE11, WLC+ 10]. Recentefforts have focused on developing statistical techniques to post-process the noisy labels in orderto improve its quality (e.g., [RYZ+ 10, ZLP+ 15, KOS11, IPSW14]). However, when the inputs tothese algorithms are erroneous, it is difficult to guarantee that the processed labels will be reliableenough for subsequent use by machine learning or other applications. In order to avoid \\xe2\\x80\\x9cgarbage in,garbage out\\xe2\\x80\\x9d, we take a complementary approach to this problem: cleaning the data at the time ofcollection.We consider crowdsourcing settings where the workers are paid for their services, such as in thepopular crowdsourcing platforms of Amazon Mechanical Turk and others. These commercial platforms have gained substantial popularity due to their support for a diverse range of tasks for machinelearning labeling, varying from image annotation and text recognition to speech captioning and machine translation. We consider problems that are objective in nature, that is, have a definite answer.Figure 1a depicts an example of such a question where the worker is shown a set of images, and foreach image, the worker is required to identify if the image depicts the Golden Gate Bridge.1\\x0cIs this the Golden Gate Bridge?Is this the Golden Gate Bridge?Yes!Yes!No!NoI\\xe2\\x80\\x99m not sure(b)!(a)!Figure 1: Different interfaces in a crowdsourcing setup: (a) the conventional interface, and (b) withan option to skip.Our approach builds on the simple insight that in typical crowdsourcing setups, workers are simplypaid in proportion to the amount of tasks they complete. As a result, workers attempt to answerquestions that they are not sure of, thereby increasing the error rate of the labels. For the questionsthat a worker is not sure of, her answers could be very unreliable [WLC+ 10, KKKMF11, VdVE11,JSV14]. To ensure acquisition of only high-quality labels, we wish to encourage the worker toskip the questions about which she is unsure, for instance, by providing an explicit \\xe2\\x80\\x9cI\\xe2\\x80\\x99m not sure\\xe2\\x80\\x9doption for every question (see Figure 1b). Our goal is to develop payment mechanisms to encouragethe worker to select this option when she is unsure. We will term any payment mechanism thatincentivizes the worker to do so as \\xe2\\x80\\x9cincentive compatible\\xe2\\x80\\x9d.In addition to incentive compatibility, preventing spammers is another desirable requirement fromincentive mechanisms in crowdsourcing. Spammers are workers who answer randomly withoutregard to the question being asked, in the hope of earning some free money, and are known to existin large numbers on crowdsourcing platforms [WLC+ 10, Boh11, KKKMF11, VdVE11]. It is thusof interest to deter spammers by paying them as low as possible. An intuitive objective, to this end,is to ensure a zero expenditure on spammers who answer randomly. In this paper, however, weimpose a strictly and significantly weaker condition, and then show that there is one and only oneincentive-compatible mechanism that can satisfy this weak condition. Our requirement, referred toas the \\xe2\\x80\\x9cno-free-lunch\\xe2\\x80\\x9d axiom, says that if all the questions attempted by the worker are answeredincorrectly, then the payment must be zero.We propose a payment mechanism for the aforementioned setting (\\xe2\\x80\\x9cincentive compatibility\\xe2\\x80\\x9d plus\\xe2\\x80\\x9cno-free-lunch\\xe2\\x80\\x9d), and show that surprisingly, this is the only possible mechanism. We also show thatadditionally, our mechanism makes the smallest possible payment to spammers among all possibleincentive compatible mechanisms that may or may not satisfy the no-free-lunch axiom. Our paymentmechanism takes a multiplicative form: the evaluation of the worker\\xe2\\x80\\x99s response to each question isa certain score, and the final payment is a product of these scores. This mechanism has additionalappealing features in that it is simple to compute, and is also simple to explain to the workers. Ourmechanism is applicable to any type of objective questions, including multiple choice annotationquestions, transcription tasks, etc.In order to test whether our mechanism is practical, and to assess the quality of the final labelsobtained, we conducted experiments on the Amazon Mechanical Turk crowdsourcing platform. Inour preliminary experiments that involved over several hundred workers, we found that the qualityof data improved by two-fold under our unique mechanism, with the total monetary expenditurebeing the same or lower as compared to the conventional baseline.2Problem SettingIn the crowdsourcing setting that we consider, one or more workers perform a task, where a taskconsists of multiple questions. The questions are objective, by which we mean, each question hasprecisely one correct answer. Examples of objective questions include multiple-choice classificationquestions such as Figure 1, questions on transcribing text from audio or images, etc.For any possible answer to any question, we define the worker\\xe2\\x80\\x99s confidence about an answer as theprobability, according to her belief, of this answer being correct. In other words, one can assumethat the worker has (in her mind) a probability distribution over all possible answers to a question,and the confidence for an answer is the probability of that answer being correct. As a shorthand, wealso define the confidence about a question as the confidence for the answer that the worker is most2\\x0cconfident about for that question. We assume that the worker\\xe2\\x80\\x99s confidences for different questionsare independent. Our goal is that for every question, the worker should be incentivized to:1. skip if the confidence is below a certain pre-defined threshold, otherwise:2. select the answer that she thinks is most confident about.More formally, let T 2 (0, 1) be a predefined value. The goal is to design payment mechanisms thatincentivize the worker to skip the questions for which her confidence is lower than T , and attemptthose for which her confidence is higher than T . 1 Moreover, for the questions that she attempts toanswer, she must be incentivized to select the answer that she believes is most likely to be correct.The threshold T may be chosen based on various factors of the problem at hand, for example, onthe downstream machine learning algorithms using the crowdsourced data, or the knowledge of thestatistics of worker abilities, etc. In this paper we assume that the threshold T is given to us.Let N denote the total number of questions in the task. Among these, we assume the existence ofsome \\xe2\\x80\\x9cgold standard\\xe2\\x80\\x9d questions, that is, a set of questions whose answers are known to the requester.Let G (1 \\xef\\xa3\\xbf G \\xef\\xa3\\xbf N ) denote the number of gold standard questions. The G gold standard questionsare assumed to be distributed uniformly at random in the pool of N questions (of course, the workerdoes not know which G of the N questions form the gold standard). The payment to a worker fora task is computed after receiving her responses to all the questions in the task. The payment isbased on the worker\\xe2\\x80\\x99s performance on the gold standard questions. Since the payment is based onknown answers, the payments to different workers do not depend on each other, thereby allowing usto consider the presence of only one worker without any loss in generality.We will employ the following standard notation. For any positive integer K, the set {1, . . . , K} isdenoted by [K]. The indicator function is denoted by 1, i.e., 1{z} = 1 if z is true, and 0 otherwise.The notation R+ denotes the set of all non-negative real numbers.Let x1 , . . . , xG 2 { 1, 0, +1} denote the evaluations of the answers that the worker gives to the Ggold standard questions. Here, \\xe2\\x80\\x9c0\\xe2\\x80\\x9d denotes that the worker skipped the question, \\xe2\\x80\\x9c 1\\xe2\\x80\\x9d denotes thatthe worker attempted to answer the question and that answer was incorrect, and \\xe2\\x80\\x9c+1\\xe2\\x80\\x9d denotes thatthe worker attempted to answer the question and that answer was correct. Let f : { 1, 0, +1}G !R+ denote the payment function, namely, a function that determines the payment to the workerbased on these evaluations x1 , . . . , xG . Note that the crowdsourcing platforms of today mandate thepayments to be non-negative. We will let \\xc2\\xb5 (> 0) denote the budget, i.e., the maximum amount thatcan be paid to any individual worker for this task:max f (x1 , . . . , xG ) = \\xc2\\xb5.x1 ,...,xGThe amount \\xc2\\xb5 is thus the amount of compensation paid to a perfect agent for her work. We willassume this budget condition of \\xc2\\xb5 throughout the rest of the paper.We assume that the worker attempts to maximize her overall expected payment. In what follows, theexpression \\xe2\\x80\\x98the worker\\xe2\\x80\\x99s expected payment\\xe2\\x80\\x99 will refer to the expected payment from the worker\\xe2\\x80\\x99spoint of view, and the expectation will be taken with respect to the worker\\xe2\\x80\\x99s confidences about heranswers and the uniformly random choice of the G gold standard questions among the N questionsin the task. For any question i 2 [N ], let yi = 1 if the worker attempts question i, and set yi = 0otherwise. Further, for every question i 2 [N ] such that yi 6= 0, let pi be the confidence of theworker for the answer she has selected for question i, and for every question i 2 [N ] such thatyi = 0, let pi 2 (0, 1) be any arbitrary value. Let E = (\\xe2\\x9c\\x8f1 , . . . , \\xe2\\x9c\\x8fG ) 2 { 1, 1}G . Then from theworker\\xe2\\x80\\x99s perspective, the expected payment for the selected answers and confidence-levels is!GXXY1+\\xe2\\x9c\\x8fi1 \\xe2\\x9c\\x8fi1f (\\xe2\\x9c\\x8f1 yj1 , . . . , \\xe2\\x9c\\x8fG yjG ) (pji ) 2 (1 pji ) 2.NGi=1(j1 ,...,jG ) E2{ 1,1}G\\xe2\\x9c\\x93{1,...,N }In the expression above, the outermost summation corresponds to the expectation with respect to therandomness arising from the unknown choice of the gold standard questions. The inner summationcorresponds to the expectation with respect to the worker\\xe2\\x80\\x99s beliefs about the correctness of herresponses.1In the event that the confidence about a question is exactly equal to T , the worker may be equally incentivized to answer or skip.3\\x0cWe will call any payment function f as an incentive-compatible mechanism if the expected paymentof the worker under this payment function is strictly maximized when the worker responds in themanner desired.23Main results: Incentive-compatible mechanism and guaranteesIn this section, we present the main results of the paper, namely, the design of incentive-compatiblemechanisms with practically useful properties. To this end, we impose the following natural requirement on the payment function f that is motivated by the practical considerations of budgetconstraints and discouraging spammers and miscreants [Boh11, KKKMF11, VdVE11, WLC+ 10].We term this requirement as the \\xe2\\x80\\x9cno-free-lunch axiom\\xe2\\x80\\x9d:Axiom 1 (No-free-lunch axiom). If all the answers attempted by the worker in the gold standard arewrong, then the payment is zero. More formally, for every set of evaluations (x1 , . . . , xG ) that satisfyPGPG0 < i=1 1{xi 6= 0} = i=1 1{xi = 1}, we require the payment to satisfy f (x1 , . . . , xG ) = 0.Observe that no-free-lunch is an extremely mild requirement. In fact, it is significantly weaker thanimposing a zero payment on workers who answer randomly. For instance, if the questions are ofbinary-choice format, then randomly choosing among the two options for each question would resultin 50% of the answers being correct in expectation, while the no-free-lunch axiom is applicable onlywhen none of them turns out to be correct.3.1Proposed \\xe2\\x80\\x9cMultiplicative\\xe2\\x80\\x9d MechanismWe now present our proposed payment mechanism in Algorithm 1.Algorithm 1 \\xe2\\x80\\x9cMultiplicative\\xe2\\x80\\x9d incentive-compatible mechanism\\xe2\\x80\\xa2 Inputs: Threshold T , Budget \\xc2\\xb5, Evaluations (x1 , . . . , xG ) 2 { 1, 0, +1}G of the worker\\xe2\\x80\\x99s answers to the G gold standard questionsPGPG\\xe2\\x80\\xa2 Let C = i=1 1{xi = 1} and W = i=1 1{xi = 1}\\xe2\\x80\\xa2 The payment isf (x1 , . . . , xG ) = \\xc2\\xb5T GC1{W = 0}.The proposed mechanism has a multiplicative form: each answer in the gold standard is given ascore based on whether it was correct (score = T1 ), incorrect (score = 0) or skipped (score = 1),and the final payment is simply a product of these scores (scaled by \\xc2\\xb5). The mechanism is easy todescribe to workers: For instance, if T = 12 , G = 3 and \\xc2\\xb5 = 80 cents, then the description reads:\\xe2\\x80\\x9cThe reward starts at 10 cents. For every correct answer in the 3 gold standard questions,the reward will double. However, if any of these questions are answered incorrectly, thenthe reward will become zero. So please use the \\xe2\\x80\\x98I\\xe2\\x80\\x99m not sure\\xe2\\x80\\x99 option wisely.\\xe2\\x80\\x9dObserve how this payment rule is similar to the popular \\xe2\\x80\\x98double or nothing\\xe2\\x80\\x99 paradigm [Dou14].The algorithm makes a zero payment if one or more attempted answers in the gold standard arewrong. Note that this property is significantly stronger than the property of no-free-lunch whichwe originally required, where we wanted a zero payment only when all attempted answers werewrong. Surprisingly, as we prove shortly, Algorithm 1 is the only incentive-compatible mechanismthat satisfies no-free-lunch.The following theorem shows that the proposed payment mechanism indeed incentivizes a workerto skip the questions for which her confidence is below T , while answering those for which herconfidence is greater than T . In the latter case, the worker is incentivized to select the answer whichshe thinks is most likely to be correct.Theorem 1. The payment mechanism of Algorithm 1 is incentive-compatible and satisfies the nofree-lunch condition.2Such a payment function that is based on gold standard questions is also called a \\xe2\\x80\\x9cstrictly proper scoringrule\\xe2\\x80\\x9d [GR07].4\\x0cThe proof of Theorem 1 is presented in Appendix A. It is easy to see that the mechanism satisfies nofree-lunch. The proof of incentive compatibility is also not hard: We consider any arbitrary worker(with arbitrary belief distributions), and compute the expected payment for that worker for the casewhen her choices in the task follow the requirements. We then show that any other choice leads to astrictly smaller expected payment.While we started out with a very weak condition of no-free-lunch of making a zero payment whenall attempted answers are wrong, the mechanism proposed in Algorithm 1 is significantly morestrict and makes a zero payment when any of the attempted answers is wrong. A natural questionthat arises is: can we design an alternative mechanism satisfying incentive compatibility and nofree-lunch that operates somewhere in between?3.2Uniqueness of the MechanismIn the previous section we showed that our proposed multiplicative mechanism is incentive compatible and satisfies the intuitive requirement of no-free-lunch. It turns out, perhaps surprisingly, thatthis mechanism is unique in this respect.Theorem 2. The payment mechanism of Algorithm 1 is the only incentive-compatible mechanismthat satisfies the no-free-lunch condition.Theorem 2 gives a strong result despite imposing very weak requirements. To see this, recall our earlier discussion on deterring spammers, that is, incurring a low expenditure on workers who answerrandomly. For instance, when the task comprises binary-choice questions, one may wish to designmechanisms which make a zero payment when the responses to 50% or more of the questions in thegold standard are incorrect. The no-free-lunch axiom is a much weaker requirement, and the onlymechanism that can satisfy this requirement is the mechanism of Algorithm 1.The proof of Theorem 2 is available in Appendix B. The proof relies on the following key lemmathat establishes a condition that any incentive-compatible mechanism must necessarily satisfy. Thelemma applies to any incentive-compatible mechanism and not just to those satisfying no-free-lunch.Lemma. Any incentive-compatible payment mechanism f must satisfy, for every i 2 {1, . . . , G}and every (y1 , . . . , yi 1 , yi+1 , . . . , yG ) 2 { 1, 0, 1}G 1 ,T f (y1 , . . . , yi1 , 1, yi+1 , . . . , yG )+ (1T )f (y1 , . . . , yi 1 , 1, yi+1 , . . . , yG )= f (y1 , . . . , yi 1 , 0, yi+1 , . . . , yG ).The proof of this lemma is provided in Appendix C. Given this lemma, the proof of Theorem 2 isthen completed via an induction on the number of skipped questions.3.3Optimality against Spamming BehaviorAs discussed earlier, crowdsouring tasks, especially those with multiple choice questions, oftenencounter spammers who answer randomly without heed to the question being asked. For instance,under a binary-choice setup, a spammer will choose one of the two options uniformly at random forevery question. A highly desirable objective in crowdsourcing settings is to deter spammers. To thisend, one may wish to impose a condition of zero payment when the responses to 50% or more ofthe attempted questions in the gold standard are incorrect. A second desirable metric could be tominimize the expenditure on a worker who simply skips all questions. While the aforementionedrequirements were deterministic functions of the worker\\xe2\\x80\\x99s responses, one may alternatively wish toimpose requirements that depend on the distribution of the worker\\xe2\\x80\\x99s answering process. For instance,a third desirable feature would be to minimize the expected payment to a worker who answers allquestions uniformly at random. We now show that interestingly, our unique multiplicative paymentmechanism simultaneously satisfies all these requirements. The result is stated assuming a multiplechoice setup, but extends trivially to non-multiple-choice settings.Theorem 3.A (Distributional). Consider any value A 2 {0, . . . , G}. Among all incentivecompatible mechanisms (that may or may not satisfy no-free-lunch), Algorithm 1 strictly minimizesthe expenditure on a worker who skips some A of the questions in the the gold standard, and choosesanswers to the remaining (G A) questions uniformly at random.5\\x0cTheorem 3.B (Deterministic). Consider any value B 2 (0, 1]. Among all incentive-compatiblemechanisms (that may or may not satisfy no-free-lunch), Algorithm 1 strictly minimizes the expenditure on a worker who gives incorrect answers to a fraction B or more of the questions attemptedin the gold standard.The proof of Theorem 3 is presented in Appendix D. We see from this result that the multiplicativepayment mechanism of Algorithm 1 thus possesses very useful properties geared to deter spammers,while ensuring that a good worker will be paid a high enough amount.To illustrate this point, let us compare the mechanism of Algorithm 1 with the popular additive classof payment mechanisms.Example 1. Consider the popular class of \\xe2\\x80\\x9cadditive\\xe2\\x80\\x9d mechanisms, where the payments to a workerare added across the gold standard questions. This additive payment mechanism offers a reward of\\xc2\\xb5\\xc2\\xb5TG for every correct answer in the gold standard, G for every question skipped, and 0 for everyincorrect answer. Importantly, the final payment to the worker is the sum of the rewards across theG gold standard questions. One can verify that this additive mechanism is incentive compatible.One can also see that that as guaranteed by our theory, this additive payment mechanism does notsatisfy the no-free-lunch axiom.Suppose each question involves choosing from two options. Let us compute the expenditure thatthese two mechanisms make under a spamming behavior of choosing the answer randomly to eachquestion. Given the 50% likelihood of each question being correct, on can compute that the additivemechanism makes a payment of \\xc2\\xb52 in expectation. On the other hand, our mechanism pays anexpected amount of only \\xc2\\xb52 G . The payment to spammers thus reduces exponentially with thenumber of gold standard questions under our mechanism, whereas it does not reduce at all in theadditive mechanism.Now, consider a different means of exploiting the mechanism(s) where the worker simply skips allquestions. To this end, observe that if a worker skips all the questions then the additive paymentmechanism will incur an expenditure of \\xc2\\xb5T . On the other hand, the proposed payment mechanismof Algorithm 1 pays an exponentially smaller amount of \\xc2\\xb5T G (recall that T < 1).4Simulations and ExperimentsIn this section, we present synthetic simulations and real-world experiments to evaluate the effectsof our setting and our mechanism on the final label quality.4.1Synthetic SimulationsWe employ synthetic simulations to understand the effects of various kinds of labeling errors incrowdsourcing. We consider binary-choice questions in this set of simulations. Whenever a workeranswers a question, her confidence for the correct answer is drawn from a distribution P independentof all else. We investigate the effects of the following five choices of the distribution P:\\xe2\\x80\\xa2\\xe2\\x80\\xa2\\xe2\\x80\\xa2\\xe2\\x80\\xa2\\xe2\\x80\\xa2The uniform distribution on the support [0.5, 1].A triangular distribution with lower end-point 0.2, upper end-point 1 and a mode of 0.6.A beta distribution with parameter values \\xe2\\x86\\xb5 = 5 and = 1.The hammer-spammer distribution [KOS11], that is, uniform on the discrete set {0.5, 1}.A truncated Gaussian distribution: a truncation of N (0.75, 0.5) to the interval [0, 1].When a worker has a confidence p (drawn from the distribution P) and attempts the question, theprobability of making an error equals (1 p).We compare (a) the setting where workers attempt every question, with (b) the setting where workersskip questions for which their confidence is below a certain threshold T . In this set of simulations,we set T = 0.75. In either setting, we aggregate the labels obtained from the workers for eachquestion via a majority vote on the two classes. Ties are broken by choosing one of the two optionsuniformly at random.6\\x0cFigure 2: Error under different interfaces for synthetic simulations of five distributions of the workers\\xe2\\x80\\x99 error probabilities.Figure 2 depicts the results from these simulations. Each bar represents the fraction of questions thatare labeled incorrectly, and is an average across 50,000 trials. (The standard error of the mean is toosmall to be visible.) We see that the skip-based setting consistently outperforms the conventionalsetting, and the gains obtained are moderate to high depending on the underlying distribution of theworkers\\xe2\\x80\\x99 errors. In particular, the gains are quite striking under the hammer-spammer model: thisresult is not surprising since the mechanism (ideally) screens the spammers out and leaves only thehammers who answer perfectly.4.2Experiments on Amazon Mechanical TurkWe conducted preliminary experiments on the Amazon Mechanical Turk commercial crowdsourcingplatform (mturk.com) to evaluate our proposed scheme in real-world scenarios. The completedata, including the interface presented to the workers in each of the tasks, the results obtained fromthe workers, and the ground truth solutions, are available on the website of the first author.Goal. Before delving into details, we first note certain caveats relating to such a study of mechanism design on crowdsourcing platforms. When a worker encounters a mechanism for only asmall amount of time (a handful of tasks in typical research experiments) and for a small amount ofmoney (at most a few dollars in typical crowdsourcing tasks), we cannot expect the worker to completely understand the mechanism and act precisely as required. For instance, we wouldn\\xe2\\x80\\x99t expectour experimental results to change significantly even upon moderate modifications in the promisedamounts, and furthermore, we do expect the outcomes to be noisy. Incentive compatibility kicksin when the worker encounters a mechanism across a longer term, for example, when a proposedmechanism is adopted as a standard for a platform, or when higher amounts are involved. This iswhen we would expect workers or others (e.g., bloggers or researchers) to design strategies that cangame the mechanism. The theoretical guarantee of incentive compatibility or strict properness thenprevents such gaming in the long run.We thus regard these experiments as preliminary. Our intentions towards this experimental exercisewere (a) to evaluate the potential of our algorithms to work in practice, and (b) to investigate theeffect of the proposed algorithms on the net error in the collected labelled data.Experimental setup. We conducted the five following experiments (\\xe2\\x80\\x9ctasks\\xe2\\x80\\x9d) on Amazon Mechanical Turk: (a) identifying the golden gate bridge from pictures, (b) identifying the breeds of dogsfrom pictures, (c) identifying heads of countries, (d) identifying continents to which flags belong,and (e) identifying the textures in displayed images. Each of these tasks comprised 20 to 126 multi7\\x0cFigure 3: Error under different interfaces and mechanisms for five experiments conducted on Mechanical Turk.ple choice questions.3 For each experiment, we compared (i) a baseline setting (Figure 1a) with anadditive payment mechanism that pays a fixed amount per correct answer, and (ii) our skip-basedsetting (Figure 1b) with the multiplicative mechanism of Algorithm 1. For each experiment, and foreach of the two settings, we had 35 workers independently perform the task.Upon completion of the tasks on Amazon Mechanical Turk, we aggregated the data in the followingmanner. For each mechanism in each experiment, we subsampled 3, 5, 7, 9 and 11 workers, andtook a majority vote of their responses. We averaged the accuracy across all questions and across1, 000 iterations of this subsample-and-aggregate procedure.Results. Figure 3 reports the error in the aggregate data in the five experiments. We see that inmost cases, our skip-based setting results in a higher quality data, and in many of the instances, thereduction is two-fold or higher. All in all, in the experiments, we observed a substantial reduction inthe amount of error in the labelled data while expending the same or lower amounts and receivingno negative comments from the workers. These observations suggest that our proposed skip-basedsetting coupled with our multiplicative payment mechanisms have potential to work in practice; theunderlying fundamental theory ensures that the system cannot be gamed in the long run.5Discussion and ConclusionsIn an extended version of this paper [SZ14], we generalize the \\xe2\\x80\\x9cskip-based\\xe2\\x80\\x9d setting considered hereto one where we also elicit the workers\\xe2\\x80\\x99 confidence about their answers. Moreover, in a companionpaper [SZP15], we construct mechanisms to elicit the support of worker\\xe2\\x80\\x99s beliefs.Our mechanism offers some additional benefits. The pattern of skips of the workers provide a reasonable estimate of the difficulty of each question. In practice, the questions that are estimated tobe more difficult may now be delegated to an expert or to additional non-expert workers. Secondly,the theoretical guarantees of our mechanism may allow for better post-processing of the data, incorporating the confidence information and improving the overall accuracy. Developing statisticalaggregation algorithms or augmenting existing ones (e.g., [RYZ+ 10, KOS11, LPI12, ZLP+ 15]) forthis purpose is a useful direction of research. Thirdly, the simplicity of our mechanisms may facilitate an easier adoption among the workers. In conclusion, given the uniqueness and optimalityin theory, simplicity, and good performance observed in practice, we envisage our multiplicativepayment mechanisms to be of interest to practitioners as well as researchers who employ crowdsourcing.3See the extended version of this paper [SZ14] for additional experiments involving free-form responses,such as text transcription.8\\x0cReferences[Boh11]John Bohannon. Social science for pennies. Science, 334(6054):307\\xe2\\x80\\x93307, 2011.[CBW 10]Andrew Carlson, Justin Betteridge, Richard C Wang, Estevam R Hruschka Jr, andTom M Mitchell. Coupled semi-supervised learning for information extraction. InACM WSDM, pages 101\\xe2\\x80\\x93110, 2010.[DDS+ 09]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: Alarge-scale hierarchical image database. In IEEE Conference on Computer Vision andPattern Recognition, pages 248\\xe2\\x80\\x93255, 2009.[Dou14]Double or Nothing. http://wikipedia.org/wiki/Double_or_nothing,2014. Last accessed: July 31, 2014.[GR07]Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, andestimation. Journal of the American Statistical Association, 102(477):359\\xe2\\x80\\x93378, 2007.[HDY+ 12]Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed,Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath,et al. Deep neural networks for acoustic modeling in speech recognition: The sharedviews of four research groups. IEEE Signal Processing Magazine, 29(6):82\\xe2\\x80\\x9397, 2012.Panagiotis G Ipeirotis, Foster Provost, Victor S Sheng, and Jing Wang. Repeatedlabeling using multiple noisy labelers. Data Mining and Knowledge Discovery,28(2):402\\xe2\\x80\\x93441, 2014.+[IPSW14][JSV14]Srikanth Jagabathula, Lakshminarayanan Subramanian, and Ashwin Venkataraman.Reputation-based worker filtering in crowdsourcing. In Advances in Neural Information Processing Systems 27, pages 2492\\xe2\\x80\\x932500, 2014.[KKKMF11] Gabriella Kazai, Jaap Kamps, Marijn Koolen, and Natasa Milic-Frayling. Crowdsourcing for book search evaluation: impact of HIT design on comparative systemranking. In ACM SIGIR, pages 205\\xe2\\x80\\x93214, 2011.[KOS11]David R Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliablecrowdsourcing systems. In Advances in neural information processing systems, pages1953\\xe2\\x80\\x931961, 2011.[LPI12]Qiang Liu, Jian Peng, and Alexander T Ihler. Variational inference for crowdsourcing.In NIPS, pages 701\\xe2\\x80\\x93709, 2012.[RYZ+ 10]Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, CharlesFlorin, Luca Bogoni, and Linda Moy. Learning from crowds. The Journal of MachineLearning Research, 11:1297\\xe2\\x80\\x931322, 2010.[SZ14]Nihar B Shah and Dengyong Zhou. Double or nothing: Multiplicative incentivemechanisms for crowdsourcing. arXiv:1408.1387, 2014.[SZP15]Nihar B Shah, Dengyong Zhou, and Yuval Peres. Approval voting and incentives incrowdsourcing. In International Conference on Machine Learning (ICML), 2015.[VdVE11]Jeroen Vuurens, Arjen P de Vries, and Carsten Eickhoff. How much spam can youtake? An analysis of crowdsourcing results to increase accuracy. In ACM SIGIRWorkshop on Crowdsourcing for Information Retrieval, pages 21\\xe2\\x80\\x9326, 2011.[WLC+ 10]Paul Wais, Shivaram Lingamneni, Duncan Cook, Jason Fennell, Benjamin Goldenberg, Daniel Lubarov, David Marin, and Hari Simons. Towards building a highquality workforce with Mechanical Turk. NIPS workshop on computational socialscience and the wisdom of crowds, 2010.[ZLP+ 15]Dengyong Zhou, Qiang Liu, John C Platt, Christopher Meek, and Nihar BShah. Regularized minimax conditional entropy for crowdsourcing. arXiv preprintarXiv:1503.07240, 2015.9\\x0c']\n",
      "['5941', 'Learning with Symmetric Label Noise: The Importance of Being Unhinged', 'Spotlight', '5941-learning-with-symmetric-label-noise-the-importance-of-being-unhinged.pdf', 'Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2008] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2008] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged loss\\xe2\\x80\\x99 SLN-robustness.', 'Learning with Symmetric Label Noise: TheImportance of Being UnhingedBrendan van Rooyen\\xe2\\x88\\x97,\\xe2\\x80\\xa0\\xe2\\x88\\x97Aditya Krishna Menon\\xe2\\x80\\xa0,\\xe2\\x88\\x97The Australian National University\\xe2\\x80\\xa0Robert C. Williamson\\xe2\\x88\\x97,\\xe2\\x80\\xa0National ICT Australia{ brendan.vanrooyen, aditya.menon, bob.williamson }@nicta.com.auAbstractConvex potential minimisation is the de facto approach to binary classification.However, Long and Servedio [2010] proved that under symmetric label noise(SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensiblyshows that convex losses are not SLN-robust. In this paper, we propose a convex,classification-calibrated loss and prove that it is SLN-robust. The loss avoids theLong and Servedio [2010] result by virtue of being negatively unbounded. Theloss is a modification of the hinge loss, where one does not clamp at zero; hence,we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for anyconvex potential; this implies that strong `2 regularisation makes most standardlearners SLN-robust. Experiments confirm the unhinged loss\\xe2\\x80\\x99 SLN-robustness isborne out in practice. So, with apologies to Wilde [1895], while the truth is rarelypure, it can be simple.1Learning with symmetric label noiseBinary classification is the canonical supervised learning problem. Given an instance space X, andsamples from some distribution D over X \\xc3\\x97 {\\xc2\\xb11}, the goal is to learn a scorer s : X \\xe2\\x86\\x92 R with lowmisclassification error on future samples drawn from D. Our interest is in the more realistic scenariowhere the learner observes samples from some corruption D of D, where labels have some constantprobability of being flipped, and the goal is still to perform well with respect to D. This problem isknown as learning from symmetric label noise (SLN learning) [Angluin and Laird, 1988].Long and Servedio [2010] showed that there exist linearly separable D where, when the learnerobserves some corruption D with symmetric label noise of any nonzero rate, minimisation of anyconvex potential over a linear function class results in classification performance on D that is equivalent to random guessing. Ostensibly, this establishes that convex losses are not \\xe2\\x80\\x9cSLN-robust\\xe2\\x80\\x9d andmotivates the use of non-convex losses [Stempfel and Ralaivola, 2009, Masnadi-Shirazi et al., 2010,Ding and Vishwanathan, 2010, Denchev et al., 2012, Manwani and Sastry, 2013].In this paper, we propose a convex loss and prove that it is SLN-robust. The loss avoids the resultof Long and Servedio [2010] by virtue of being negatively unbounded. The loss is a modification of the hinge loss where one does not clamp at zero; thus, we call it the unhinged loss. Thisloss has several appealing properties, such as being the unique convex loss satisfying a notion of\\xe2\\x80\\x9cstrong\\xe2\\x80\\x9d SLN-robustness (Proposition 5), being classification-calibrated (Proposition 6), consistentwhen minimised on D (Proposition 7), and having an simple optimal solution that is the differenceof two kernel means (Equation 8). Finally, we show that this optimal solution is equivalent to that ofa strongly regularised SVM (Proposition 8), and any twice-differentiable convex potential (Proposition 9), implying that strong `2 regularisation endows most standard learners with SLN-robustness.1\\x0cThe classifier resulting from minimising the unhinged loss is not new [Devroye et al., 1996, Chapter 10], [Scho\\xcc\\x88lkopf and Smola, 2002, Section 1.2], [Shawe-Taylor and Cristianini, 2004, Section5.1]. However, establishing this classifier\\xe2\\x80\\x99s (strong) SLN-robustness, uniqueness thereof, and itsequivalence to a highly regularised SVM solution, to our knowledge is novel.2Background and problem setupFix an instance space X. We denote by D a distribution over X \\xc3\\x97 {\\xc2\\xb11}, with random variables(X, Y) \\xe2\\x88\\xbc D. Any D may be expressed via the class-conditionals (P, Q) = (P(X | Y = 1), P(X |Y = \\xe2\\x88\\x921)) and base rate \\xcf\\x80 = P(Y = 1), or via the marginal M = P(X) and class-probabilityfunction \\xce\\xb7 : x 7\\xe2\\x86\\x92 P(Y = 1 | X = x). We interchangeably write D as DP,Q,\\xcf\\x80 or DM,\\xce\\xb7 .2.1Classifiers, scorers, and risksA scorer is any function s : X \\xe2\\x86\\x92 R. A loss is any function ` : {\\xc2\\xb11} \\xc3\\x97 R \\xe2\\x86\\x92 R. We use `\\xe2\\x88\\x921 , `1 torefer to `(\\xe2\\x88\\x921, \\xc2\\xb7) and `(1, \\xc2\\xb7). The `-conditional risk L` : [0, 1] \\xc3\\x97 R \\xe2\\x86\\x92 R is defined as L` : (\\xce\\xb7, v) 7\\xe2\\x86\\x92\\xce\\xb7 \\xc2\\xb7 `1 (v) + (1 \\xe2\\x88\\x92 \\xce\\xb7) \\xc2\\xb7 `\\xe2\\x88\\x921 (v). Given a distribution D, the `-risk of a scorer s is defined as.LD` (s) =[`(Y, s(X))] ,E(X,Y)\\xe2\\x88\\xbcD(1)Dso that LD` (s) = E [L` (\\xce\\xb7(X), s(X))]. For a set S, L` (S) is the set of `-risks for all scorers in S.X\\xe2\\x88\\xbcMA function class is any F \\xe2\\x8a\\x86 RX . Given some F, the set of restricted Bayes-optimal scorers for aloss ` are those scorers in F that minimise the `-risk:.SD,F,\\xe2\\x88\\x97= Argmin LD` (s).`s\\xe2\\x88\\x88FThe set of (unrestricted) Bayes-optimal scorers is SD,\\xe2\\x88\\x97= SD,F,\\xe2\\x88\\x97for F = RX . The restricted```-regret of a scorer is its excess risk over that of any restricted Bayes-optimal scorer:.DregretD,F(s) = LD` (s) \\xe2\\x88\\x92 inf L` (t).`t\\xe2\\x88\\x88FBinary classification is concerned with the zero-one loss, `01 : (y, v) 7\\xe2\\x86\\x92 Jyv < 0K + 21 Jv = 0K.A loss ` is classification-calibrated if all its Bayes-optimal scorers are also optimal for zero-oneloss: (\\xe2\\x88\\x80D) SD,\\xe2\\x88\\x97\\xe2\\x8a\\x86 SD,\\xe2\\x88\\x9701 . A convex potential is any loss ` : (y, v) 7\\xe2\\x86\\x92 \\xcf\\x86(yv), where \\xcf\\x86 : R \\xe2\\x86\\x92 R+ is`convex, non-increasing, differentiable with \\xcf\\x860 (0) < 0, and \\xcf\\x86(+\\xe2\\x88\\x9e) = 0 [Long and Servedio, 2010,Definition 1]. All convex potentials are classification-calibrated [Bartlett et al., 2006, Theorem 2.1].2.2Learning with symmetric label noise (SLN learning)The problem of learning with symmetric label noise (SLN learning) is the following [Angluin andLaird, 1988, Kearns, 1998, Blum and Mitchell, 1998, Natarajan et al., 2013]. For some notional\\xe2\\x80\\x9cclean\\xe2\\x80\\x9d distribution D, which we would like to observe, we instead observe samples from somecorrupted distribution SLN(D, \\xcf\\x81), for some \\xcf\\x81 \\xe2\\x88\\x88 [0, 1/2). The distribution SLN(D, \\xcf\\x81) is such thatthe marginal distribution of instances is unchanged, but each label is independently flipped withprobability \\xcf\\x81. The goal is to learn a scorer from these corrupted samples such that LD01 (s) is small.For any quantity in D, we denote its corrupted counterparts in SLN(D, \\xcf\\x81) with a bar, e.g. M forthe corrupted marginal distribution, and \\xce\\xb7 for the corrupted class-probability function; additionally,when \\xcf\\x81 is clear from context, we will occasionally refer to SLN(D, \\xcf\\x81) by D. It is easy to check thatthe corrupted marginal distribution M = M , and [Natarajan et al., 2013, Lemma 7](\\xe2\\x88\\x80x \\xe2\\x88\\x88 X) \\xce\\xb7(x) = (1 \\xe2\\x88\\x92 2\\xcf\\x81) \\xc2\\xb7 \\xce\\xb7(x) + \\xcf\\x81.3(2)SLN-robustness: formalisationWe consider learners (`, F) for a loss ` and a function class F, with learning being the search forsome s \\xe2\\x88\\x88 F that minimises the `-risk. Informally, (`, F) is \\xe2\\x80\\x9crobust\\xe2\\x80\\x9d to symmetric label noise (SLNrobust) if minimising ` over F gives the same classifier on both the clean distribution D, which2\\x0cthe learner would like to observe, and SLN(D, \\xcf\\x81) for any \\xcf\\x81 \\xe2\\x88\\x88 [0, 1/2), which the learner actuallyobserves. We now formalise this notion, and review what is known about SLN-robust learners.3.1SLN-robust learners: a formal definitionFor some fixed instance space X, let \\xe2\\x88\\x86 denote the set of distributions on X \\xc3\\x97 {\\xc2\\xb11}. Given a notional\\xe2\\x80\\x9cclean\\xe2\\x80\\x9d distribution D, Nsln : \\xe2\\x88\\x86 \\xe2\\x86\\x92 2\\xe2\\x88\\x86 returns the set of possible corrupted versions of D the learnermay observe, where labels are flipped with unknown probability \\xcf\\x81:\\x1a\\x14\\x13\\x1b1Nsln : D 7\\xe2\\x86\\x92 SLN(D, \\xcf\\x81) | \\xcf\\x81 \\xe2\\x88\\x88 0,.2Equipped with this, we define our notion of SLN-robustness.Definition 1 (SLN-robustness). We say that a learner (`, F) is SLN-robust ifD,F,\\xe2\\x88\\x97D,F,\\xe2\\x88\\x97(\\xe2\\x88\\x80D \\xe2\\x88\\x88 \\xe2\\x88\\x86) (\\xe2\\x88\\x80D \\xe2\\x88\\x88 Nsln (D)) LD) = LD).01 (S`01 (S`(3)That is, SLN-robustness requires that for any level of label noise in the observed distribution D, theclassification performance (wrt D) of the learner is the same as if the learner directly observes D.Unfortunately, a widely adopted class of learners is not SLN-robust, as we will now see.3.2Convex potentials with linear function classes are not SLN-robustFix X = Rd , and consider learners with a convex potential `, and a function class of linear scorersFlin = {x 7\\xe2\\x86\\x92 hw, xi | w \\xe2\\x88\\x88 Rd }.This captures e.g. the linear SVM and logistic regression, which are widely studied in theory andapplied in practice. Disappointingly, these learners are not SLN-robust: Long and Servedio [2010,Theorem 2] give an example where, when learning under symmetric label noise, for any convexpotential `, the corrupted `-risk minimiser over Flin has classification performance equivalent torandom guessing on D. This implies that (`, Flin ) is not SLN-robust1 as per Definition 1.Proposition 1 (Long and Servedio [2010, Theorem 2]). Let X = Rd for any d \\xe2\\x89\\xa5 2. Pick any convexpotential `. Then, (`, Flin ) is not SLN-robust.3.3The fallout: what learners are SLN-robust?In light of Proposition 1, there are two ways to proceed in order to obtain SLN-robust learners: eitherwe change the class of losses `, or we change the function class F.The first approach has been pursued in a large body of work that embraces non-convex losses[Stempfel and Ralaivola, 2009, Masnadi-Shirazi et al., 2010, Ding and Vishwanathan, 2010,Denchev et al., 2012, Manwani and Sastry, 2013]. While such losses avoid the conditions of Proposition 1, this does not automatically imply that they are SLN-robust when used with Flin . In AppendixB, we present evidence that some of these losses are in fact not SLN-robust when used with Flin .The second approach is to consider suitably rich F that contains the Bayes-optimal scorer for D,e.g. by employing a universal kernel. With this choice, one can still use a convex potential loss, andin fact, owing to Equation 2, any classification-calibrated loss.Proposition 2. Pick any classification-calibrated `. Then, (`, RX ) is SLN-robust.Both approaches have drawbacks. The first approach has a computational penalty, as it requiresoptimising a non-convex loss. The second approach has a statistical penalty, as estimation rateswith a rich F will require a larger sample size. Thus, it appears that SLN-robustness involves acomputational-statistical tradeoff. However, there is a variant of the first option: pick a loss that isconvex, but not a convex potential. Such a loss would afford the computational and statistical advantages of minimising convex risks with linear scorers. Manwani and Sastry [2013] demonstratedthat square loss, `(y, v) = (1 \\xe2\\x88\\x92 yv)2 , is one such loss. We will show that there is a simpler loss thatis convex and SLN-robust, but is not in the class of convex potentials by virtue of being negativelyunbounded. To derive this loss, we first re-interpret robustness via a noise-correction procedure.1Even if we were content with a difference of \\x0f \\xe2\\x88\\x88 [0, 1/2] between the clean and corrupted minimisers\\xe2\\x80\\x99performance, Long and Servedio [2010, Theorem 2] implies that in the worst case \\x0f = 1/2.3\\x0c4A noise-corrected loss perspective on SLN-robustnessWe now re-express SLN-robustness to reason about optimal scorers on the same distribution, butwith two different losses. This will help characterise a set of \\xe2\\x80\\x9cstrongly SLN-robust\\xe2\\x80\\x9d losses.4.1Reformulating SLN-robustness via noise-corrected lossesGiven any \\xcf\\x81 \\xe2\\x88\\x88 [0, 1/2), Natarajan et al. [2013, Lemma 1] showed how to associate with a loss ` aDnoise-corrected counterpart ` such that LD` (s) = L` (s). The loss ` is defined as follows.Definition 2 (Noise-corrected loss). Given any loss ` and \\xcf\\x81 \\xe2\\x88\\x88 [0, 1/2), the noise-corrected loss ` is(\\xe2\\x88\\x80y \\xe2\\x88\\x88 {\\xc2\\xb11}) (\\xe2\\x88\\x80v \\xe2\\x88\\x88 R) `(y, v) =(1 \\xe2\\x88\\x92 \\xcf\\x81) \\xc2\\xb7 `(y, v) \\xe2\\x88\\x92 \\xcf\\x81 \\xc2\\xb7 `(\\xe2\\x88\\x92y, v).1 \\xe2\\x88\\x92 2\\xcf\\x81(4)Since ` depends on the unknown parameter \\xcf\\x81, it is not directly usable to design an SLN-robustlearner. Nonetheless, it is a useful theoretical device, since, by construction, for any F, SD,F,\\xe2\\x88\\x97=`SD,F,\\xe2\\x88\\x97= SD,F,\\xe2\\x88\\x97. This means that a sufficient condition for (`, F) to be SLN-robust is for SD,F,\\xe2\\x88\\x97.```Ghosh et al. [2015, Theorem 1] proved a sufficient condition on ` such that this holds, namely,(\\xe2\\x88\\x83C \\xe2\\x88\\x88 R)(\\xe2\\x88\\x80v \\xe2\\x88\\x88 R) `1 (v) + `\\xe2\\x88\\x921 (v) = C.(5)Interestingly, Equation 5 is necessary for a stronger notion of robustness, which we now explore.4.2Characterising a stronger notion of SLN-robustnessAs the first step towards a stronger notion of robustness, we rewrite (with a slight abuse of notation)LD` (s) =E(X,Y)\\xe2\\x88\\xbcD[`(Y, s(X))] =E(Y,S)\\xe2\\x88\\xbcR(D,s).[`(Y, S)] = L` (R(D, s)),where R(D, s) is a distribution over labels and scores. Standard SLN-robustness requires that labelnoise does not change the `-risk minimisers, i.e. that if s is such that L` (R(D, s)) \\xe2\\x89\\xa4 L` (R(D, s0 ))for all s0 , the same relation holds with D in place of D. Strong SLN-robustness strengthens thisnotion by requiring that label noise does not affect the ordering of all pairs of joint distributions overlabels and scores. (This of course trivially implies SLN-robustness.) As with the definition of D,given a distribution R over labels and scores, let R be the corresponding distribution where labelsare flipped with probability \\xcf\\x81. Strong SLN-robustness can then be made precise as follows.Definition 3 (Strong SLN-robustness). Call a loss ` strongly SLN-robust if for every \\xcf\\x81 \\xe2\\x88\\x88 [0, 1/2),(\\xe2\\x88\\x80R, R0 ) L` (R) \\xe2\\x89\\xa4 L` (R0 ) \\xe2\\x87\\x90\\xe2\\x87\\x92 L` (R) \\xe2\\x89\\xa4 L` (R0 ).We now re-express strong SLN-robustness using a notion of order equivalence of loss pairs, whichsimply requires that two losses order all distributions over labels and scores identically.\\xcb\\x9c order equivalent ifDefinition 4 (Order equivalent loss pairs). Call a pair of losses (`, `)(\\xe2\\x88\\x80R, R0 ) L` (R) \\xe2\\x89\\xa4 L` (R0 ) \\xe2\\x87\\x90\\xe2\\x87\\x92 L`\\xcb\\x9c(R) \\xe2\\x89\\xa4 L`\\xcb\\x9c(R0 ).Clearly, order equivalence of (`, `) implies SD,F,\\xe2\\x88\\x97= SD,F,\\xe2\\x88\\x97, which in turn implies SLN-robustness.``It is thus not surprising that we can relate order equivalence to strong SLN-robustness of `.Proposition 3. A loss ` is strongly SLN-robust iff for every \\xcf\\x81 \\xe2\\x88\\x88 [0, 1/2), (`, `) are order equivalent.This connection now lets us exploit a classical result in decision theory about order equivalent lossesbeing affine transformations of each other. Combined with the definition of `, this lets us concludethat the sufficient condition of Equation 5 is also necessary for strong SLN-robustness of `.Proposition 4. A loss ` is strongly SLN-robust if and only if it satisfies Equation 5.We now return to our original goal, which was to find a convex ` that is SLN-robust for Flin (andideally more general function classes). The above suggests that to do so, it is reasonable to considerthose losses that satisfy Equation 5. Unfortunately, it is evident that if ` is convex, non-constant, andbounded below by zero, then it cannot possibly be admissible in this sense. But we now show thatremoving the boundedness restriction allows for the existence of a convex admissible loss.4\\x0c5The unhinged loss: a convex, strongly SLN-robust lossConsider the following simple, but non-standard convex loss:unh`unh1 (v) = 1 \\xe2\\x88\\x92 v and `\\xe2\\x88\\x921 (v) = 1 + v.Compared to the hinge loss, the loss does not clamp at zero, i.e. it does not have a hinge. (Thus, peculiarly, it is negatively unbounded, an issue we discuss in \\xc2\\xa75.3.) Thus, we call this the unhinged loss2 .The loss has a number of attractive properties, the most immediate being is its SLN-robustness.5.1The unhinged loss is strongly SLN-robustunhunhSince `unhis strongly SLN-robust, and thus that1 (v) + `\\xe2\\x88\\x921 (v) = 2, Proposition 4 implies that `unh(` , F) is SLN-robust for any F. Further, the following uniqueness property is not hard to show.Proposition 5. Pick any convex loss `. Then,(\\xe2\\x88\\x83C \\xe2\\x88\\x88 R) `1 (v) + `\\xe2\\x88\\x921 (v) = C \\xe2\\x87\\x90\\xe2\\x87\\x92 (\\xe2\\x88\\x83A, B, D \\xe2\\x88\\x88 R) `1 (v) = \\xe2\\x88\\x92A \\xc2\\xb7 v + B, `\\xe2\\x88\\x921 (v) = A \\xc2\\xb7 v + D.That is, up to scaling and translation, `unh is the only convex loss that is strongly SLN-robust.Returning to the case of linear scorers, the above implies that (`unh , Flin ) is SLN-robust. This doesnot contradict Proposition 1, since `unh is not a convex potential as it is negatively unbounded. Intuitively, this property allows the loss to offset the penalty incurred by instances that are misclassifiedwith high margin by awarding a \\xe2\\x80\\x9cgain\\xe2\\x80\\x9d for instances that correctly classified with high margin.5.2The unhinged loss is classification calibratedSLN-robustness is by itself insufficient for a learner to be useful. For example, a loss that is uniformly zero is strongly SLN-robust, but is useless as it is not classification-calibrated. Fortunately,the unhinged loss is classification-calibrated, as we now establish. For technical reasons (see \\xc2\\xa75.3),we operate with FB = [\\xe2\\x88\\x92B, +B]X , the set of scorers with range bounded by B \\xe2\\x88\\x88 [0, \\xe2\\x88\\x9e).Proposition 6. Fix ` = `unh . For any DM,\\xce\\xb7 , B \\xe2\\x88\\x88 [0, \\xe2\\x88\\x9e), S`D,FB ,\\xe2\\x88\\x97 = {x 7\\xe2\\x86\\x92 B \\xc2\\xb7 sign(2\\xce\\xb7(x) \\xe2\\x88\\x92 1)}.Thus, for every B \\xe2\\x88\\x88 [0, \\xe2\\x88\\x9e), the restricted Bayes-optimal scorer over FB has the same sign as theBayes-optimal classifier for 0-1 loss. In the limiting case where F = RX , the optimal scorer isattainable if we operate over the extended reals R \\xe2\\x88\\xaa {\\xc2\\xb1\\xe2\\x88\\x9e}, so that `unh is classification-calibrated.5.3Enforcing boundedness of the lossWhile the classification-calibration of `unh is encouraging, Proposition 6 implies that its (unrestricted) Bayes-risk is \\xe2\\x88\\x92\\xe2\\x88\\x9e. Thus, the regret of every non-optimal scorer s is identically +\\xe2\\x88\\x9e, whichhampers analysis of consistency. In orthodox decision theory, analogous theoretical issues arisewhen attempting to establish basic theorems with unbounded losses [Ferguson, 1967, pg. 78].We can side-step this issue by restricting attention to bounded scorers, so that `unh is effectivelybounded. By Proposition 6, this does not affect the classification-calibration of the loss. In the context of linear scorers, boundedness of scorers can be achieved by regularisation:instead of work\\xe2\\x88\\x9aing with Flin , one can instead use Flin,\\xce\\xbb = {x 7\\xe2\\x86\\x92 hw, xi | ||w||2 \\xe2\\x89\\xa4 1/ \\xce\\xbb}, where \\xce\\xbb > 0, sothat Flin,\\xce\\xbb \\xe2\\x8a\\x86 FR/\\xe2\\x88\\x9a\\xce\\xbb for R = supx\\xe2\\x88\\x88X ||x||2 . Observe that as (`unh , F) is SLN-robust for any F,(`unh , Flin,\\xce\\xbb ) is SLN-robust for any \\xce\\xbb > 0. As we shall see in \\xc2\\xa76.3, working with Flin,\\xce\\xbb also lets usestablish SLN-robustness of the hinge loss when \\xce\\xbb is large.5.4Unhinged loss minimisation on corrupted distribution is consistentUsing bounded scorers makes it possible to establish a surrogate regret bound for the unhinged loss.This shows classification consistency of unhinged loss minimisation on the corrupted distribution.2This loss has been considered in Sriperumbudur et al. [2009], Reid and Williamson [2011] in the contextof maximum mean discrepancy; see the Appendix. The analysis of its SLN-robustness is to our knowledgenovel.5\\x0cProposition 7. Fix ` = `unh . Then, for any D, \\xcf\\x81 \\xe2\\x88\\x88 [0, 1/2), B \\xe2\\x88\\x88 [1, \\xe2\\x88\\x9e), and scorer s \\xe2\\x88\\x88 FB ,1D,FBregretD(s) =\\xc2\\xb7 regret`D,FB (s).01 (s) \\xe2\\x89\\xa4 regret`1 \\xe2\\x88\\x92 2\\xcf\\x81Standard rates of convergence via generalisation bounds are also trivial to derive; see the Appendix.6Learning with the unhinged loss and kernelsWe now show that the optimal solution for the unhinged loss when employing regularisation andkernelised scorers has a simple form. This sheds further light on SLN-robustness and regularisation.6.1The centroid classifier optimises the unhinged lossConsider minimising the unhingedrisk over the class of kernelised scorers FH,\\xce\\xbb = {s : x 7\\xe2\\x86\\x92\\xe2\\x88\\x9ahw, \\xce\\xa6(x)iH | ||w||H \\xe2\\x89\\xa4 1/ \\xce\\xbb} for some \\xce\\xbb > 0, where \\xce\\xa6 : X \\xe2\\x86\\x92 H is a feature mapping into areproducing kernel Hilbert space H with kernel k. Equivalently, given a distribution3 D, we want\\xce\\xbb\\xe2\\x88\\x97wunh,\\xce\\xbb= argmin E [1 \\xe2\\x88\\x92 Y \\xc2\\xb7 hw, \\xce\\xa6(X)i] + hw, wiH .(6)2(X,Y)\\xe2\\x88\\xbcDw\\xe2\\x88\\x88HThe first-order optimality condition implies that1\\xe2\\x88\\x97(7)wunh,\\xce\\xbb= \\xc2\\xb7 E [Y \\xc2\\xb7 \\xce\\xa6(X)] ,\\xce\\xbb (X,Y)\\xe2\\x88\\xbcDwhich is the kernel mean map of D [Smola et al., 2007], and thus the optimal unhinged scorer is\\x12\\x1311s\\xe2\\x88\\x97unh,\\xce\\xbb : x 7\\xe2\\x86\\x92 \\xc2\\xb7 E [Y \\xc2\\xb7 k(X, x)] = x 7\\xe2\\x86\\x92 \\xc2\\xb7 \\xcf\\x80 \\xc2\\xb7 E [k(X, x)] \\xe2\\x88\\x92 (1 \\xe2\\x88\\x92 \\xcf\\x80) \\xc2\\xb7 E [k(X, x)] .X\\xe2\\x88\\xbcPX\\xe2\\x88\\xbcQ\\xce\\xbb (X,Y)\\xe2\\x88\\xbcD\\xce\\xbb(8)From Equation 8, the unhinged solution is equivalent to a nearest centroid classifier [Manning et al.,2008, pg. 181] [Tibshirani et al., 2002] [Shawe-Taylor and Cristianini, 2004, Section 5.1]. Equation8 gives a simple way to understand the SLN-robustness of (`unh , FH,\\xce\\xbb ), as the optimal scorers onthe clean and corrupted distributions only differ by a scaling (see the Appendix):\\x03\\x021\\xc2\\xb7 EY \\xc2\\xb7 k(X, x) .(9)(\\xe2\\x88\\x80x \\xe2\\x88\\x88 X) E [Y \\xc2\\xb7 k(X, x)] =1 \\xe2\\x88\\x92 2\\xcf\\x81 (X,Y)\\xe2\\x88\\xbcD(X,Y)\\xe2\\x88\\xbcDInterestingly, Servedio [1999, Theorem 4] established that a nearest centroid classifier (which theytermed \\xe2\\x80\\x9cAVERAGE \\xe2\\x80\\x9d) is robust to a general class of label noise, but required the assumption thatM is uniform over the unit sphere. Our result establishes that SLN robustness of the classifierholds without any assumptions on M . In fact, Ghosh et al. [2015, Theorem 1] lets one quantify theunhinged loss\\xe2\\x80\\x99 performance under a more general noise model; see the Appendix for discussion.6.2Practical considerationsWe note several points relating to practical usage of the unhinged loss with kernelised scorers. First,cross-validation is not required to select \\xce\\xbb, since changing \\xce\\xbb only changes the magnitude of scores,not their sign. Thus, for the purposes of classification, one can simply use \\xce\\xbb = 1.Second, we can easily extend the scorers to use a bias regularised with strength 0 < \\xce\\xbbb 6= \\xce\\xbb. Tuning\\xce\\xbbb is equivalent to computing s\\xe2\\x88\\x97unh,\\xce\\xbb as per Equation 8, and tuning a threshold on a holdout set.\\xe2\\x88\\x97Third, when H = Rd for d small, we can store wunh,\\xce\\xbbexplicitly, and use this to make predictions.For high (or infinite) dimensional H, we can either make predictions directly via Equation 8, oruse random Fourier features [Rahimi and Recht, 2007] to (approximately) embed H into some low\\xe2\\x88\\x97dimensional Rd , and then store wunh,\\xce\\xbbas usual. (The latter requires a translation-invariant kernel.)\\xe2\\x88\\x97We now show that under some assumptions, wunh,\\xce\\xbbcoincides with the solution of two establishedmethods; the Appendix discusses some further relationships, e.g. to the maximum mean discrepancy.3Given a training sample S \\xe2\\x88\\xbc Dn , we can use plugin estimates as appropriate.6\\x0c6.3Equivalence to a highly regularised SVM and other convex potentialsThere is an interesting equivalence between the unhinged solution and that of a highly regularisedSVM. This has been noted in e.g. Hastie et al. [2004, Section 6], which showed how SVMs approacha nearest centroid classifier, which is of course the optimal unhinged solution.Proposition 8. Pick any D and \\xce\\xa6 : X \\xe2\\x86\\x92 H with R = supx\\xe2\\x88\\x88X ||\\xce\\xa6(x)||H < \\xe2\\x88\\x9e. For any \\xce\\xbb > 0, let\\xe2\\x88\\x97whinge,\\xce\\xbb= argminw\\xe2\\x88\\x88HE(X,Y)\\xe2\\x88\\xbcD[max(0, 1 \\xe2\\x88\\x92 Y \\xc2\\xb7 hw, \\xce\\xa6(x)iH )] +\\xce\\xbbhw, wiH2\\xe2\\x88\\x97\\xe2\\x88\\x97be the soft-margin SVM solution. Then, if \\xce\\xbb \\xe2\\x89\\xa5 R2 , whinge,\\xce\\xbb= wunh,\\xce\\xbb.Since (`unh , FH,\\xce\\xbb ) is SLN-robust, it follows that for `hinge : (y, v) 7\\xe2\\x86\\x92 max(0, 1\\xe2\\x88\\x92yv), (`hinge , FH,\\xce\\xbb )is similarly SLN-robust provided \\xce\\xbb is sufficiently large. That is, strong `2 regularisation (and abounded feature map) endows the hinge loss with SLN-robustness4 . Proposition 8 can be generalised\\xe2\\x88\\x97to show that wunh,\\xce\\xbbis the limiting solution of any twice differentiable convex potential. This showsthat strong `2 regularisation endows most learners with SLN-robustness. Intuitively, with strongregularisation, one only considers the behaviour of a loss near zero; since a convex potential \\xcf\\x86 has\\xcf\\x860 (0) < 0, it will behave similarly to its linear approximation around zero, viz. the unhinged loss.Proposition 9. Pick any D, bounded feature mapping \\xce\\xa6 : X \\xe2\\x86\\x92 H, and twice differentiable convex\\xe2\\x88\\x97potential \\xcf\\x86 with \\xcf\\x8600 ([\\xe2\\x88\\x921, 1]) bounded. Let w\\xcf\\x86,\\xce\\xbbbe the minimiser of the regularised \\xcf\\x86 risk. Then,\\x0c\\x0c\\x0c\\x0c2\\xe2\\x88\\x97\\x0c\\x0c w \\xe2\\x88\\x97\\x0c\\x0cwunh,\\xce\\xbb\\x0c\\x0c\\x0c\\x0c\\xcf\\x86,\\xce\\xbblim \\x0c\\x0c \\xe2\\x88\\x97\\xe2\\x88\\x92\\x0c\\x0c = 0.\\xe2\\x88\\x97\\xce\\xbb\\xe2\\x86\\x92\\xe2\\x88\\x9e \\x0c\\x0c ||w\\xcf\\x86,\\xce\\xbb ||H||wunh,\\xce\\xbb||H \\x0c\\x0cH6.4Equivalence to Fisher Linear Discriminant with whitened dataFor binary classification on DM,\\xce\\xb7 , the Fisher Linear Discriminant (FLD) finds a weight vector proportional to the minimiser of square loss `sq : (y, v) 7\\xe2\\x86\\x92 (1 \\xe2\\x88\\x92 yv)2 [Bishop, 2006, Section 4.1.5],\\xe2\\x88\\x97wsq,\\xce\\xbb= (EX\\xe2\\x88\\xbcM [XXT ] + \\xce\\xbbI)\\xe2\\x88\\x921 \\xc2\\xb7 E(X,Y)\\xe2\\x88\\xbcD [Y \\xc2\\xb7 X].(10)\\xe2\\x88\\x97wsq,\\xce\\xbbis only changed by a scalingBy Equation 9, and the fact that the corrupted marginal M = M ,factor under label noise. This provides an alternate proof of the fact that (`sq , Flin ) is SLN-robust5\\xe2\\x88\\x97[Manwani and Sastry, 2013, Theorem 2]. Clearly, the unhinged loss solution wunh,\\xce\\xbbis equivalent to\\x02 T\\x03\\xe2\\x88\\x97the FLD and square loss solution wsq,\\xce\\xbb when the input data is whitened i.e. E XX = I. WithX\\xe2\\x88\\xbcMa well-specified F, e.g. with a universal kernel, both the unhinged and square loss asymptoticallyrecover the optimal classifier, but the unhinged loss does not require a matrix inversion. With amisspecified F, one cannot in general argue for the superiority of the unhinged loss over square loss,or vice-versa, as there is no universally good surrogate to the 0-1 loss [Reid and Williamson, 2010,Appendix A]; the Appendix illustrate examples where both losses may underperform.7SLN-robustness of unhinged loss: empirical illustrationWe now illustrate that the unhinged loss\\xe2\\x80\\x99 SLN-robustness is empirically manifest. We reiteratethat with high regularisation, the unhinged solution is equivalent to an SVM (and in the limit anyclassification-calibrated loss) solution. Thus, we do not aim to assert that the unhinged loss is\\xe2\\x80\\x9cbetter\\xe2\\x80\\x9d than other losses, but rather, to demonstrate that its SLN-robustness is not purely theoretical.We first show that the unhinged risk minimiser performs well on the example of Longand Servedio [2010] (henceforth LS10). Figure 1 shows the distribution D, where X ={(1, 0), (\\xce\\xb3, 5\\xce\\xb3), (\\xce\\xb3, \\xe2\\x88\\x92\\xce\\xb3)} \\xe2\\x8a\\x82 R2 , with marginal distribution M = { 14 , 14 , 12 } and all three instancesare deterministically positive. We pick \\xce\\xb3 = 1/2. The unhinged minimiser perfectly classifies allthree points, regardless of the level of label noise (Figure 1). The hinge minimiser is perfect whenthere is no noise, but with even a small amount of noise, achieves a 50% error rate.45Long and Servedio [2010, Section 6] show that `1 regularisation does not endow SLN-robustness.Square loss escapes the result of Long and Servedio [2010] since it is not monotone decreasing.7\\x0c1UnhingedHinge 0% noiseHinge 1% noise0.50.5\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x811\\xe2\\x88\\x920.5======00.10.20.30.40.49Hinget-logisticUnhinged0.00 \\xc2\\xb1 0.000.15 \\xc2\\xb1 0.270.21 \\xc2\\xb1 0.300.38 \\xc2\\xb1 0.370.42 \\xc2\\xb1 0.360.47 \\xc2\\xb1 0.380.00 \\xc2\\xb1 0.000.00 \\xc2\\xb1 0.000.00 \\xc2\\xb1 0.000.22 \\xc2\\xb1 0.080.22 \\xc2\\xb1 0.080.39 \\xc2\\xb1 0.230.00 \\xc2\\xb1 0.000.00 \\xc2\\xb1 0.000.00 \\xc2\\xb1 0.000.00 \\xc2\\xb1 0.000.00 \\xc2\\xb1 0.000.34 \\xc2\\xb1 0.48Table 1: Mean and standard deviation of the 01 error over 125 trials on LS10. Grayed cellsdenote the best performer at that noise rate.\\xe2\\x88\\x921Figure 1: LS10 dataset.We next consider empirical risk minimisers from a random training sample: we construct a trainingset of 800 instances, injected with varying levels of label noise, and evaluate classification performance on a test set of 1000 instances. We compare the hinge, t-logistic (for t = 2) [Ding andVishwanathan, 2010] and unhinged minimisers using a linear scorer without a bias term, and regularisation strength \\xce\\xbb = 10\\xe2\\x88\\x9216 . From Table 1, even at 40% label noise, the unhinged classifier is ableto find a perfect solution. By contrast, both other losses suffer at even moderate noise rates.We next report results on some UCI datasets, where we additionally tune a threshold so as to ensurethe best training set 0-1 accuracy. Table 2 summarises results on a sample of four datasets. (TheAppendix contains results with more datasets, performance metrics, and losses.) Even at noise closeto 50%, the unhinged loss is often able to learn a classifier with some discriminative power.\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x81======00.10.20.30.40.49Hinget-LogisticUnhinged0.00 \\xc2\\xb1 0.000.01 \\xc2\\xb1 0.030.06 \\xc2\\xb1 0.120.17 \\xc2\\xb1 0.200.35 \\xc2\\xb1 0.240.60 \\xc2\\xb1 0.200.00 \\xc2\\xb1 0.000.01 \\xc2\\xb1 0.030.04 \\xc2\\xb1 0.050.09 \\xc2\\xb1 0.110.24 \\xc2\\xb1 0.160.49 \\xc2\\xb1 0.200.00 \\xc2\\xb1 0.000.00 \\xc2\\xb1 0.000.00 \\xc2\\xb1 0.010.02 \\xc2\\xb1 0.070.13 \\xc2\\xb1 0.220.45 \\xc2\\xb1 0.33\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x81======00.10.20.30.40.49Hinget-LogisticUnhinged0.05 \\xc2\\xb1 0.000.06 \\xc2\\xb1 0.010.06 \\xc2\\xb1 0.010.08 \\xc2\\xb1 0.040.14 \\xc2\\xb1 0.100.45 \\xc2\\xb1 0.260.05 \\xc2\\xb1 0.000.07 \\xc2\\xb1 0.020.08 \\xc2\\xb1 0.030.11 \\xc2\\xb1 0.050.24 \\xc2\\xb1 0.130.49 \\xc2\\xb1 0.160.05 \\xc2\\xb1 0.000.05 \\xc2\\xb1 0.000.05 \\xc2\\xb1 0.000.05 \\xc2\\xb1 0.010.09 \\xc2\\xb1 0.100.46 \\xc2\\xb1 0.30(a) iris.\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x81======00.10.20.30.40.49(b) housing.Hinget-LogisticUnhinged0.00 \\xc2\\xb1 0.000.10 \\xc2\\xb1 0.080.19 \\xc2\\xb1 0.110.31 \\xc2\\xb1 0.130.39 \\xc2\\xb1 0.130.50 \\xc2\\xb1 0.160.00 \\xc2\\xb1 0.000.11 \\xc2\\xb1 0.020.15 \\xc2\\xb1 0.020.22 \\xc2\\xb1 0.030.33 \\xc2\\xb1 0.040.48 \\xc2\\xb1 0.040.00 \\xc2\\xb1 0.000.00 \\xc2\\xb1 0.000.00 \\xc2\\xb1 0.000.01 \\xc2\\xb1 0.000.02 \\xc2\\xb1 0.020.34 \\xc2\\xb1 0.21\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x81\\xcf\\x81(c) usps0v7.======00.10.20.30.40.49Hinget-LogisticUnhinged0.05 \\xc2\\xb1 0.000.15 \\xc2\\xb1 0.030.21 \\xc2\\xb1 0.030.25 \\xc2\\xb1 0.030.31 \\xc2\\xb1 0.050.48 \\xc2\\xb1 0.090.04 \\xc2\\xb1 0.000.24 \\xc2\\xb1 0.000.24 \\xc2\\xb1 0.000.24 \\xc2\\xb1 0.000.24 \\xc2\\xb1 0.000.40 \\xc2\\xb1 0.240.19 \\xc2\\xb1 0.000.19 \\xc2\\xb1 0.010.19 \\xc2\\xb1 0.010.19 \\xc2\\xb1 0.030.22 \\xc2\\xb1 0.050.45 \\xc2\\xb1 0.08(d) splice.Table 2: Mean and standard deviation of the 0-1 error over 125 trials on UCI datasets.8Conclusion and future workWe proposed a convex, classification-calibrated loss, proved that is robust to symmetric label noise(SLN-robust), showed it is the unique loss that satisfies a notion of strong SLN-robustness, established that it is optimised by the nearest centroid classifier, and showed that most convex potentials,such as the SVM, are also SLN-robust when highly regularised. So, with apologies to Wilde [1895]:While the truth is rarely pure, it can be simple.AcknowledgmentsNICTA is funded by the Australian Government through the Department of Communications andthe Australian Research Council through the ICT Centre of Excellence Program. The authors thankCheng Soon Ong for valuable comments on a draft of this paper.8\\x0cReferencesDana Angluin and Philip Laird. Learning from noisy examples. Machine Learning, 2(4):343\\xe2\\x80\\x93370, 1988.Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk bounds. Journalof the American Statistical Association, 101(473):138 \\xe2\\x80\\x93 156, 2006.Christopher M Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York, Inc., 2006.Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Conference onComputational Learning Theory (COLT), pages 92\\xe2\\x80\\x93100, 1998.Vasil Denchev, Nan Ding, Hartmut Neven, and S.V.N. Vishwanathan. Robust classification with adiabaticquantum optimization. In International Conference on Machine Learning (ICML), pages 863\\xe2\\x80\\x93870, 2012.Luc Devroye, La\\xcc\\x81szlo\\xcc\\x81 Gyo\\xcc\\x88rfi, and Ga\\xcc\\x81bor Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, 1996.Nan Ding and S.V.N. Vishwanathan. t-logistic regression. In Advances in Neural Information ProcessingSystems (NIPS), pages 514\\xe2\\x80\\x93522. Curran Associates, Inc., 2010.Thomas S. Ferguson. Mathematical Statistics: A Decision Theoretic Approach. Academic Press, 1967.Aritra Ghosh, Naresh Manwani, and P. S. Sastry. Making risk minimization tolerant to label noise. Neurocomputing, 160:93 \\xe2\\x80\\x93 107, 2015.Trevor Hastie, Saharon Rosset, Robert Tibshirani, and Ji Zhu. The entire regularization path for the supportvector machine. Journal of Machine Learning Research, 5:1391\\xe2\\x80\\x931415, December 2004. ISSN 1532-4435.Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM, 5(6):392\\xe2\\x80\\x93401,November 1998.Philip M. Long and Rocco A. Servedio. Random classification noise defeats all convex potential boosters.Machine Learning, 78(3):287\\xe2\\x80\\x93304, 2010. ISSN 0885-6125.Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schu\\xcc\\x88tze. Introduction to Information Retrieval.Cambridge University Press, New York, NY, USA, 2008. ISBN 0521865719, 9780521865715.Naresh Manwani and P. S. Sastry. Noise tolerance under risk minimization. IEEE Transactions on Cybernetics,43(3):1146\\xe2\\x80\\x931151, June 2013.Hamed Masnadi-Shirazi, Vijay Mahadevan, and Nuno Vasconcelos. On the design of robust classifiers forcomputer vision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep D. Ravikumar, and Ambuj Tewari. Learning with noisylabels. In Advances in Neural Information Processing Systems (NIPS), pages 1196\\xe2\\x80\\x931204, 2013.Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in NeuralInformation Processing Systems (NIPS), pages 1177\\xe2\\x80\\x931184, 2007.Mark D. Reid and Robert C. Williamson. Composite binary losses. Journal of Machine Learning Research,11:2387\\xe2\\x80\\x932422, December 2010.Mark D Reid and Robert C Williamson. Information, divergence and risk for binary experiments. Journal ofMachine Learning Research, 12:731\\xe2\\x80\\x93817, Mar 2011.Bernhard Scho\\xcc\\x88lkopf and Alexander J Smola. Learning with kernels, volume 129. MIT Press, 2002.Rocco A. Servedio. On PAC learning using Winnow, Perceptron, and a Perceptron-like algorithm. In Conference on Computational Learning Theory (COLT), 1999.John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge Uni. Press, 2004.Alex Smola, Arthur Gretton, Le Song, and Bernhard Scho\\xcc\\x88lkopf. A Hilbert space embedding for distributions.In Algorithmic Learning Theory (ALT), 2007.Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Gert R. G. Lanckriet, and Bernhard Scho\\xcc\\x88lkopf.Kernel choice and classifiability for RKHS embeddings of probability distributions. In Advances in NeuralInformation Processing Systems (NIPS), 2009.Guillaume Stempfel and Liva Ralaivola. Learning SVMs from sloppily labeled data. In Artificial NeuralNetworks (ICANN), volume 5768, pages 884\\xe2\\x80\\x93893. Springer Berlin Heidelberg, 2009.Robert Tibshirani, Trevor Hastie, Balasubramanian Narasimhan, and Gilbert Chu. Diagnosis of multiple cancertypes by shrunken centroids of gene expression. Proceedings of the National Academy of Sciences, 99(10):6567\\xe2\\x80\\x936572, 2002.Oscar Wilde. The Importance of Being Earnest, 1895.9\\x0c']\n",
      "['6019', 'Algorithmic Stability and Uniform Generalization', 'Poster', '6019-algorithmic-stability-and-uniform-generalization.pdf', 'One of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience. This includes the necessary and sufficient conditions for generalization from a given finite training set to new observations. In this paper, we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions. We provide various interpretations of this result.  For instance,  a relationship is proved between stability and data processing, which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning. In addition, we establish a relationship between algorithmic stability and the size of the observation space, which provides a formal justification for dimensionality reduction methods. Finally, we connect algorithmic stability to the size of the hypothesis space, which recovers the classical PAC result that the size (complexity) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization.', 'Algorithmic Stability and Uniform GeneralizationIbrahim AlabdulmohsinKing Abdullah University of Science and TechnologyThuwal 23955, Saudi Arabiaibrahim.alabdulmohsin@kaust.edu.saAbstractOne of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience. This includes the necessary and sufficient conditions for generalization from a given finite training setto new observations. In this paper, we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric lossfunctions. We provide various interpretations of this result. For instance, a relationship is proved between stability and data processing, which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or byaugmenting training examples with artificial noise prior to learning. In addition,we establish a relationship between algorithmic stability and the size of the observation space, which provides a formal justification for dimensionality reductionmethods. Finally, we connect algorithmic stability to the size of the hypothesisspace, which recovers the classical PAC result that the size (complexity) of thehypothesis space should be controlled in order to improve algorithmic stabilityand improve generalization.1IntroductionOne fundamental goal of any learning algorithm is to strike a right balance between underfittingand overfitting. In mathematical terms, this is often translated into two separate objectives. First,we would like the learning algorithm to produce a hypothesis that is reasonably consistent with theempirical evidence (i.e. to have a small empirical risk). Second, we would like to guarantee that theempirical risk (training error) is a valid estimate of the true unknown risk (test error). The formercondition protects against underfitting while the latter condition protects against overfitting.The rationale behind these two objectives can be understood if we define the generalizationrisk\\x0c. \\x0cRgen by the absolute difference between the empirical and true risks: Rgen = \\x0cRemp \\xe2\\x88\\x92 Rtrue \\x0c.Then, it is elementary to observe that the true risk Rtrue is bounded from above by the sumRemp + Rgen . Hence, by minimizing both the empirical risk (underfitting) and the generalizationrisk (overfitting), one obtains an inference procedure whose true risk is minimal.Minimizing the empirical risk alone can be carried out using the empirical risk minimization (ERM)procedure [1] or some approximations to it. However, the generalization risk is often impossible todeal with directly. Instead, it is a common practice to bound it analyticaly so that we can establishconditions under which it is guaranteed to be small. By establishing conditions for generalization,one hopes to design better learning algorithms that both perform well empirically and generalizewell to novel observations in the future. A prominent example of such an approach is the SupportVector Machines (SVM) algorithm for binary classification [2].However, bounding the generalization risk is quite intricate because it can be approached fromvarious angles. In fact, several methods have been proposed in the past to prove generalization bounds including uniform convergence, algorithmic stability, Rademacher and Gaussian complexities, generic chaining bounds, the PAC-Bayesian framework, and robustness-based analysis1\\x0c[1, 3, 4, 5, 6, 7, 8, 9]. Concentration of measure inequalities form the building blocks of these richtheories.The proliferation of generalization bounds can be understood if we look into the general setting oflearning introduced by Vapnik [1]. In this setting, we have an observation space Z and a hypothesismspace H. A learning algorithm, henceforth denoted L : \\xe2\\x88\\xaa\\xe2\\x88\\x9e\\xe2\\x86\\x92 H, uses a finite set ofm=1 Zobservations to infer a hypothesis H \\xe2\\x88\\x88 H. In the general setting, the inference process end-to-endis influenced by three key factors: (1) the nature of the observation space Z, (2) the nature of thehypothesis space H, and (3) the details of the learning algorithm L. By imposing constraints onany of these three components, one may be able to derive new generalization bounds. For example,the Vapnik-Chervonenkis (VC) theory derives generalization bounds by assuming constraints on H,while stability bounds, e.g. [6, 10, 11, 12], are derived by assuming constraints on L.Given that different generalization bounds can be established by imposing constraints on any ofZ, H, or L, it is intriguing to ask if there exists a single view for generalization that ties all of thesedifferent components together. In this paper, we answer this question in the affirmative by establishing that algorithmic stability alone is equivalent to uniform generalization. Informally speaking, aninference process is said to generalize uniformly if the generalization risk vanishes uniformly acrossall bounded parametric loss functions at the limit of large training sets. A more precise definitionwill be presented in the sequel. We will show why constraints that are imposed on either H, Z, orL to improve uniform generalization can be interpreted as methods of improving the stability of thelearning algorithm L. This is similar in spirit to a result by Kearns and Ron, who showed that having a finite VC dimension in the hypothesis space H implies a certain notion of algorithmic stabilityin the inference process [13]. Our statement, however, is more general as it applies to all learningalgorithms that fall under Vapnik\\xe2\\x80\\x99s general setting of learning, well beyond uniform convergence.The rest of the paper is as follows. First, we review the current literature on algorithmic stability,generalization, and learnability. Then, we introduce key definitions that will be repeatedly usedthroughout the paper. Next, we prove the central theorem, which reveals that algorithmic stability isequivalent to uniform generalization, and provide various interpretations of this result afterward.2Related WorkPerhaps, the two most fundamental concepts in statistical learning theory are those of learnabilityand generalization [12, 14]. The two concepts are distinct from each other. As will be discussedin more details next, whereas learnability is concerned with measuring the excess risk within ahypothesis space, generalization is concerned with estimating the true risk.In order to define learnability and generalization, suppose we have an observation space Z, a probability distribution of observations P(z), and a bounded stochastic loss function L(\\xc2\\xb7; H) : Z \\xe2\\x86\\x92[0, 1], where H \\xe2\\x88\\x88 H is an inferred hypothesis. Note that L is implicitly a function of (parameterized by) H as well. We define the true risk of a hypothesis H \\xe2\\x88\\x88 H by the risk functional:\\x02\\x03Rtrue (H) = EZ\\xe2\\x88\\xbcP(z) L(Z; H)(1)Then, a learning algorithm is called consistent if the true risk of its inferred hypothesis H convergesto the optimal true risk within the hypothesis space H at the limit of large training sets m \\xe2\\x86\\x92 \\xe2\\x88\\x9e.A problem is called learnable if it admits a consistent learning algorithm [14]. It has been knownthat learnability for supervised classification and regression problems is equivalent to uniform convergence [3, 14]. However, Shalev-Shwartz et al. recently showed that uniform convergence is notnecessary in Vapnik\\xe2\\x80\\x99s general setting of learning and proposed algorithmic stability as an alternativekey condition for learnability [14].Unlike learnability, the question of generalization is concerned primarily with how representativethe empirical risk Remp is to the true risk Rtrue . To elaborate, suppose we have a finite training setSm = {Zi }i=1,..,m , which comprises of m i.i.d. observations Zi \\xe2\\x88\\xbc P(z). We define the empiricalrisk of a hypothesis H with respect to Sm by:1 XRemp (H; Sm ) =L(Zi ; H)(2)mZi \\xe2\\x88\\x88SmWe also let Rtrue (H) be the true risk as defined in Eq. (1). Then, a learning algorithm L is said togeneralize if the empirical risk of its inferred hypothesis converges to its true risk as m \\xe2\\x86\\x92 \\xe2\\x88\\x9e.2\\x0cSimilar to learnability, uniform convergence is, by definition, sufficient for generalization [1], butit is not necessary because the learning algorithm can always restrict its search space to a smallersubset of H (artificially so to speak). By contrast, it is not known whether algorithmic stability isnecessary for generalization. It has been shown that various notions of algorithmic stability can bedefined that are sufficient for generalization [6, 10, 11, 12, 15, 16]. However, it is not known whetheran appropriate notion of algorithmic stability can be defined that is both necessary and sufficient forgeneralization in Vapnik\\xe2\\x80\\x99s general setting of learning. In this paper, we answer this question byshowing that stability in the inference process is not only sufficient for generalization, but it is, infact, equivalent to uniform generalization, which is a notion of generalization that is stronger thanthe one traditionally considered in the literature.3PreliminariesTo simplify the discussion, we will always assume that all sets are countable, including the observation space Z and the hypothesis space H. This is similar to the assumptions used in some previousworks such as [6]. However, the main results, which are presented in Section 4, can be readilygeneralized. In addition, we assume that all learning algorithms are invariant to permutations of thetraining set. Hence, the order of training examples is irrelevant.Moreover, if X \\xe2\\x88\\xbc P(x) is a random variabledrawn from the alphabet X and f (X) is a function ofPX, we write EX\\xe2\\x88\\xbcP(x) f (X) to mean x\\xe2\\x88\\x88X P(x) f (x). Often, we will simply write EX f (X) tomean EX\\xe2\\x88\\xbcP(x) f (X) if the distribution of X is clear from the context. If X takes its values froma finite set S uniformly at random, we write X \\xe2\\x88\\xbc S to denote this distribution of X. If X is aboolean random variable, then I{X} = 1 if and only if X is true, otherwise I{X} = 0. In general,random variables are denoted with capital letters, instances of random variables are denoted withsmall letters, and alphabets are denoted with calligraphic typeface. Also, given two probability massfunctions P and Q defined on the same alphabet A, we will write hP,Qi to denote the overlapping. Pcoefficient, i.e. intersection, between P and Q. That is, hP, Qi = a\\xe2\\x88\\x88A min{P (a), Q(a)}. Notethat hP, Qi = 1\\x01\\xe2\\x88\\x92 ||P , Q||T , where ||P , Q||T is the total variation distance. Last, we will writeB(k; \\xcf\\x86, n) = nk \\xcf\\x86k (1 \\xe2\\x88\\x92 \\xcf\\x86)n\\xe2\\x88\\x92k to denote the binomial distribution.In this paper, we consider the general setting of learning introduced by Vapnik [1]. To reiterate, wehave an observation space Z and a hypothesis space H. Our learning algorithm L receives a set ofm observations Sm = {Zi }i=1,..,m \\xe2\\x88\\x88 Z m generated i.i.d. from a fixed unknown distribution P(z),mand picks a hypothesis H \\xe2\\x88\\x88 H with probability PL (H = h|Sm ). Formally, L : \\xe2\\x88\\xaa\\xe2\\x88\\x9e\\xe2\\x86\\x92 H is am=1 Zstochastic map. In this paper, we allow the hypothesis H to be any summary statistic of the trainingset. It can be a measure of central tendency, as in unsupervised learning, or it can be a mapping froman input space to an output space, as in supervised learning. In fact, we even allow H to be a subsetof the training set itself. In formal terms, L is a stochastic map between the two random variablesH \\xe2\\x88\\x88 H and Sm \\xe2\\x88\\x88 Z m , where the exact interpretation of those random variables is irrelevant.In any learning task, we assume a non-negative bounded loss function L(Z; H) : Z \\xe2\\x86\\x92 [0, 1] isused to measure the quality of the inferred hypothesis H \\xe2\\x88\\x88 H on the observation Z \\xe2\\x88\\x88 Z. Mostimportantly, we assume that L(\\xc2\\xb7; H) : Z \\xe2\\x86\\x92 [0, 1] is parametric:Definition 1 (Parametric Loss Functions). A loss function L(\\xc2\\xb7; H) : Z \\xe2\\x86\\x92 [0, 1] is called parametric if it is independent of the training set Sm given the inferred hypothesis H. That is, a parametricloss function satisfies the Markov chain: Sm \\xe2\\x86\\x92 H \\xe2\\x86\\x92 L(\\xc2\\xb7; H).For any fixed hypothesis H \\xe2\\x88\\x88 H, we define its true risk Rtrue (H) by Eq. (1), and define itsempirical risk on a training set Sm , denoted Remp (H; Sm ), by Eq. (2). We also define the true andempirical risks of the learning algorithm L by the expected risk of its inferred hypothesis:R\\xcc\\x82true (L) = ESm EH \\xe2\\x88\\xbcPL (h|Sm ) Rtrue (H)= ESm EH|Sm Rtrue (H)(3)R\\xcc\\x82emp (L) = ESm EH \\xe2\\x88\\xbcPL (h|Sm ) Remp (H; Sm )= ESm EH|Sm Remp (H; Sm )(4)To simplify notation, we will write R\\xcc\\x82true and R\\xcc\\x82emp instead of R\\xcc\\x82true (L) and R\\xcc\\x82emp (L). We willconsider the following definition of generalization:mDefinition 2 (Generalization). A learning algorithm L : \\xe2\\x88\\xaa\\xe2\\x88\\x9e\\xe2\\x86\\x92 H with a parametricm=1 Zloss function L(\\xc2\\xb7; H) : \\x0c Z \\xe2\\x86\\x92 [0, 1] generalizes if for any distribution P(z) on Z, we havelimm\\xe2\\x86\\x92\\xe2\\x88\\x9e |R\\xcc\\x82emp \\xe2\\x88\\x92 R\\xcc\\x82true \\x0c = 0, where R\\xcc\\x82true and R\\xcc\\x82emp are given in Eq. (3) and Eq. (4) respectively.3\\x0cIn other words, a learning algorithm L generalizes according to Definition 2 if its empirical performance (training loss) becomes an unbiased estimator to the true risk as m \\xe2\\x86\\x92 \\xe2\\x88\\x9e. Next, we defineuniform generalization:mDefinition 3 (Uniform Generalization). A learning algorithm L : \\xe2\\x88\\xaa\\xe2\\x88\\x9e\\xe2\\x86\\x92 H generalizesm=1 Zuniformly if for any \\x0f > 0, there exists m0 (\\x0f) > 0 such that for all distributions P(z) on\\x0c Z, allparametric loss functions, and all sample sizes m > m0 (\\x0f), we have |R\\xcc\\x82emp (L) \\xe2\\x88\\x92 R\\xcc\\x82true (L)\\x0c \\xe2\\x89\\xa4 \\x0f.Uniform generalization is stronger than the original notion of generalization in Definition 2. Inparticular, if a learning algorithm generalizes uniformly, then it generalizes according to Definition2 as well. The converse, however, is not true. Even though uniform generalization appears to bequite a strong condition, at first sight, a key contribution of this paper is to show that it is not a strongcondition because it is equivalent to a simple condition, namely algorithmic stability.4Main ResultsBefore we prove that algorithmic stability is equivalent to uniform generalization, we introduce aprobabilistic notion of mutual stability between two random variables. In order to abstract away anylabeling information the random variables might possess, e.g. the observation space may or may notbe a metric space, we define stability by the impact of observations on probability distributions:Definition 4 (Mutual Stability). Let X \\xe2\\x88\\x88 X and Y \\xe2\\x88\\x88 Y be two random variables. Then, the mutualstability between X and Y is defined by:.S(X; Y ) = hP(X) P(Y ), P(X, Y )i = EX hP(Y ), P(Y |X)i = EY hP(X), P(X|Y )iIf we recall that 0 \\xe2\\x89\\xa4 hP, Qi \\xe2\\x89\\xa4 1 is the overlapping coefficient between the two probability distributions P and Q, we see that S(X; Y ) given by Definition 4 is indeed a probabilistic measureof mutual stability. It measures how stable the distribution of Y is before and after observing aninstance of X, and vice versa. A small value of S(X; Y ) means that the probability distribution ofX or Y is heavily perturbed by a single observation of the other random variable. Perfect mutualstability is achieved when the two random variables are independent of each other.With this probabilistic notion of mutual stability in mind, we define the stability of a learning algorithm L by the mutual stability between its inferred hypothesis and a random training example.m\\xe2\\x86\\x92 H be a learning algorithm that receivesDefinition 5 (Algorithmic Stability). Let L : \\xe2\\x88\\xaa\\xe2\\x88\\x9em=1 Za finite set of training examples Sm = {Zi }i=1,..,m \\xe2\\x88\\x88 Z m drawn i.i.d. from a fixed distributionP(z). Let H \\xe2\\x88\\xbc PL (h|Sm ) be the hypothesis inferred by L, and let Ztrn \\xe2\\x88\\xbc Sm be a single random training example. We define the stability of L by: S(L) = inf P(z) S(H; Ztrn ), where theinfimum is taken over all possible distributions of observations P(z). A learning algorithm is calledalgorithmically stable if limm\\xe2\\x86\\x92\\xe2\\x88\\x9e S(L) = 1.Note that the above definition of algorithmic stability is rather weak; it only requires that the contribution of any single training example on the overall inference process to be more and more negligibleas the sample size increases. In addition, it is well-defined even if the learning algorithm is deterministic because the hypothesis H, if it is a deterministic function of an entire training set of mobservations, remains a stochastic function of any individual observation. We illustrate this conceptwith the following example:Example 1. Suppose that observations Zi \\xe2\\x88\\x88 {0, 1} are i.i.d. Bernoulli Ptrials with P(Zi = 1) = \\xcf\\x86,m1and thatthehypothesisproducedbyListheempiricalaverageH=i=1 Zi . Because P(H =m\\x0c\\x0ck/m \\x0c Ztrn = 1) = B(k \\xe2\\x88\\x92 1; \\xcf\\x86, m \\xe2\\x88\\x92 1) and P(H = k/m \\x0c Ztrn = 0) = B(k; \\xcf\\x86, m \\xe2\\x88\\x92 1), it can beshown using Stirling\\xe2\\x80\\x99s approximation [17] that the algorithmic stability of this learning algorithmis asymptotically given by S(L) \\xe2\\x88\\xbc 1 \\xe2\\x88\\x92 \\xe2\\x88\\x9a21\\xcf\\x80 m , which is achieved when \\xcf\\x86 = 1/2. A more generalstatement will be proved later in Section 5.Next, we show that the notion of algorithmic stability in Definition 5 is equivalent to the notion ofuniform generalization in Definition 3. Before we do that, we first state the following lemma.Lemma 1 (Data Processing Inequality). Let A, B, and C be three random variables that satisfy theMarkov chain A \\xe2\\x86\\x92 B \\xe2\\x86\\x92 C. Then: S(A; B) \\xe2\\x89\\xa4 S(A; C).4\\x0cProof. The proof consists of two steps 1 . First, we note that because the Markov chain implies thatP(C|B, A) = P(C|B), we have S(A; (B, C)) = S(A; B) by direct substitution into Definition5. Second, similar to the information-cannot-hurt inequality in information theory [18], it can beshown that S(A; (B, C)) \\xe2\\x89\\xa4 S(A; C) for any random variables A, B and C. This is proved usingsome algebraic manipulation\\x08andminimum of the sums is always larger than the\\t thePP the factP that\\xce\\xb1,\\xce\\xb2sum of minimums, i.e. min\\xe2\\x89\\xa5i ii ii min{\\xce\\xb1i , \\xce\\xb2i }. Combining both results yieldsS(A; B) = S(A; (B, C)) \\xe2\\x89\\xa4 S(A; C), which is the desired result.Now, we are ready to state the main result of this paper.mTheorem 1. For any learning algorithm L : \\xe2\\x88\\xaa\\xe2\\x88\\x9e\\xe2\\x86\\x92 H, algorithmic stability as given in Defm=1 Zinition5isbothnecessaryandsufficientforuniformgeneralization(see Definition 3). In addition,\\x0c\\x0c\\x0cR\\xcc\\x82true \\xe2\\x88\\x92 R\\xcc\\x82emp \\x0c \\xe2\\x89\\xa4 1 \\xe2\\x88\\x92 S(H; Ztrn ) \\xe2\\x89\\xa4 1 \\xe2\\x88\\x92 S(L), where Rtrue and Remp are the true and empiricalrisks of the learning algorithm defined in Eq. (3) and (4) respectively.Proof. Here is an outline of the proof. First, because a parametric loss function L(\\xc2\\xb7; H) : Z \\xe2\\x86\\x92 [0, 1]is itself a random variable that satisfies the Markov chain Sm \\xe2\\x86\\x92 H \\xe2\\x86\\x92 L(\\xc2\\xb7; H), it is not independentof Ztrn \\xe2\\x88\\xbc Sm . Hence, the empirical risk is given by R\\xcc\\x82emp = EL(\\xc2\\xb7;H) EZtrn |L(\\xc2\\xb7;H) L(Ztrn ; H). Bycontrast, the true risk is given by R\\xcc\\x82true = EL(\\xc2\\xb7;H) EZtrn \\xe2\\x88\\xbcP(z) L(Ztrn ; H). The difference is:\\x02\\x03R\\xcc\\x82true \\xe2\\x88\\x92 R\\xcc\\x82emp = EL(\\xc2\\xb7;H) EZtrn L(Ztrn ; H) \\xe2\\x88\\x92 EZtrn |L(\\xc2\\xb7;H) L(Ztrn ; H)To sandwich the right-hand side between an upper and a lower bound, we note that if P1 (z) andP2 (z) are two distributionsdefined on the same alphabetZ and F (\\xc2\\xb7) : Z \\xe2\\x86\\x92 [0, 1] is a bounded loss\\x0c\\x0c\\x0c\\x0cfunction, then \\x0cEZ\\xe2\\x88\\xbcP1 (z) F (Z) \\xe2\\x88\\x92 EZ\\xe2\\x88\\xbcP2 (z) F (Z)\\x0c \\xe2\\x89\\xa4 ||P1 (z) , P2 (z)||T , where ||P , Q||T is thetotal variation distance. The proof to this result can be immediately deduced by considering the tworegions {z \\xe2\\x88\\x88 Z : P1 (z) > P2 (z)} and {z \\xe2\\x88\\x88 Z : P1 (z) < P2 (z)} separately. This is, then, used todeduce the inequalities:\\x0c\\x0c\\x0cR\\xcc\\x82true \\xe2\\x88\\x92 R\\xcc\\x82emp \\x0c \\xe2\\x89\\xa4 1 \\xe2\\x88\\x92 S(L(\\xc2\\xb7; H); Ztrn ) \\xe2\\x89\\xa4 1 \\xe2\\x88\\x92 S(H; Ztrn ) \\xe2\\x89\\xa4 1 \\xe2\\x88\\x92 S(L),where the second inequality follows by the data processing inequality in Lemma 1, whereas thelast inequality follows by definition of algorithmic stability (see Definition5). This\\x0c proves that\\x0cif L is algorithmically stable, i.e. S(L) \\xe2\\x86\\x92 1 as m \\xe2\\x86\\x92 \\xe2\\x88\\x9e, then \\x0cR\\xcc\\x82true \\xe2\\x88\\x92 R\\xcc\\x82emp \\x0c converges tozero uniformly across all parametric loss functions. Therefore, algorithmic stability is sufficient foruniform generalization. The converse is proved by showing that for anya bounded\\x0c \\xce\\xb4 > 0, there exists\\x0cparametric loss and a distribution P\\xce\\xb4 (z) such that 1 \\xe2\\x88\\x92 S(L) \\xe2\\x88\\x92 \\xce\\xb4 \\xe2\\x89\\xa4 \\x0cR\\xcc\\x82true \\xe2\\x88\\x92 R\\xcc\\x82emp \\x0c \\xe2\\x89\\xa4 1 \\xe2\\x88\\x92 S(L).Therefore, algorithmic stability is also necessary for uniform generalization.5Interpreting Algorithmic Stability and Uniform GeneralizationIn this section, we provide several interpretations of algorithmic stability and uniform generalization.In addition, we show how Theorem 1 recovers some classical results in learning theory.5.1Algorithmic Stability and Data ProcessingThe relationship between algorithmic stability and data processing is presented in Lemma 1. Giventhe random variables A, B, and C and the Markov chain A \\xe2\\x86\\x92 B \\xe2\\x86\\x92 C, we always have S(A; B) \\xe2\\x89\\xa4S(A; C). This presents us with qualitative insights into the design of machine learning algorithms.First, suppose we have two different hypotheses H1 and H2 . We will say that H2 contains lessinformative than H1 if the Markov chain Sm \\xe2\\x86\\x92 H1 \\xe2\\x86\\x92 H2 holds. For example, if observationsZi \\xe2\\x88\\x88 {0, 1} are Bernoulli trials, then H1 \\xe2\\x88\\x88 R can be the empirical average as given in Example 1while H2 \\xe2\\x88\\x88 {0, 1} can be the label that occurs most often in the training set. Because H2 = I{H1 \\xe2\\x89\\xa5m/2}, the hypothesis H2 contains strictly less information about the original training set than H1 .Formally, we have Sm \\xe2\\x86\\x92 H1 \\xe2\\x86\\x92 H2 . In this case, H2 enjoys a better uniform generalization boundthan H1 because of data-processing. Intuitively, we know that such a result should hold because H2is less tied to the original training set than H1 . This brings us to the following remark.1Detailed proofs are available in the supplementary file.5\\x0cRemark 1. We can improve the uniform generalization bound (or equivalently algorithmic stability)of a learning algorithm by post-processing its inferred hypothesis H in a manner that is conditionally independent of the original training set given H.Example 2. Post-processing hypotheses is a common technique used in machine learning. Thisincludes sparsifying the coefficient vector w \\xe2\\x88\\x88 Rd in linear methods, where wj is set to zero if it hasa small absolute magnitude. It also includes methods that have been proposed to reduce the numberof support vectors in SVM by exploiting linear dependence [19]. By the data processing inequality,such methods improve algorithmic stability and uniform generalization.Needless to mention, better generalization does not immediately translate into a smaller true risk.This is because the empirical risk itself may increase when the inferred hypothesis is post-processedindependently of the original training set.Second, if the Markov chain A \\xe2\\x86\\x92 B \\xe2\\x86\\x92 C holds, we also obtain S(A; C) \\xe2\\x89\\xa5 S(B; C) by applyingthe data processing inequality to the reverse Markov chain C \\xe2\\x86\\x92 B \\xe2\\x86\\x92 A. As a result, we can improve algorithmic stability by contaminating training examples with artificial noise prior to learning.This is because if S\\xcc\\x82m is a perturbed version of a training set Sm , then Sm \\xe2\\x86\\x92 S\\xcc\\x82m \\xe2\\x86\\x92 H implies thatS(Ztrn ; H) \\xe2\\x89\\xa5 S(Z\\xcc\\x82trn ; H), when Ztrn \\xe2\\x88\\xbc Sm and Z\\xcc\\x82trn \\xe2\\x88\\xbc S\\xcc\\x82m are random training examples drawnuniformly at random from each training set respectively. This brings us to the following remark:Remark 2. We can improve the algorithmic stability of a learning algorithm by introducing artificialnoise to training examples, and applying the learning algorithm on the perturbed training set.Example 3. Corrupting training examples with artificial noise, such as the recent dropout method,are popular techniques in neural networks to improve generalization [20]. By the data processinginequality, such methods indeed improve algorithmic stability and uniform generalization.5.2Algorithmic Stability and the Size of the Observation SpaceNext, we look into how the size of the observation space Z influences algorithmic stability. First,we start with the following definition:Definition 6 (Lazy Learning). A learning algorithm L is called lazy if its hypothesis H \\xe2\\x88\\x88 H ismapped one-to-one with the training set Sm , i.e. the mapping H \\xe2\\x86\\x92 Sm is injective.A lazy learner is called lazy if its hypothesis is equivalent to the original training set in its information content. Hence, no learning actually takes place. One example is instance-based learningwhen H = Sm . Despite their simple nature, lazy learners are useful in practice. They are usefultheoretical tools as well. In particular, because of the equivalence H \\xe2\\x89\\xa1 Sm and the data processinginequality, the algorithmic stability of a lazy learner provides a lower bound to the stability of anypossible learning algorithm. Therefore, we can relate algorithmic stability (uniform generalization)to the size of the observation space by quantifying the algorithmic stability of lazy learners. Becausethe size of Z is usually infinite, however, we introduce the following definition of effective set size.Definition 7. In a countable space Z endowed with a probability mass function P(z), the effectivep\\x012P.size of Z w.r.t. P(z) is defined by: Ess [Z; P(z)] = 1 +P(z) (1 \\xe2\\x88\\x92 P(z)) .z\\xe2\\x88\\x88ZAt one extreme, if P(z) is uniform over a finite alphabet Z, then Ess [Z; P(z)] = |Z|. At theother extreme, if P(z) is a Kronecker delta distribution, then Ess [Z; P(z)] = 1. As proved next,this notion of effective set size determines the rate of convergence of an empirical probability massfunction to its true distribution when the distance is measured in the total variation sense. As a result,it allows us to relate algorithmic stability to a property of the observation space Z.Theorem 2. Let Z be a countable space endowed with a probability mass function P(z). Let Smbe a set of m i.i.d. samples Zi \\xe2\\x88\\xbc P(z). Define PSm (z) to be the empirical probability mass functionq induced by drawing samples uniformly at random from Sm . Then: ESm ||P(z), PSm (z)||T =\\xe2\\x88\\x9aEss [Z; P(z)]\\xe2\\x88\\x921+ o(1/ m), where 1 \\xe2\\x89\\xa4 Ess [Z; P(z)] \\xe2\\x89\\xa4 |Z| is the effective size of Z (see Def2\\xcf\\x80m\\xe2\\x88\\x9eminitionq 7). In addition, for any learning algorithm L : \\xe2\\x88\\xaam=1 Z \\xe2\\x86\\x92 H, we have S(H; Ztrn ) \\xe2\\x89\\xa5\\xe2\\x88\\x9aP(z)]\\xe2\\x88\\x9211 \\xe2\\x88\\x92 Ess [Z;\\xe2\\x88\\x92 o(1/ m), where the bound is achieved by lazy learners (see Definition 6)2 .2\\xcf\\x80m2A special case of Theorem 2 was proved by de Moivre in the 1730s, who showed that thepempirical mean ofi.i.d. Bernoulli trials with a probability of success \\xcf\\x86 converges to the true mean at a rate of 2\\xcf\\x86(1 \\xe2\\x88\\x92 \\xcf\\x86)/(\\xcf\\x80m)6\\x0c\\x01 m1 m2mProof. Here is an outline of the proof. First, we know that P(Sm ) = m1 , mp1 p2 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 , where,...2\\x01\\xc2\\xb71\\xc2\\xb7 is the multinomial coefficient. Using the relation ||P, Q||T = 2 ||P \\xe2\\x88\\x92 Q||1 , the multinomialseries, and De Moivre\\xe2\\x80\\x99s formula for the mean deviation of the binomial random variable [22], it canbe shown with some algebraic manipulations that:1 Xm!kESm ||P(z), PSm (z)||T =(1 \\xe2\\x88\\x92 pk )(1\\xe2\\x88\\x92pk )m p1+mpkm(pk m)! ((1 \\xe2\\x88\\x92 pk )m \\xe2\\x88\\x92 1)!k=1,2,...Using Stirling\\xe2\\x80\\x99s approximation to the factorial [17], we obtain the simple asymptotic expression:rrX12pk (1 \\xe2\\x88\\x92 pk )Ess [Z; P(z)] \\xe2\\x88\\x92 1ESm ||P(z), PSm (z)||T \\xe2\\x88\\xbc=1\\xe2\\x88\\x92,2\\xcf\\x80m2\\xcf\\x80mk=1,2,3,...which is tight due to the tightness of the Stirling approximation. The rest of the theorem followsfrom the Markov chain Sm \\xe2\\x86\\x92 Sm \\xe2\\x86\\x92 H, the data processing inequality, and Definition 6.Corollary 1. Given the conditions of Theorem 2,qif Z is in addition finite (i.e. |Z| < \\xe2\\x88\\x9e), then for\\xe2\\x88\\x9aany learning algorithm L, we have: S(L) \\xe2\\x89\\xa5 1 \\xe2\\x88\\x92 |Z|\\xe2\\x88\\x9212\\xcf\\x80m \\xe2\\x88\\x92 o(1/ m)Proof. Because in a finite observation space Z, the maximum effective set size (see Definition 7) is|Z|, which is attained at the uniform distribution P(z) = 1/|Z|.Intuitively speaking, Theorem 2 and its corollary state that in order to guarantee good uniformgeneralization for all possible learning algorithms, the number of observations must be sufficientlylarge to cover the entire effective size of the observation space Z. Needless to mention, this isdifficult to achieve in practice so the algorithmic stability of machine learning algorithms must becontrolled in order to guarantee a good generalization from a few empirical observations. Similarly,the uniform generalization bound can be improved by reducing the effective size of the observationspace, such as by using dimensionality reduction methods.5.3Algorithmic Stability and the Complexity of the Hypothesis SpaceFinally, we look into the hypothesis space and how it influences algorithmic stability. First, we lookinto the role of the size of the hypothesis space. This is formalized in the following theorem.mTheorem 3. Denote by H \\xe2\\x88\\x88 H the hypothesis inferred by a learning algorithm L : \\xe2\\x88\\xaa\\xe2\\x88\\x9e\\xe2\\x86\\x92m=1 ZH. Then, the following bound on algorithmic stability always holds:rrH(H)log |H|\\xe2\\x89\\xa51\\xe2\\x88\\x92,S(L) \\xe2\\x89\\xa5 1 \\xe2\\x88\\x922m2mwhere H is the Shannon entropy measured in nats (i.e. using natural logarithms).Proof. The proof is information-theoretic. If we let I(X; Y ) be the mutual information between ther.v.\\xe2\\x80\\x99s X and Y and let Sm = {Z1 , Z2 , . . . , Zm } be a random choice of a training set, we have:mhXi hiI(Sm ; H) = H(Sm ) \\xe2\\x88\\x92 H(Sm | H) =H(Zi ) \\xe2\\x88\\x92 H(Z1 |H) + H(Z2 |Z1 , H) + \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7i=1Because conditioning reduces entropy, i.e. H(A|B) \\xe2\\x89\\xa4 H(A) for any r.v.\\xe2\\x80\\x99s A and B, we have:I(Sm ; H) \\xe2\\x89\\xa5mX[H(Zi ) \\xe2\\x88\\x92 H(Zi | H)] = m [H(Ztrn ) \\xe2\\x88\\x92 H(Ztrn | H)]i=1Therefore:I(Ztrn ; H) \\xe2\\x89\\xa4I(Sm ; H)m(5)on average. This is believed to be the first appearance of the square-root law in statistical inference in theliterature [21]. Because the effective set size of the Bernoulli distribution, according to Definition 7, is givenby 1 + 4\\xcf\\x86(1 \\xe2\\x88\\x92 \\xcf\\x86), Theorem 2 agrees with, in fact generalizes, de Moivre\\xe2\\x80\\x99s result.7\\x0cNext, we use Pinsker\\xe2\\x80\\x99sq inequality [18], which states that for any probability distributions P andD(P || Q)Q: ||P , Q||T \\xe2\\x89\\xa4, where ||P , Q||T is total variation distance and D(P || Q) is2the Kullback-Leibler divergence measured in nats (i.e. using natural logarithms). If we recallthat S(Sm ; H) = 1 \\xe2\\x88\\x92 ||P(Sm ) P(H) , P(Sm , H)||T while mutual information is I(Sm ; H) =D(P(Sm , H) || P(Sm ) P(H)), we deduce from Pinsker\\xe2\\x80\\x99s inequality and Eq. (5):S(Ztrn ; H) = 1 \\xe2\\x88\\x92 ||P(Ztrn ) P(H) , P(Ztrn , H)||TrrrrI(Ztrn ; H)I(Sm ; H)H(H)log |H|\\xe2\\x89\\xa51\\xe2\\x88\\x92\\xe2\\x89\\xa51\\xe2\\x88\\x92\\xe2\\x89\\xa51\\xe2\\x88\\x92\\xe2\\x89\\xa51\\xe2\\x88\\x9222m2m2mIn the last line, we used the fact that I(X; Y ) \\xe2\\x89\\xa4 H(X) for any random variables X and Y .Theorem 3 re-establishes the classical PAC result on the finite hypothesis space [23]. In terms ofalgorithmic stability, a learning algorithm will enjoy a high stability if the size of the hypothesisspace is small. In terms of uniform generalization, it states that the generalizationprisk of a learningalgorithmisboundedfromaboveuniformlyacrossallparametriclossfunctionsbyH(H)/(2m) \\xe2\\x89\\xa4plog |H|/(2m), where H(H) is the Shannon entropy of H.Next, we relate algorithmic stability to the Vapnik-Chervonenkis (VC) dimension. Despite the factthat the VC dimension is defined on binary-valued functions whereas algorithmic stability is a functional of probability distributions, there exists a connection between the two concepts. To show this,we first introduce a notion of an induced concept class that exists for any learning algorithm L:mDefinition 8. The concept class C induced by a learning algorithm L : \\xe2\\x88\\xaa\\xe2\\x88\\x9e\\xe2\\x86\\x92 H is definedm=1 Zto be the set of total Boolean functions c(z) = I{P(Ztrn = z | H) \\xe2\\x89\\xa5 P(Ztrn = z)} for all H \\xe2\\x88\\x88 H.Intuitively, every hypothesis H \\xe2\\x88\\x88 H induces a total partition on the observation space Z given bythe Boolean function in Definition 8. That is, H splits Z into two disjoint sets: the set of values inZ that are, a posteriori, less likely to have been present in the training set than before given that theinferred hypothesis is H, and the set of all other values. The complexity (richness) of the inducedconcept class C is related to algorithmic stability via the VC dimension.mTheorem 4. Let L : \\xe2\\x88\\xaa\\xe2\\x88\\x9e\\xe2\\x86\\x92 H be a learning algorithm with an induced concept class C. Letm=1 ZdV C (C) be the VC dimension of C. Then, the following bound holds if m > dV C (C) + 1:p4 + dV C (C) (1 + log(2m))\\xe2\\x88\\x9aS(L) \\xe2\\x89\\xa5 1 \\xe2\\x88\\x922mIn particular, L is algorithmically stable if its induced concept class C has a finite VC dimension.Proof. Theis bounded from below by 1 \\xe2\\x88\\x92n proof relies on\\x0c the fact that algorithmic stability S(L)\\x0co\\x0c\\x0csupP(z) ESm suph\\xe2\\x88\\x88H EZ\\xe2\\x88\\xbcP(z) ch (Z) \\xe2\\x88\\x92 EZ\\xe2\\x88\\xbcSm ch (Z) , where cH (z) = I{P(Ztrn =z|H) \\xe2\\x89\\xa5 P(Ztrn ) = z}. The final bound follows by applying uniform convergence results [23].6ConclusionsIn this paper, we showed that a probabilistic notion of algorithmic stability was equivalent to uniformgeneralization. In informal terms, a learning algorithm is called algorithmically stable if the impactof a single training example on the probability distribution of the final hypothesis always vanishes atthe limit of large training sets. In other words, the inference process never depends heavily on anysingle training example. If algorithmic stability holds, then the learning algorithm generalizes wellregardless of the choice of the parametric loss function. We also provided several interpretations ofthis result. For instance, the relationship between algorithmic stability and data processing revealsthat algorithmic stability can be improved by either post-processing the inferred hypothesis or byaugmenting training examples with artificial noise prior to learning. In addition, we established arelationship between algorithmic stability and the effective size of the observation space, which provided a formal justification for dimensionality reduction methods. Finally, we connected algorithmicstability to the complexity (richness) of the hypothesis space, which re-established the classical PACresult that the complexity of the hypothesis space should be controlled in order to improve stability,and, hence, improve generalization.8\\x0cReferences[1] V. N. Vapnik, \\xe2\\x80\\x9cAn overview of statistical learning theory,\\xe2\\x80\\x9d Neural Networks, IEEE Transactionson, vol. 10, September 1999.[2] C. Cortes and V. Vapnik, \\xe2\\x80\\x9cSupport-vector networks,\\xe2\\x80\\x9d Machine learning, vol. 20, pp. 273\\xe2\\x80\\x93297,1995.[3] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth, \\xe2\\x80\\x9cLearnability and the VapnikChervonenkis dimension,\\xe2\\x80\\x9d Journal of the ACM (JACM), vol. 36, no. 4, pp. 929\\xe2\\x80\\x93965, 1989.[4] M. Talagrand, \\xe2\\x80\\x9cMajorizing measures: the generic chaining,\\xe2\\x80\\x9d The Annals of Probability, vol. 24,no. 3, pp. 1049\\xe2\\x80\\x931103, 1996.[5] D. A. McAllester, \\xe2\\x80\\x9cPAC-Bayesian stochastic model selection,\\xe2\\x80\\x9d Machine Learning, vol. 51,pp. 5\\xe2\\x80\\x9321, 2003.[6] O. Bousquet and A. Elisseeff, \\xe2\\x80\\x9cStability and generalization,\\xe2\\x80\\x9d The Journal of Machine LearningResearch (JMLR), vol. 2, pp. 499\\xe2\\x80\\x93526, 2002.[7] P. L. Bartlett and S. Mendelson, \\xe2\\x80\\x9cRademacher and gaussian complexities: Risk bounds andstructural results,\\xe2\\x80\\x9d The Journal of Machine Learning Research (JMLR), vol. 3, pp. 463\\xe2\\x80\\x93482,2002.[8] J.-Y. Audibert and O. Bousquet, \\xe2\\x80\\x9cCombining PAC-Bayesian and generic chaining bounds,\\xe2\\x80\\x9dThe Journal of Machine Learning Research (JMLR), vol. 8, pp. 863\\xe2\\x80\\x93889, 2007.[9] H. Xu and S. Mannor, \\xe2\\x80\\x9cRobustness and generalization,\\xe2\\x80\\x9d Machine learning, vol. 86, no. 3,pp. 391\\xe2\\x80\\x93423, 2012.[10] A. Elisseeff, M. Pontil, et al., \\xe2\\x80\\x9cLeave-one-out error and stability of learning algorithms withapplications,\\xe2\\x80\\x9d NATO-ASI series on Learning Theory and Practice Science Series Sub SeriesIII: Computer and Systems Sciences, 2002.[11] S. Kutin and P. Niyogi, \\xe2\\x80\\x9cAlmost-everywhere algorithmic stability and generalization error,\\xe2\\x80\\x9d inProceedings of the Eighteenth conference on Uncertainty in artificial intelligence (UAI), 2002.[12] T. Poggio, R. Rifkin, S. Mukherjee, and P. Niyogi, \\xe2\\x80\\x9cGeneral conditions for predictivity inlearning theory,\\xe2\\x80\\x9d Nature, vol. 428, pp. 419\\xe2\\x80\\x93422, 2004.[13] M. Kearns and D. Ron, \\xe2\\x80\\x9cAlgorithmic stability and sanity-check bounds for leave-one-out crossvalidation,\\xe2\\x80\\x9d Neural Computation, vol. 11, no. 6, pp. 1427\\xe2\\x80\\x931453, 1999.[14] S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan, \\xe2\\x80\\x9cLearnability, stability and uniform convergence,\\xe2\\x80\\x9d The Journal of Machine Learning Research (JMLR), vol. 11, pp. 2635\\xe2\\x80\\x932670, 2010.[15] L. Devroye, L. Gyo\\xcc\\x88rfi, and G. Lugosi, A probabilistic theory of pattern recognition. Springer,1996.[16] V. Vapnik and O. Chapelle, \\xe2\\x80\\x9cBounds on error expectation for support vector machines,\\xe2\\x80\\x9d NeuralComputation, vol. 12, no. 9, pp. 2013\\xe2\\x80\\x932036, 2000.[17] H. Robbins, \\xe2\\x80\\x9cA remark on stirling\\xe2\\x80\\x99s formula,\\xe2\\x80\\x9d American Mathematical Monthly, pp. 26\\xe2\\x80\\x9329,1955.[18] T. M. Cover and J. A. Thomas, Elements of information theory. Wiley & Sons, 1991.[19] T. Downs, K. E. Gates, and A. Masters, \\xe2\\x80\\x9cExact simplification of support vector solutions,\\xe2\\x80\\x9dJMLR, vol. 2, pp. 293\\xe2\\x80\\x93297, 2002.[20] S. Wager, S. Wang, and P. S. Liang, \\xe2\\x80\\x9cDropout training as adaptive regularization,\\xe2\\x80\\x9d in NIPS,pp. 351\\xe2\\x80\\x93359, 2013.[21] S. M. Stigler, The history of statistics: The measurement of uncertainty before 1900. HarvardUniversity Press, 1986.[22] P. Diaconis and S. Zabell, \\xe2\\x80\\x9cClosed form summation for classical distributions: Variations on atheme of de moivre,\\xe2\\x80\\x9d Statlstlcal Science, vol. 6, no. 3, pp. 284\\xe2\\x80\\x93302, 1991.[23] S. Shalev-Shwartz and S. Ben-David, Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014.9\\x0c']\n"
     ]
    }
   ],
   "source": [
    "#Look at full data\n",
    "limit = 3\n",
    "l = 0\n",
    "for d in data:\n",
    "    print d\n",
    "    l += 1\n",
    "    if l > limit: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title\n",
      "Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing\n",
      "Learning with Symmetric Label Noise: The Importance of Being Unhinged\n",
      "Algorithmic Stability and Uniform Generalization\n"
     ]
    }
   ],
   "source": [
    "#Look at one column of the data\n",
    "limit = 3\n",
    "l = 0\n",
    "for d in data:\n",
    "    print d[1] #title\n",
    "    l += 1\n",
    "    if l > limit: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing', 'Learning with Symmetric Label Noise: The Importance of Being Unhinged', 'Algorithmic Stability and Uniform Generalization', 'Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models', 'Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling', 'Robust Portfolio Optimization', 'Logarithmic Time Online Multiclass prediction', 'Planar Ultrametrics for Image Segmentation', 'Expressing an Image Stream with a Sequence of Natural Sentences', 'Parallel Correlation Clustering on Big Graphs', 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks', 'Space-Time Local Embeddings', 'A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements', 'Smooth Interactive Submodular Set Cover', 'Galileo: Perceiving Physical Object Properties by Integrating a Physics Engine with Deep Learning', 'On the Pseudo-Dimension of Nearly Optimal Auctions', 'Unlocking neural population non-stationarities using hierarchical dynamics models', 'Bayesian Manifold Learning: The Locally Linear Latent Variable Model (LL-LVM)', 'Color Constancy by Learning to Predict Chromaticity from Luminance', 'Fast and Accurate Inference of Plackett\\xe2\\x80\\x93Luce Models', 'Probabilistic Line Searches for Stochastic Optimization', 'Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets', 'Where are they looking?', 'The Pareto Regret Frontier for Bandits', 'On the Limitation of Spectral Methods: From the Gaussian Hidden Clique Problem to Rank-One Perturbations of Gaussian Tensors', \"Measuring Sample Quality with Stein's Method\", 'Bidirectional Recurrent Convolutional Networks for Multi-Frame Super-Resolution', 'Bounding errors of Expectation-Propagation', 'A fast, universal algorithm to learn parametric nonlinear embeddings', 'Texture Synthesis Using Convolutional Neural Networks', 'Extending Gossip Algorithms to Distributed Estimation of U-statistics', 'Streaming, Distributed Variational Inference for Bayesian Nonparametrics', 'Learning visual biases from human imagination', 'Smooth and Strong: MAP Inference with Linear Convergence', 'Copeland Dueling Bandits', 'Optimal Ridge Detection using Coverage Risk', 'Top-k Multiclass SVM', 'Policy Evaluation Using the \\xce\\xa9-Return', 'Orthogonal NMF through Subspace Exploration', 'Stochastic Online Greedy Learning with Semi-bandit Feedbacks', 'Deeply Learning the Messages in Message Passing Inference', 'Synaptic Sampling: A Bayesian Approach to Neural Network Plasticity and Rewiring', 'Accelerated Proximal Gradient Methods for Nonconvex Programming', 'Approximating Sparse PCA from Incomplete Data', 'Nonparametric von Mises Estimators for Entropies, Divergences and Mutual Informations', 'Column Selection via Adaptive Sampling', 'HONOR: Hybrid Optimization for NOn-convex Regularized problems', '3D Object Proposals for Accurate Object Class Detection', 'Algorithms with Logarithmic or Sublinear Regret for  Constrained Contextual Bandits', 'Tensorizing Neural Networks', 'Parallelizing MCMC with Random Partition Trees', 'A Reduced-Dimension fMRI Shared Response Model', 'Spectral Learning of Large Structured HMMs for Comparative Epigenomics', 'Individual Planning in Infinite-Horizon Multiagent Settings: Inference, Structure and Scalability', 'Estimating Mixture Models via Mixtures of Polynomials', 'On the Global Linear Convergence of Frank-Wolfe Optimization Variants', 'Deep Knowledge Tracing', 'Rethinking LDA: Moment Matching for Discrete ICA', 'Efficient Compressive Phase Retrieval with Constrained Sensing Vectors', 'Barrier Frank-Wolfe for Marginal Inference', 'Learning Theory and Algorithms for Forecasting Non-stationary Time Series', 'Compressive spectral embedding: sidestepping the SVD', 'A Nonconvex Optimization Framework for Low Rank Matrix Estimation', 'Automatic Variational Inference in Stan', 'Attention-Based Models for Speech Recognition', 'Closed-form Estimators for High-dimensional Generalized Linear Models', 'Online F-Measure Optimization', 'Online Rank Elicitation for Plackett-Luce: A Dueling Bandits Approach', 'M-Best-Diverse Labelings for Submodular Energies and Beyond', 'Tractable Bayesian Network Structure Learning with Bounded Vertex Cover Number', 'Learning Large-Scale Poisson DAG Models based on OverDispersion Scoring', 'Training Restricted Boltzmann Machine via the \\xef\\xbf\\xbcThouless-Anderson-Palmer free energy', 'Character-level Convolutional Networks for Text Classification', 'Robust Feature-Sample Linear Discriminant Analysis for Brain Disorders Diagnosis', 'Black-box optimization of noisy functions with unknown smoothness', 'Recovering Communities in the General Stochastic Block Model Without Knowing the Parameters', 'Deep learning with Elastic Averaging SGD', 'Monotone k-Submodular Function Maximization with Size Constraints', 'Active Learning from Weak and Strong Labelers', 'On the Optimality of Classifier Chain for Multi-label Classification', 'Robust Regression via Hard Thresholding', 'Sparse Local Embeddings for Extreme Multi-label Classification', 'Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems', 'A Framework for Individualizing Predictions of Disease Trajectories by Exploiting Multi-Resolution Structure', 'Subspace Clustering with Irrelevant Features via Robust Dantzig Selector', 'Sparse PCA via Bipartite Matchings', 'Fast Randomized Kernel Ridge Regression with Statistical Guarantees', 'Online Learning for Adversaries with Memory: Price of Past Mistakes', 'Convolutional spike-triggered covariance analysis for neural subunit models', 'Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting', 'GAP Safe screening rules for sparse multi-task and multi-class models', 'Empirical Localization of Homogeneous Divergences on Discrete Sample Spaces', 'Statistical Model Criticism using Kernel Two Sample Tests', 'Precision-Recall-Gain Curves: PR Analysis Done Right', 'A Generalization of Submodular Cover via the Diminishing Return Property on the Integer Lattice', 'Bidirectional Recurrent Neural Networks as Generative Models', 'Quartz: Randomized Dual Coordinate Ascent with Arbitrary Sampling', 'Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing Parameter Sets', 'Hessian-free Optimization for Learning Deep Multidimensional Recurrent Neural Networks', 'Large-scale probabilistic predictors with and without guarantees of validity', 'Shepard Convolutional Neural Networks', 'Matrix Manifold Optimization for Gaussian Mixtures', 'Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding', 'Parallel Recursive Best-First AND/OR Search for Exact MAP Inference in Graphical Models', 'Convolutional Neural Networks with Intra-Layer Recurrent Connections for Scene Labeling', 'Bounding the Cost of Search-Based Lifted Inference', 'Gradient-free Hamiltonian Monte Carlo with Efficient Kernel Exponential Families', 'Linear Multi-Resource Allocation with Semi-Bandit Feedback', 'Unsupervised Learning by Program Synthesis', 'Enforcing balance allows local supervised learning in spiking recurrent networks', 'Fast and Guaranteed Tensor Decomposition via Sketching', 'Differentially private subspace clustering', 'Predtron: A Family of Online Algorithms for General Prediction Problems', 'Weighted Theta Functions and Embeddings with Applications to Max-Cut, Clustering and Summarization', 'SGD Algorithms based on Incomplete U-statistics: Large-Scale Minimization of Empirical Risk', 'On Top-k Selection in Multi-Armed Bandits and Hidden Bipartite Graphs', 'The Brain Uses Reliability of Stimulus Information when Making Perceptual Decisions', 'Fast Classification Rates for High-dimensional Gaussian Generative Models', 'Fast Distributed k-Center Clustering with Outliers on Massive Data', 'Human Memory Search as Initial-Visit Emitting Random Walk', 'Non-convex Statistical Optimization for Sparse Tensor Graphical Model', 'Convergence Rates of Active Learning for Maximum Likelihood Estimation', 'Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis', 'Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets', 'Backpropagation for Energy-Efficient Neuromorphic Computing', 'Alternating Minimization for Regression Problems with Vector-valued Outputs', 'Learning both Weights and Connections for Efficient Neural Network', 'Optimal Rates for Random Fourier Features', 'The Population Posterior and Bayesian Modeling on Streams', 'Frank-Wolfe Bayesian Quadrature: Probabilistic Integration with Theoretical Guarantees', 'Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks', 'Unified View of Matrix Completion under General Structural Constraints', 'Efficient Output Kernel Learning for Multiple Tasks', 'Scalable Adaptation of State Complexity for Nonparametric Hidden Markov Models', 'Variational Consensus Monte Carlo', \"Newton-Stein Method: A Second Order Method for GLMs via Stein's Lemma\", 'Practical and Optimal LSH for Angular Distance', 'Learning to Linearize Under Uncertainty', 'Finite-Time Analysis of Projected Langevin Monte Carlo', 'Deep Visual Analogy-Making', 'Matrix Completion from Fewer Entries: Spectral Detectability and Rank Estimation', 'Online Learning with Adversarial Delays', 'Multi-Layer Feature Reduction for Tree Structured Group Lasso via Hierarchical Projection', 'Minimum Weight Perfect Matching via Blossom Belief Propagation', 'Efficient Thompson Sampling for Online \\xef\\xbf\\xbcMatrix-Factorization Recommendation', 'Improved Iteration Complexity Bounds of Cyclic Block Coordinate Descent for Convex Problems', 'Lifted Symmetry Detection and Breaking for MAP Inference', 'Evaluating the statistical significance of biclusters', 'Discriminative Robust Transformation Learning', 'Bandits with Unobserved Confounders: A Causal Approach', 'Scalable Semi-Supervised Aggregation of Classifiers', 'Online Learning with Gaussian Payoffs and Side Observations', 'Private Graphon Estimation for Sparse Graphs', 'SubmodBoxes: Near-Optimal Search for a Set of Diverse Object Proposals', 'Fast Second Order Stochastic Backpropagation for Variational Inference', 'Randomized Block Krylov Methods for Stronger and Faster Approximate Singular Value Decomposition', 'Cross-Domain Matching for Bag-of-Words Data via Kernel Embeddings of Latent Distributions', 'Scalable Inference for Gaussian Process Models with Black-Box Likelihoods', 'Fast Bidirectional Probability Estimation in Markov Models', 'Probabilistic Variational Bounds for Graphical Models', 'Linear Response Methods for Accurate Covariance Estimates from Mean Field Variational Bayes', 'Combinatorial Cascading Bandits', 'Mixing Time Estimation in Reversible Markov Chains from a Single Sample Path', 'Policy Gradient for Coherent Risk Measures', 'Fast Rates for Exp-concave Empirical Risk Minimization', 'Deep Generative Image Models using a \\xef\\xbf\\xbcLaplacian Pyramid of Adversarial Networks', 'Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation', 'Equilibrated adaptive learning rates for non-convex optimization', 'BACKSHIFT: Learning causal cyclic graphs from unknown shift interventions', 'Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach', \"Asynchronous stochastic convex optimization: the noise is in the noise and SGD don't care\", 'Lifelong Learning with Non-i.i.d. Tasks', 'Optimal Linear Estimation under Unknown Nonlinear Transform', 'Learning with Group Invariant Features: A Kernel Perspective.', 'Regularized EM Algorithms: A Unified Framework and Statistical Guarantees', 'Adaptive Stochastic Optimization: From Sets to Paths', 'Beyond Convexity: Stochastic Quasi-Convex Optimization', 'A Tractable Approximation to Optimal Point Process Filtering: Application to Neural Encoding', 'Sum-of-Squares Lower Bounds for Sparse PCA', 'Max-Margin Majority Voting for Learning from Crowds', 'Learning with Incremental Iterative Regularization', 'Halting in Random Walk Kernels', 'MCMC for Variationally Sparse Gaussian Processes', 'Less is More: Nystr\\xc3\\xb6m Computational Regularization', 'Infinite Factorial Dynamical Model', 'Regularization Path of Cross-Validation Error Lower Bounds', 'Attractor Network Dynamics Enable Preplay and Rapid Path Planning in Maze\\xe2\\x80\\x93like Environments', 'Teaching Machines to Read and Comprehend', 'Principal Differences Analysis: Interpretable Characterization of Differences between Distributions', 'When are Kalman-Filter Restless Bandits Indexable?', 'Segregated Graphs and Marginals of Chain Graph Models', 'Efficient Non-greedy Optimization of Decision Trees', 'Probabilistic Curve Learning: Coulomb Repulsion and the Electrostatic Gaussian Process', 'Inverse Reinforcement Learning with Locally Consistent Reward Functions', 'Communication Complexity of Distributed Convex Learning and Optimization', 'End-to-end Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture', 'Subset Selection by Pareto Optimization', 'On the Accuracy of Self-Normalized Log-Linear Models', 'Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial Monitoring', 'Is Approval Voting Optimal Given Approval Votes?', 'Regressive Virtual Metric Learning', 'Analysis of Robust PCA via Local Incoherence', 'Learning to Transduce with Unbounded Memory', 'Max-Margin Deep Generative Models', 'Spherical Random Features for Polynomial Kernels', 'Rectified Factor Networks', 'Learning Bayesian Networks with Thousands of Variables', 'Matrix Completion Under Monotonic Single Index Models', 'Visalogy: Answering Visual Analogy Questions', 'Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models', 'Streaming Min-max Hypergraph Partitioning', 'Collaboratively Learning Preferences from Ordinal Data', 'Biologically Inspired Dynamic Textures for Probing Motion Perception', 'Generative Image Modeling Using Spatial LSTMs', 'Robust PCA with compressed data', 'Sampling from Probabilistic Submodular Models', 'COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution', 'Supervised Learning for Dynamical System Learning', 'Regret-Based Pruning in Extensive-Form Games', 'Fast Two-Sample Testing with Analytic Representations of Probability Measures', 'Learning to Segment Object Candidates', 'GP Kernels for Cross-Spectrum Analysis', 'Secure Multi-party Differential Privacy', 'Spatial Transformer Networks', 'Anytime Influence Bounds and the Explosive Behavior of Continuous-Time Diffusion Networks', 'Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms', 'High-dimensional neural spike train analysis with generalized count linear dynamical systems', 'Learning with a Wasserstein Loss', 'b-bit Marginal Regression', 'Natural Neural Networks', 'Optimization Monte Carlo: Efficient and Embarrassingly Parallel Likelihood-Free Inference', 'Adaptive Primal-Dual Splitting Methods for Statistical Learning and Image Processing', 'On some provably correct cases of variational inference for topic models', 'Collaborative Filtering with Graph Information: Consistency and Scalable Methods', 'Combinatorial Bandits Revisited', 'Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning', 'A Structural Smoothing Framework For Robust Graph Comparison', 'Competitive Distribution Estimation: Why is Good-Turing Good', 'Efficient Learning by Directed Acyclic Graph For Resource Constrained Prediction', 'A hybrid sampler for Poisson-Kingman mixture models', 'An Active Learning Framework using Sparse-Graph Codes for Sparse Polynomials and Graph Sketching', 'Local Smoothness in Variance Reduced Optimization', 'Saliency, Scale and Information: Towards a Unifying Theory', 'Fighting Bandits with a New Kind of Smoothness', 'Beyond Sub-Gaussian Measurements: High-Dimensional Structured Estimation with Sub-Exponential Designs', 'Spectral Norm Regularization of Orthonormal Representations for Graph Transduction', 'Convolutional Networks on Graphs for Learning Molecular Fingerprints', 'Mixed Robust/Average Submodular Partitioning: Fast Algorithms, Guarantees, and Applications', 'Tractable Learning for Complex Probability Queries', 'StopWasting My Gradients: Practical SVRG', 'Mind the Gap: A Generative Approach to Interpretable Feature Selection and Extraction', 'A Normative Theory of Adaptive Dimensionality Reduction in Neural Networks', 'On the Convergence of Stochastic Gradient MCMC Algorithms with High-Order Integrators', 'Learning structured densities via infinite dimensional exponential families', 'Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question', 'Variance Reduced Stochastic Gradient Descent with Neighbors', 'Sample Efficient Path Integral Control under Uncertainty', 'Stochastic Expectation Propagation', 'Exactness of Approximate MAP Inference in Continuous MRFs', 'Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients', 'Generalization in Adaptive Data Analysis and Holdout Reuse', 'Market Scoring Rules Act As Opinion Pools For Risk-Averse Agents', 'Sparse Linear Programming via Primal and Dual Augmented Coordinate Descent', 'Training Very Deep Networks', 'Bayesian Active Model Selection with an Application to Automated Audiometry', 'Particle Gibbs for Infinite Hidden Markov Models', 'Learning spatiotemporal trajectories from manifold-valued longitudinal data', 'A Bayesian Framework for Modeling Confidence in Perceptual Decision Making', 'Path-SGD: Path-Normalized Optimization in Deep Neural Networks', 'On the consistency theory of high dimensional variable screening', 'End-To-End Memory Networks', 'Spectral Representations for Convolutional Neural Networks', 'Online Gradient Boosting', 'Deep Temporal Sigmoid Belief Networks for Sequence Modeling', 'Recognizing retinal ganglion cells in the dark', 'A Theory of Decision Making Under Dynamic Context', 'A Gaussian Process Model of Quasar Spectral Energy Distributions', 'Hidden Technical Debt in Machine Learning Systems', 'Local Causal Discovery of Direct Causes and Effects', 'High Dimensional EM Algorithm: Statistical Optimization and Asymptotic Normality', 'Revenue Optimization against Strategic Buyers', 'Deep Convolutional Inverse Graphics Network', 'Sparse and Low-Rank Tensor Decomposition', 'Minimax Time Series Prediction', 'Differentially Private Learning of Structured Discrete Distributions', 'Sample Complexity of Learning Mahalanobis Distance Metrics', 'Learning Wake-Sleep Recurrent Attention Models', 'Robust Gaussian Graphical Modeling with the Trimmed Graphical Lasso', 'Testing Closeness With Unequal Sized Samples', 'Estimating Jaccard Index with Missing Observations: A Matrix Calibration Approach', 'Neural Adaptive Sequential Monte Carlo', 'Local Expectation Gradients for Black Box Variational Inference', 'On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants', 'NEXT: A System for Real-World Development, Evaluation, and Application of Active Learning', 'Super-Resolution Off the Grid', 'Taming the Wild: A Unified Analysis of Hogwild-Style Algorithms', 'The Return of the Gating Network: Combining Generative Models and Discriminative Training in Natural Image Priors', 'Pointer Networks', 'Associative Memory via a Sparse Recovery Model', 'Robust Spectral Inference for Joint Stochastic Matrix Factorization', 'Fast, Provable Algorithms for Isotonic Regression in all L_p-norms', 'Adversarial Prediction Games for Multivariate Losses', 'Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization', 'Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images', 'Efficient and Parsimonious Agnostic Active Learning', 'Softstar: Heuristic-Guided Probabilistic Inference', 'Grammar as a Foreign Language', 'Regularization-Free Estimation in Trace Regression with Symmetric Positive Semidefinite Matrices', 'Winner-Take-All Autoencoders', 'Deep Poisson Factor Modeling', 'Bayesian Optimization with Exponential Convergence', 'Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning', 'Learning with Relaxed Supervision', \"Subsampled Power Iteration: a Unified Algorithm for Block Models and Planted CSP's\", 'Accelerated Mirror Descent in Continuous and Discrete Time', 'The Human Kernel', 'Action-Conditional Video Prediction using Deep Networks in Atari Games', 'A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA', 'Distributed Submodular Cover: Succinctly Summarizing Massive Data', 'Community Detection via Measure Space Embedding', 'Basis refinement strategies for linear value function approximation in MDPs', 'Structured Estimation with Atomic Norms: General Bounds and Applications', 'A Complete Recipe for Stochastic Gradient MCMC', 'Bandit Smooth Convex Optimization: Improving the Bias-Variance Tradeoff', 'Online Prediction at the Limit of Zero Temperature', 'Learning Continuous Control Policies by Stochastic Value Gradients', 'Exploring Models and Data for Image Question Answering', 'Efficient and Robust Automated Machine Learning', 'Preconditioned Spectral Descent for Deep Learning', 'A Recurrent Latent Variable Model for Sequential Data', 'Fast Convergence of Regularized Learning in Games', 'Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation', 'Reflection, Refraction, and Hamiltonian Monte Carlo', 'The Consistency of Common Neighbors for Link Prediction in Stochastic Blockmodels', 'Nearly Optimal Private LASSO', 'Convergence Analysis of Prediction Markets via Randomized Subspace Descent', 'The Poisson Gamma Belief Network', 'Convergence rates of sub-sampled Newton methods', 'No-Regret Learning in Bayesian Games', 'Statistical Topological Data Analysis - A Kernel Perspective', 'Semi-supervised Sequence Learning', 'Structured Transforms for Small-Footprint Deep Learning', 'Rapidly Mixing Gibbs Sampling for a Class of Factor Graphs Using Hierarchy Width', 'Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm', 'Sample Complexity Bounds for Iterative Stochastic Policy Optimization', 'BinaryConnect: Training Deep Neural Networks with binary weights during propagations', 'Interactive Control of Diverse Complex Characters with Neural Networks', 'Submodular Hamming Metrics', 'A Universal Primal-Dual Convex Optimization Framework', 'Learning From Small Samples: An Analysis of Simple Decision Heuristics', 'Explore no more: Improved high-probability regret bounds for non-stochastic bandits', 'Fast and Memory Optimal Low-Rank Matrix Approximation', 'Learnability of Influence in Networks', 'Learning Causal Graphs with Small Interventions', 'Information-theoretic lower bounds for convex optimization with erroneous oracles', 'Fixed-Length Poisson MRF: Adding Dependencies to the Multinomial', 'Large-Scale Bayesian Multi-Label Learning via Topic-Based Label Embeddings', 'The Self-Normalized Estimator for Counterfactual Learning', 'Fast Lifted MAP Inference via Partitioning', 'Data Generation as Sequential Decision Making', 'On Elicitation Complexity', 'Decomposition Bounds for Marginal MAP', 'Discrete R\\xc3\\xa9nyi Classifiers', 'A class of network models recoverable by spectral clustering', 'Skip-Thought Vectors', 'Rate-Agnostic (Causal) Structure Learning', 'Principal Geodesic Analysis for Probability Measures under the Optimal Transport Metric', 'Consistent Multilabel Classification', 'Parallel Predictive Entropy Search for Batch Global Optimization of Expensive Objective Functions', 'Cornering Stationary and Restless Mixing Bandits with Remix-UCB', 'Semi-Supervised Factored Logistic Regression for High-Dimensional Neuroimaging Data', 'Gaussian Process Random Fields', 'M-Statistic for Kernel Change-Point Detection', 'Adaptive Online Learning', 'A Universal Catalyst for First-Order Optimization', 'Inference for determinantal point processes without spectral knowledge', 'Kullback-Leibler Proximal Variational Inference', 'Semi-Proximal Mirror-Prox for Nonsmooth Composite Minimization', 'LASSO with Non-linear Measurements is Equivalent to One With Linear Measurements', 'From random walks to distances on unweighted graphs', 'Bayesian dark knowledge', 'Matrix Completion with Noisy Side Information', 'Dependent Multinomial Models Made Easy: Stick-Breaking with the Polya-gamma Augmentation', 'On-the-Job Learning with Bayesian Decision Theory', 'Calibrated Structured Prediction', 'Learning Structured Output Representation using Deep Conditional Generative Models', 'Time-Sensitive Recommendation From Recurrent User Activities', 'Learning Stationary Time Series using Gaussian Processes with Nonparametric Kernels', 'A Market Framework for Eliciting Private Data', 'Lifted Inference Rules With Constraints', 'Gradient Estimation Using Stochastic Computation Graphs', 'Model-Based Relative Entropy Stochastic Search', 'Semi-supervised Learning with Ladder Networks', 'Embedding Inference for Structured Multilabel Prediction', 'Copula variational inference', 'Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary Prediction', 'A Dual Augmented Block Minimization Framework for Learning with Limited Memory', 'Optimal Testing for Properties of Distributions', 'Efficient Learning of Continuous-Time Hidden Markov Models for Disease Progression', 'Expectation Particle Belief Propagation', 'Latent Bayesian melding for integrating individual and population models', 'Distributionally Robust Logistic Regression', 'Variational Dropout and the Local Reparameterization Trick']\n"
     ]
    }
   ],
   "source": [
    "#LOAD ONE COLUMN OF THE DATA INTO A DOCUMENTS ARRAY\n",
    "documents = []\n",
    "\n",
    "limit = 1000\n",
    "l = 0\n",
    "for d in data:\n",
    "    documents.append(d[1]) #title\n",
    "    l += 1\n",
    "    if l > limit: break\n",
    "\n",
    "documents = documents[1:]\n",
    "print documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['double', 'nothing', 'multiplicative', 'incentive', 'mechanisms', 'crowdsourcing'], ['learning', 'symmetric', 'label', 'noise', 'importance', 'being', 'unhinged'], ['algorithmic', 'stability', 'uniform', 'generalization'], ['adaptive', 'lowcomplexity', 'sequential', 'inference', 'dirichlet', 'process', 'mixture', 'models'], ['covariancecontrolled', 'adaptive', 'langevin', 'thermostat', 'largescale', 'bayesian', 'sampling'], ['robust', 'portfolio', 'optimization'], ['logarithmic', 'time', 'online', 'multiclass', 'prediction'], ['planar', 'ultrametrics', 'image', 'segmentation'], ['expressing', 'an', 'image', 'stream', 'sequence', 'natural', 'sentences'], ['parallel', 'correlation', 'clustering', 'big', 'graphs'], ['faster', 'rcnn', 'towards', 'realtime', 'object', 'detection', 'region', 'proposal', 'networks'], ['spacetime', 'local', 'embeddings'], ['convergent', 'gradient', 'descent', 'algorithm', 'rank', 'minimization', 'semidefinite', 'programming', 'random', 'linear', 'measurements'], ['smooth', 'interactive', 'submodular', 'set', 'cover'], ['galileo', 'perceiving', 'physical', 'object', 'properties', 'integrating', 'physics', 'engine', 'deep', 'learning'], ['pseudodimension', 'nearly', 'optimal', 'auctions'], ['unlocking', 'neural', 'population', 'nonstationarities', 'hierarchical', 'dynamics', 'models'], ['bayesian', 'manifold', 'learning', 'locally', 'linear', 'latent', 'variable', 'model', 'lllvm'], ['color', 'constancy', 'learning', 'predict', 'chromaticity', 'luminance'], ['fast', 'accurate', 'inference', 'plackett\\xe2\\x80\\x93luce', 'models'], ['probabilistic', 'line', 'searches', 'stochastic', 'optimization'], ['inferring', 'algorithmic', 'patterns', 'stackaugmented', 'recurrent', 'nets'], ['where', 'they', 'looking'], ['pareto', 'regret', 'frontier', 'bandits'], ['limitation', 'spectral', 'methods', 'gaussian', 'hidden', 'clique', 'problem', 'rankone', 'perturbations', 'gaussian', 'tensors'], ['measuring', 'sample', 'quality', 'steins', 'method'], ['bidirectional', 'recurrent', 'convolutional', 'networks', 'multiframe', 'superresolution'], ['bounding', 'errors', 'expectationpropagation'], ['fast', 'universal', 'algorithm', 'learn', 'parametric', 'nonlinear', 'embeddings'], ['texture', 'synthesis', 'convolutional', 'neural', 'networks'], ['extending', 'gossip', 'algorithms', 'distributed', 'estimation', 'ustatistics'], ['streaming', 'distributed', 'variational', 'inference', 'bayesian', 'nonparametrics'], ['learning', 'visual', 'biases', 'human', 'imagination'], ['smooth', 'strong', 'map', 'inference', 'linear', 'convergence'], ['copeland', 'dueling', 'bandits'], ['optimal', 'ridge', 'detection', 'coverage', 'risk'], ['topk', 'multiclass', 'svm'], ['policy', 'evaluation', '\\xce\\xa9return'], ['orthogonal', 'nmf', 'through', 'subspace', 'exploration'], ['stochastic', 'online', 'greedy', 'learning', 'semibandit', 'feedbacks'], ['deeply', 'learning', 'messages', 'message', 'passing', 'inference'], ['synaptic', 'sampling', 'bayesian', 'approach', 'neural', 'network', 'plasticity', 'rewiring'], ['accelerated', 'proximal', 'gradient', 'methods', 'nonconvex', 'programming'], ['approximating', 'sparse', 'pca', 'incomplete', 'data'], ['nonparametric', 'von', 'mises', 'estimators', 'entropies', 'divergences', 'mutual', 'informations'], ['column', 'selection', 'adaptive', 'sampling'], ['honor', 'hybrid', 'optimization', 'nonconvex', 'regularized', 'problems'], ['3d', 'object', 'proposals', 'accurate', 'object', 'class', 'detection'], ['algorithms', 'logarithmic', 'sublinear', 'regret', 'constrained', 'contextual', 'bandits'], ['tensorizing', 'neural', 'networks'], ['parallelizing', 'mcmc', 'random', 'partition', 'trees'], ['reduceddimension', 'fmri', 'shared', 'response', 'model'], ['spectral', 'learning', 'large', 'structured', 'hmms', 'comparative', 'epigenomics'], ['individual', 'planning', 'infinitehorizon', 'multiagent', 'settings', 'inference', 'structure', 'scalability'], ['estimating', 'mixture', 'models', 'mixtures', 'polynomials'], ['global', 'linear', 'convergence', 'frankwolfe', 'optimization', 'variants'], ['deep', 'knowledge', 'tracing'], ['rethinking', 'lda', 'moment', 'matching', 'discrete', 'ica'], ['efficient', 'compressive', 'phase', 'retrieval', 'constrained', 'sensing', 'vectors'], ['barrier', 'frankwolfe', 'marginal', 'inference'], ['learning', 'theory', 'algorithms', 'forecasting', 'nonstationary', 'time', 'series'], ['compressive', 'spectral', 'embedding', 'sidestepping', 'svd'], ['nonconvex', 'optimization', 'framework', 'low', 'rank', 'matrix', 'estimation'], ['automatic', 'variational', 'inference', 'stan'], ['attentionbased', 'models', 'speech', 'recognition'], ['closedform', 'estimators', 'highdimensional', 'generalized', 'linear', 'models'], ['online', 'fmeasure', 'optimization'], ['online', 'rank', 'elicitation', 'plackettluce', 'dueling', 'bandits', 'approach'], ['mbestdiverse', 'labelings', 'submodular', 'energies', 'beyond'], ['tractable', 'bayesian', 'network', 'structure', 'learning', 'bounded', 'vertex', 'cover', 'number'], ['learning', 'largescale', 'poisson', 'dag', 'models', 'based', 'overdispersion', 'scoring'], ['training', 'restricted', 'boltzmann', 'machine', '\\xef\\xbf\\xbcthoulessandersonpalmer', 'free', 'energy'], ['characterlevel', 'convolutional', 'networks', 'text', 'classification'], ['robust', 'featuresample', 'linear', 'discriminant', 'analysis', 'brain', 'disorders', 'diagnosis'], ['blackbox', 'optimization', 'noisy', 'functions', 'unknown', 'smoothness'], ['recovering', 'communities', 'general', 'stochastic', 'block', 'model', 'without', 'knowing', 'parameters'], ['deep', 'learning', 'elastic', 'averaging', 'sgd'], ['monotone', 'ksubmodular', 'function', 'maximization', 'size', 'constraints'], ['active', 'learning', 'weak', 'strong', 'labelers'], ['optimality', 'classifier', 'chain', 'multilabel', 'classification'], ['robust', 'regression', 'hard', 'thresholding'], ['sparse', 'local', 'embeddings', 'extreme', 'multilabel', 'classification'], ['solving', 'random', 'quadratic', 'systems', 'equations', 'is', 'nearly', 'easy', 'solving', 'linear', 'systems'], ['framework', 'individualizing', 'predictions', 'disease', 'trajectories', 'exploiting', 'multiresolution', 'structure'], ['subspace', 'clustering', 'irrelevant', 'features', 'robust', 'dantzig', 'selector'], ['sparse', 'pca', 'bipartite', 'matchings'], ['fast', 'randomized', 'kernel', 'ridge', 'regression', 'statistical', 'guarantees'], ['online', 'learning', 'adversaries', 'memory', 'price', 'past', 'mistakes'], ['convolutional', 'spiketriggered', 'covariance', 'analysis', 'neural', 'subunit', 'models'], ['convolutional', 'lstm', 'network', 'machine', 'learning', 'approach', 'precipitation', 'nowcasting'], ['gap', 'safe', 'screening', 'rules', 'sparse', 'multitask', 'multiclass', 'models'], ['empirical', 'localization', 'homogeneous', 'divergences', 'discrete', 'sample', 'spaces'], ['statistical', 'model', 'criticism', 'kernel', 'two', 'sample', 'tests'], ['precisionrecallgain', 'curves', 'pr', 'analysis', 'done', 'right'], ['generalization', 'submodular', 'cover', 'diminishing', 'return', 'property', 'integer', 'lattice'], ['bidirectional', 'recurrent', 'neural', 'networks', 'generative', 'models'], ['quartz', 'randomized', 'dual', 'coordinate', 'ascent', 'arbitrary', 'sampling'], ['maximum', 'likelihood', 'learning', 'arbitrary', 'treewidth', 'fastmixing', 'parameter', 'sets'], ['hessianfree', 'optimization', 'learning', 'deep', 'multidimensional', 'recurrent', 'neural', 'networks'], ['largescale', 'probabilistic', 'predictors', 'without', 'guarantees', 'validity'], ['shepard', 'convolutional', 'neural', 'networks'], ['matrix', 'manifold', 'optimization', 'gaussian', 'mixtures'], ['semisupervised', 'convolutional', 'neural', 'networks', 'text', 'categorization', 'region', 'embedding'], ['parallel', 'recursive', 'bestfirst', 'andor', 'search', 'exact', 'map', 'inference', 'graphical', 'models'], ['convolutional', 'neural', 'networks', 'intralayer', 'recurrent', 'connections', 'scene', 'labeling'], ['bounding', 'cost', 'searchbased', 'lifted', 'inference'], ['gradientfree', 'hamiltonian', 'monte', 'carlo', 'efficient', 'kernel', 'exponential', 'families'], ['linear', 'multiresource', 'allocation', 'semibandit', 'feedback'], ['unsupervised', 'learning', 'program', 'synthesis'], ['enforcing', 'balance', 'allows', 'local', 'supervised', 'learning', 'spiking', 'recurrent', 'networks'], ['fast', 'guaranteed', 'tensor', 'decomposition', 'sketching'], ['differentially', 'private', 'subspace', 'clustering'], ['predtron', 'family', 'online', 'algorithms', 'general', 'prediction', 'problems'], ['weighted', 'theta', 'functions', 'embeddings', 'applications', 'maxcut', 'clustering', 'summarization'], ['sgd', 'algorithms', 'based', 'incomplete', 'ustatistics', 'largescale', 'minimization', 'empirical', 'risk'], ['topk', 'selection', 'multiarmed', 'bandits', 'hidden', 'bipartite', 'graphs'], ['brain', 'uses', 'reliability', 'stimulus', 'information', 'when', 'making', 'perceptual', 'decisions'], ['fast', 'classification', 'rates', 'highdimensional', 'gaussian', 'generative', 'models'], ['fast', 'distributed', 'kcenter', 'clustering', 'outliers', 'massive', 'data'], ['human', 'memory', 'search', 'initialvisit', 'emitting', 'random', 'walk'], ['nonconvex', 'statistical', 'optimization', 'sparse', 'tensor', 'graphical', 'model'], ['convergence', 'rates', 'active', 'learning', 'maximum', 'likelihood', 'estimation'], ['weaklysupervised', 'disentangling', 'recurrent', 'transformations', '3d', 'view', 'synthesis'], ['efficient', 'exact', 'gradient', 'update', 'training', 'deep', 'networks', 'very', 'large', 'sparse', 'targets'], ['backpropagation', 'energyefficient', 'neuromorphic', 'computing'], ['alternating', 'minimization', 'regression', 'problems', 'vectorvalued', 'outputs'], ['learning', 'both', 'weights', 'connections', 'efficient', 'neural', 'network'], ['optimal', 'rates', 'random', 'fourier', 'features'], ['population', 'posterior', 'bayesian', 'modeling', 'streams'], ['frankwolfe', 'bayesian', 'quadrature', 'probabilistic', 'integration', 'theoretical', 'guarantees'], ['scheduled', 'sampling', 'sequence', 'prediction', 'recurrent', 'neural', 'networks'], ['unified', 'view', 'matrix', 'completion', 'under', 'general', 'structural', 'constraints'], ['efficient', 'output', 'kernel', 'learning', 'multiple', 'tasks'], ['scalable', 'adaptation', 'state', 'complexity', 'nonparametric', 'hidden', 'markov', 'models'], ['variational', 'consensus', 'monte', 'carlo'], ['newtonstein', 'method', 'second', 'order', 'method', 'glms', 'steins', 'lemma'], ['practical', 'optimal', 'lsh', 'angular', 'distance'], ['learning', 'linearize', 'under', 'uncertainty'], ['finitetime', 'analysis', 'projected', 'langevin', 'monte', 'carlo'], ['deep', 'visual', 'analogymaking'], ['matrix', 'completion', 'fewer', 'entries', 'spectral', 'detectability', 'rank', 'estimation'], ['online', 'learning', 'adversarial', 'delays'], ['multilayer', 'feature', 'reduction', 'tree', 'structured', 'group', 'lasso', 'hierarchical', 'projection'], ['minimum', 'weight', 'perfect', 'matching', 'blossom', 'belief', 'propagation'], ['efficient', 'thompson', 'sampling', 'online', '\\xef\\xbf\\xbcmatrixfactorization', 'recommendation'], ['improved', 'iteration', 'complexity', 'bounds', 'cyclic', 'block', 'coordinate', 'descent', 'convex', 'problems'], ['lifted', 'symmetry', 'detection', 'breaking', 'map', 'inference'], ['evaluating', 'statistical', 'significance', 'biclusters'], ['discriminative', 'robust', 'transformation', 'learning'], ['bandits', 'unobserved', 'confounders', 'causal', 'approach'], ['scalable', 'semisupervised', 'aggregation', 'classifiers'], ['online', 'learning', 'gaussian', 'payoffs', 'side', 'observations'], ['private', 'graphon', 'estimation', 'sparse', 'graphs'], ['submodboxes', 'nearoptimal', 'search', 'set', 'diverse', 'object', 'proposals'], ['fast', 'second', 'order', 'stochastic', 'backpropagation', 'variational', 'inference'], ['randomized', 'block', 'krylov', 'methods', 'stronger', 'faster', 'approximate', 'singular', 'value', 'decomposition'], ['crossdomain', 'matching', 'bagofwords', 'data', 'kernel', 'embeddings', 'latent', 'distributions'], ['scalable', 'inference', 'gaussian', 'process', 'models', 'blackbox', 'likelihoods'], ['fast', 'bidirectional', 'probability', 'estimation', 'markov', 'models'], ['probabilistic', 'variational', 'bounds', 'graphical', 'models'], ['linear', 'response', 'methods', 'accurate', 'covariance', 'estimates', 'mean', 'field', 'variational', 'bayes'], ['combinatorial', 'cascading', 'bandits'], ['mixing', 'time', 'estimation', 'reversible', 'markov', 'chains', 'single', 'sample', 'path'], ['policy', 'gradient', 'coherent', 'risk', 'measures'], ['fast', 'rates', 'expconcave', 'empirical', 'risk', 'minimization'], ['deep', 'generative', 'image', 'models', '\\xef\\xbf\\xbclaplacian', 'pyramid', 'adversarial', 'networks'], ['decoupled', 'deep', 'neural', 'network', 'semisupervised', 'semantic', 'segmentation'], ['equilibrated', 'adaptive', 'learning', 'rates', 'nonconvex', 'optimization'], ['backshift', 'learning', 'causal', 'cyclic', 'graphs', 'unknown', 'shift', 'interventions'], ['risksensitive', 'robust', 'decisionmaking', 'cvar', 'optimization', 'approach'], ['asynchronous', 'stochastic', 'convex', 'optimization', 'noise', 'is', 'noise', 'sgd', 'dont', 'care'], ['lifelong', 'learning', 'noniid', 'tasks'], ['optimal', 'linear', 'estimation', 'under', 'unknown', 'nonlinear', 'transform'], ['learning', 'group', 'invariant', 'features', 'kernel', 'perspective'], ['regularized', 'em', 'algorithms', 'unified', 'framework', 'statistical', 'guarantees'], ['adaptive', 'stochastic', 'optimization', 'sets', 'paths'], ['beyond', 'convexity', 'stochastic', 'quasiconvex', 'optimization'], ['tractable', 'approximation', 'optimal', 'point', 'process', 'filtering', 'application', 'neural', 'encoding'], ['sumofsquares', 'lower', 'bounds', 'sparse', 'pca'], ['maxmargin', 'majority', 'voting', 'learning', 'crowds'], ['learning', 'incremental', 'iterative', 'regularization'], ['halting', 'random', 'walk', 'kernels'], ['mcmc', 'variationally', 'sparse', 'gaussian', 'processes'], ['less', 'is', 'more', 'nystr\\xc3\\xb6m', 'computational', 'regularization'], ['infinite', 'factorial', 'dynamical', 'model'], ['regularization', 'path', 'crossvalidation', 'error', 'lower', 'bounds'], ['attractor', 'network', 'dynamics', 'enable', 'preplay', 'rapid', 'path', 'planning', 'maze\\xe2\\x80\\x93like', 'environments'], ['teaching', 'machines', 'read', 'comprehend'], ['principal', 'differences', 'analysis', 'interpretable', 'characterization', 'differences', 'between', 'distributions'], ['when', 'kalmanfilter', 'restless', 'bandits', 'indexable'], ['segregated', 'graphs', 'marginals', 'chain', 'graph', 'models'], ['efficient', 'nongreedy', 'optimization', 'decision', 'trees'], ['probabilistic', 'curve', 'learning', 'coulomb', 'repulsion', 'electrostatic', 'gaussian', 'process'], ['inverse', 'reinforcement', 'learning', 'locally', 'consistent', 'reward', 'functions'], ['communication', 'complexity', 'distributed', 'convex', 'learning', 'optimization'], ['endtoend', 'learning', 'lda', 'mirrordescent', 'back', 'propagation', 'over', 'deep', 'architecture'], ['subset', 'selection', 'pareto', 'optimization'], ['accuracy', 'selfnormalized', 'loglinear', 'models'], ['regret', 'lower', 'bound', 'optimal', 'algorithm', 'finite', 'stochastic', 'partial', 'monitoring'], ['is', 'approval', 'voting', 'optimal', 'given', 'approval', 'votes'], ['regressive', 'virtual', 'metric', 'learning'], ['analysis', 'robust', 'pca', 'local', 'incoherence'], ['learning', 'transduce', 'unbounded', 'memory'], ['maxmargin', 'deep', 'generative', 'models'], ['spherical', 'random', 'features', 'polynomial', 'kernels'], ['rectified', 'factor', 'networks'], ['learning', 'bayesian', 'networks', 'thousands', 'variables'], ['matrix', 'completion', 'under', 'monotonic', 'single', 'index', 'models'], ['visalogy', 'answering', 'visual', 'analogy', 'questions'], ['treeguided', 'mcmc', 'inference', 'normalized', 'random', 'measure', 'mixture', 'models'], ['streaming', 'minmax', 'hypergraph', 'partitioning'], ['collaboratively', 'learning', 'preferences', 'ordinal', 'data'], ['biologically', 'inspired', 'dynamic', 'textures', 'probing', 'motion', 'perception'], ['generative', 'image', 'modeling', 'spatial', 'lstms'], ['robust', 'pca', 'compressed', 'data'], ['sampling', 'probabilistic', 'submodular', 'models'], ['coevolve', 'joint', 'point', 'process', 'model', 'information', 'diffusion', 'network', 'coevolution'], ['supervised', 'learning', 'dynamical', 'system', 'learning'], ['regretbased', 'pruning', 'extensiveform', 'games'], ['fast', 'twosample', 'testing', 'analytic', 'representations', 'probability', 'measures'], ['learning', 'segment', 'object', 'candidates'], ['gp', 'kernels', 'crossspectrum', 'analysis'], ['secure', 'multiparty', 'differential', 'privacy'], ['spatial', 'transformer', 'networks'], ['anytime', 'influence', 'bounds', 'explosive', 'behavior', 'continuoustime', 'diffusion', 'networks'], ['multiclass', 'svms', 'tighter', 'datadependent', 'generalization', 'bounds', 'novel', 'algorithms'], ['highdimensional', 'neural', 'spike', 'train', 'analysis', 'generalized', 'count', 'linear', 'dynamical', 'systems'], ['learning', 'wasserstein', 'loss'], ['bbit', 'marginal', 'regression'], ['natural', 'neural', 'networks'], ['optimization', 'monte', 'carlo', 'efficient', 'embarrassingly', 'parallel', 'likelihoodfree', 'inference'], ['adaptive', 'primaldual', 'splitting', 'methods', 'statistical', 'learning', 'image', 'processing'], ['some', 'provably', 'correct', 'cases', 'variational', 'inference', 'topic', 'models'], ['collaborative', 'filtering', 'graph', 'information', 'consistency', 'scalable', 'methods'], ['combinatorial', 'bandits', 'revisited'], ['variational', 'information', 'maximisation', 'intrinsically', 'motivated', 'reinforcement', 'learning'], ['structural', 'smoothing', 'framework', 'robust', 'graph', 'comparison'], ['competitive', 'distribution', 'estimation', 'why', 'is', 'goodturing', 'good'], ['efficient', 'learning', 'directed', 'acyclic', 'graph', 'resource', 'constrained', 'prediction'], ['hybrid', 'sampler', 'poissonkingman', 'mixture', 'models'], ['an', 'active', 'learning', 'framework', 'sparsegraph', 'codes', 'sparse', 'polynomials', 'graph', 'sketching'], ['local', 'smoothness', 'variance', 'reduced', 'optimization'], ['saliency', 'scale', 'information', 'towards', 'unifying', 'theory'], ['fighting', 'bandits', 'new', 'kind', 'smoothness'], ['beyond', 'subgaussian', 'measurements', 'highdimensional', 'structured', 'estimation', 'subexponential', 'designs'], ['spectral', 'norm', 'regularization', 'orthonormal', 'representations', 'graph', 'transduction'], ['convolutional', 'networks', 'graphs', 'learning', 'molecular', 'fingerprints'], ['mixed', 'robustaverage', 'submodular', 'partitioning', 'fast', 'algorithms', 'guarantees', 'applications'], ['tractable', 'learning', 'complex', 'probability', 'queries'], ['stopwasting', 'my', 'gradients', 'practical', 'svrg'], ['mind', 'gap', 'generative', 'approach', 'interpretable', 'feature', 'selection', 'extraction'], ['normative', 'theory', 'adaptive', 'dimensionality', 'reduction', 'neural', 'networks'], ['convergence', 'stochastic', 'gradient', 'mcmc', 'algorithms', 'highorder', 'integrators'], ['learning', 'structured', 'densities', 'infinite', 'dimensional', 'exponential', 'families'], ['you', 'talking', 'machine', 'dataset', 'methods', 'multilingual', 'image', 'question'], ['variance', 'reduced', 'stochastic', 'gradient', 'descent', 'neighbors'], ['sample', 'efficient', 'path', 'integral', 'control', 'under', 'uncertainty'], ['stochastic', 'expectation', 'propagation'], ['exactness', 'approximate', 'map', 'inference', 'continuous', 'mrfs'], ['scale', 'up', 'nonlinear', 'component', 'analysis', 'doubly', 'stochastic', 'gradients'], ['generalization', 'adaptive', 'data', 'analysis', 'holdout', 'reuse'], ['market', 'scoring', 'rules', 'act', 'opinion', 'pools', 'riskaverse', 'agents'], ['sparse', 'linear', 'programming', 'primal', 'dual', 'augmented', 'coordinate', 'descent'], ['training', 'very', 'deep', 'networks'], ['bayesian', 'active', 'model', 'selection', 'an', 'application', 'automated', 'audiometry'], ['particle', 'gibbs', 'infinite', 'hidden', 'markov', 'models'], ['learning', 'spatiotemporal', 'trajectories', 'manifoldvalued', 'longitudinal', 'data'], ['bayesian', 'framework', 'modeling', 'confidence', 'perceptual', 'decision', 'making'], ['pathsgd', 'pathnormalized', 'optimization', 'deep', 'neural', 'networks'], ['consistency', 'theory', 'high', 'dimensional', 'variable', 'screening'], ['endtoend', 'memory', 'networks'], ['spectral', 'representations', 'convolutional', 'neural', 'networks'], ['online', 'gradient', 'boosting'], ['deep', 'temporal', 'sigmoid', 'belief', 'networks', 'sequence', 'modeling'], ['recognizing', 'retinal', 'ganglion', 'cells', 'dark'], ['theory', 'decision', 'making', 'under', 'dynamic', 'context'], ['gaussian', 'process', 'model', 'quasar', 'spectral', 'energy', 'distributions'], ['hidden', 'technical', 'debt', 'machine', 'learning', 'systems'], ['local', 'causal', 'discovery', 'direct', 'causes', 'effects'], ['high', 'dimensional', 'em', 'algorithm', 'statistical', 'optimization', 'asymptotic', 'normality'], ['revenue', 'optimization', 'against', 'strategic', 'buyers'], ['deep', 'convolutional', 'inverse', 'graphics', 'network'], ['sparse', 'lowrank', 'tensor', 'decomposition'], ['minimax', 'time', 'series', 'prediction'], ['differentially', 'private', 'learning', 'structured', 'discrete', 'distributions'], ['sample', 'complexity', 'learning', 'mahalanobis', 'distance', 'metrics'], ['learning', 'wakesleep', 'recurrent', 'attention', 'models'], ['robust', 'gaussian', 'graphical', 'modeling', 'trimmed', 'graphical', 'lasso'], ['testing', 'closeness', 'unequal', 'sized', 'samples'], ['estimating', 'jaccard', 'index', 'missing', 'observations', 'matrix', 'calibration', 'approach'], ['neural', 'adaptive', 'sequential', 'monte', 'carlo'], ['local', 'expectation', 'gradients', 'black', 'box', 'variational', 'inference'], ['variance', 'reduction', 'stochastic', 'gradient', 'descent', 'its', 'asynchronous', 'variants'], ['next', 'system', 'realworld', 'development', 'evaluation', 'application', 'active', 'learning'], ['superresolution', 'off', 'grid'], ['taming', 'wild', 'unified', 'analysis', 'hogwildstyle', 'algorithms'], ['return', 'gating', 'network', 'combining', 'generative', 'models', 'discriminative', 'training', 'natural', 'image', 'priors'], ['pointer', 'networks'], ['associative', 'memory', 'sparse', 'recovery', 'model'], ['robust', 'spectral', 'inference', 'joint', 'stochastic', 'matrix', 'factorization'], ['fast', 'provable', 'algorithms', 'isotonic', 'regression', 'all', 'lpnorms'], ['adversarial', 'prediction', 'games', 'multivariate', 'losses'], ['asynchronous', 'parallel', 'stochastic', 'gradient', 'nonconvex', 'optimization'], ['embed', 'control', 'locally', 'linear', 'latent', 'dynamics', 'model', 'control', 'raw', 'images'], ['efficient', 'parsimonious', 'agnostic', 'active', 'learning'], ['softstar', 'heuristicguided', 'probabilistic', 'inference'], ['grammar', 'foreign', 'language'], ['regularizationfree', 'estimation', 'trace', 'regression', 'symmetric', 'positive', 'semidefinite', 'matrices'], ['winnertakeall', 'autoencoders'], ['deep', 'poisson', 'factor', 'modeling'], ['bayesian', 'optimization', 'exponential', 'convergence'], ['sample', 'complexity', 'episodic', 'fixedhorizon', 'reinforcement', 'learning'], ['learning', 'relaxed', 'supervision'], ['subsampled', 'power', 'iteration', 'unified', 'algorithm', 'block', 'models', 'planted', 'csps'], ['accelerated', 'mirror', 'descent', 'continuous', 'discrete', 'time'], ['human', 'kernel'], ['actionconditional', 'video', 'prediction', 'deep', 'networks', 'atari', 'games'], ['pseudoeuclidean', 'iteration', 'optimal', 'recovery', 'noisy', 'ica'], ['distributed', 'submodular', 'cover', 'succinctly', 'summarizing', 'massive', 'data'], ['community', 'detection', 'measure', 'space', 'embedding'], ['basis', 'refinement', 'strategies', 'linear', 'value', 'function', 'approximation', 'mdps'], ['structured', 'estimation', 'atomic', 'norms', 'general', 'bounds', 'applications'], ['complete', 'recipe', 'stochastic', 'gradient', 'mcmc'], ['bandit', 'smooth', 'convex', 'optimization', 'improving', 'biasvariance', 'tradeoff'], ['online', 'prediction', 'at', 'limit', 'zero', 'temperature'], ['learning', 'continuous', 'control', 'policies', 'stochastic', 'value', 'gradients'], ['exploring', 'models', 'data', 'image', 'question', 'answering'], ['efficient', 'robust', 'automated', 'machine', 'learning'], ['preconditioned', 'spectral', 'descent', 'deep', 'learning'], ['recurrent', 'latent', 'variable', 'model', 'sequential', 'data'], ['fast', 'convergence', 'regularized', 'learning', 'games'], ['parallel', 'multidimensional', 'lstm', 'application', 'fast', 'biomedical', 'volumetric', 'image', 'segmentation'], ['reflection', 'refraction', 'hamiltonian', 'monte', 'carlo'], ['consistency', 'common', 'neighbors', 'link', 'prediction', 'stochastic', 'blockmodels'], ['nearly', 'optimal', 'private', 'lasso'], ['convergence', 'analysis', 'prediction', 'markets', 'randomized', 'subspace', 'descent'], ['poisson', 'gamma', 'belief', 'network'], ['convergence', 'rates', 'subsampled', 'newton', 'methods'], ['noregret', 'learning', 'bayesian', 'games'], ['statistical', 'topological', 'data', 'analysis', '', 'kernel', 'perspective'], ['semisupervised', 'sequence', 'learning'], ['structured', 'transforms', 'smallfootprint', 'deep', 'learning'], ['rapidly', 'mixing', 'gibbs', 'sampling', 'class', 'factor', 'graphs', 'hierarchy', 'width'], ['interpolating', 'convex', 'nonconvex', 'tensor', 'decompositions', 'subspace', 'norm'], ['sample', 'complexity', 'bounds', 'iterative', 'stochastic', 'policy', 'optimization'], ['binaryconnect', 'training', 'deep', 'neural', 'networks', 'binary', 'weights', 'during', 'propagations'], ['interactive', 'control', 'diverse', 'complex', 'characters', 'neural', 'networks'], ['submodular', 'hamming', 'metrics'], ['universal', 'primaldual', 'convex', 'optimization', 'framework'], ['learning', 'small', 'samples', 'an', 'analysis', 'simple', 'decision', 'heuristics'], ['explore', 'no', 'more', 'improved', 'highprobability', 'regret', 'bounds', 'nonstochastic', 'bandits'], ['fast', 'memory', 'optimal', 'lowrank', 'matrix', 'approximation'], ['learnability', 'influence', 'networks'], ['learning', 'causal', 'graphs', 'small', 'interventions'], ['informationtheoretic', 'lower', 'bounds', 'convex', 'optimization', 'erroneous', 'oracles'], ['fixedlength', 'poisson', 'mrf', 'adding', 'dependencies', 'multinomial'], ['largescale', 'bayesian', 'multilabel', 'learning', 'topicbased', 'label', 'embeddings'], ['selfnormalized', 'estimator', 'counterfactual', 'learning'], ['fast', 'lifted', 'map', 'inference', 'partitioning'], ['data', 'generation', 'sequential', 'decision', 'making'], ['elicitation', 'complexity'], ['decomposition', 'bounds', 'marginal', 'map'], ['discrete', 'r\\xc3\\xa9nyi', 'classifiers'], ['class', 'network', 'models', 'recoverable', 'spectral', 'clustering'], ['skipthought', 'vectors'], ['rateagnostic', 'causal', 'structure', 'learning'], ['principal', 'geodesic', 'analysis', 'probability', 'measures', 'under', 'optimal', 'transport', 'metric'], ['consistent', 'multilabel', 'classification'], ['parallel', 'predictive', 'entropy', 'search', 'batch', 'global', 'optimization', 'expensive', 'objective', 'functions'], ['cornering', 'stationary', 'restless', 'mixing', 'bandits', 'remixucb'], ['semisupervised', 'factored', 'logistic', 'regression', 'highdimensional', 'neuroimaging', 'data'], ['gaussian', 'process', 'random', 'fields'], ['mstatistic', 'kernel', 'changepoint', 'detection'], ['adaptive', 'online', 'learning'], ['universal', 'catalyst', 'firstorder', 'optimization'], ['inference', 'determinantal', 'point', 'processes', 'without', 'spectral', 'knowledge'], ['kullbackleibler', 'proximal', 'variational', 'inference'], ['semiproximal', 'mirrorprox', 'nonsmooth', 'composite', 'minimization'], ['lasso', 'nonlinear', 'measurements', 'is', 'equivalent', 'one', 'linear', 'measurements'], ['random', 'walks', 'distances', 'unweighted', 'graphs'], ['bayesian', 'dark', 'knowledge'], ['matrix', 'completion', 'noisy', 'side', 'information'], ['dependent', 'multinomial', 'models', 'made', 'easy', 'stickbreaking', 'polyagamma', 'augmentation'], ['onthejob', 'learning', 'bayesian', 'decision', 'theory'], ['calibrated', 'structured', 'prediction'], ['learning', 'structured', 'output', 'representation', 'deep', 'conditional', 'generative', 'models'], ['timesensitive', 'recommendation', 'recurrent', 'user', 'activities'], ['learning', 'stationary', 'time', 'series', 'gaussian', 'processes', 'nonparametric', 'kernels'], ['market', 'framework', 'eliciting', 'private', 'data'], ['lifted', 'inference', 'rules', 'constraints'], ['gradient', 'estimation', 'stochastic', 'computation', 'graphs'], ['modelbased', 'relative', 'entropy', 'stochastic', 'search'], ['semisupervised', 'learning', 'ladder', 'networks'], ['embedding', 'inference', 'structured', 'multilabel', 'prediction'], ['copula', 'variational', 'inference'], ['recursive', 'training', '2d3d', 'convolutional', 'networks', 'neuronal', 'boundary', 'prediction'], ['dual', 'augmented', 'block', 'minimization', 'framework', 'learning', 'limited', 'memory'], ['optimal', 'testing', 'properties', 'distributions'], ['efficient', 'learning', 'continuoustime', 'hidden', 'markov', 'models', 'disease', 'progression'], ['expectation', 'particle', 'belief', 'propagation'], ['latent', 'bayesian', 'melding', 'integrating', 'individual', 'population', 'models'], ['distributionally', 'robust', 'logistic', 'regression'], ['variational', 'dropout', 'local', 'reparameterization', 'trick']]\n"
     ]
    }
   ],
   "source": [
    "#REMOVE PUNCTUATION AND COMMON WORDS WITHOUT DISCRIMINATORY QUALITIES (\"STOP WORDS\")\n",
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "stoplist = set('for a of the and to in or on from using via with by as are'.split())\n",
    "#texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\n",
    "texts = [[''.join(ch for ch in word if ch not in exclude) for word in document.lower().split() if word not in stoplist] for document in documents]\n",
    "print texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[],\n",
      " ['learning', 'symmetric', 'label', 'noise'],\n",
      " ['algorithmic', 'generalization'],\n",
      " ['adaptive', 'sequential', 'inference', 'process', 'mixture', 'models'],\n",
      " ['adaptive', 'langevin', 'largescale', 'bayesian', 'sampling'],\n",
      " ['robust', 'optimization'],\n",
      " ['logarithmic', 'time', 'online', 'multiclass', 'prediction'],\n",
      " ['image', 'segmentation'],\n",
      " ['an', 'image', 'sequence', 'natural'],\n",
      " ['parallel', 'clustering', 'graphs'],\n",
      " ['faster', 'towards', 'object', 'detection', 'region', 'networks'],\n",
      " ['local', 'embeddings'],\n",
      " ['gradient',\n",
      "  'descent',\n",
      "  'algorithm',\n",
      "  'rank',\n",
      "  'minimization',\n",
      "  'semidefinite',\n",
      "  'programming',\n",
      "  'random',\n",
      "  'linear',\n",
      "  'measurements'],\n",
      " ['smooth', 'interactive', 'submodular', 'set', 'cover'],\n",
      " ['object', 'properties', 'integrating', 'deep', 'learning'],\n",
      " ['nearly', 'optimal'],\n",
      " ['neural', 'population', 'hierarchical', 'dynamics', 'models'],\n",
      " ['bayesian',\n",
      "  'manifold',\n",
      "  'learning',\n",
      "  'locally',\n",
      "  'linear',\n",
      "  'latent',\n",
      "  'variable',\n",
      "  'model'],\n",
      " ['learning'],\n",
      " ['fast', 'accurate', 'inference', 'models'],\n",
      " ['probabilistic', 'stochastic', 'optimization'],\n",
      " ['algorithmic', 'recurrent'],\n",
      " [],\n",
      " ['pareto', 'regret', 'bandits'],\n",
      " ['spectral', 'methods', 'gaussian', 'hidden', 'gaussian'],\n",
      " ['sample', 'steins', 'method'],\n",
      " ['bidirectional',\n",
      "  'recurrent',\n",
      "  'convolutional',\n",
      "  'networks',\n",
      "  'superresolution'],\n",
      " ['bounding'],\n",
      " ['fast', 'universal', 'algorithm', 'nonlinear', 'embeddings'],\n",
      " ['synthesis', 'convolutional', 'neural', 'networks'],\n",
      " ['algorithms', 'distributed', 'estimation', 'ustatistics'],\n",
      " ['streaming', 'distributed', 'variational', 'inference', 'bayesian'],\n",
      " ['learning', 'visual', 'human'],\n",
      " ['smooth', 'strong', 'map', 'inference', 'linear', 'convergence'],\n",
      " ['dueling', 'bandits'],\n",
      " ['optimal', 'ridge', 'detection', 'risk'],\n",
      " ['topk', 'multiclass'],\n",
      " ['policy', 'evaluation'],\n",
      " ['subspace'],\n",
      " ['stochastic', 'online', 'learning', 'semibandit'],\n",
      " ['learning', 'inference'],\n",
      " ['sampling', 'bayesian', 'approach', 'neural', 'network'],\n",
      " ['accelerated',\n",
      "  'proximal',\n",
      "  'gradient',\n",
      "  'methods',\n",
      "  'nonconvex',\n",
      "  'programming'],\n",
      " ['sparse', 'pca', 'incomplete', 'data'],\n",
      " ['nonparametric', 'estimators', 'divergences'],\n",
      " ['selection', 'adaptive', 'sampling'],\n",
      " ['hybrid', 'optimization', 'nonconvex', 'regularized', 'problems'],\n",
      " ['3d', 'object', 'proposals', 'accurate', 'object', 'class', 'detection'],\n",
      " ['algorithms', 'logarithmic', 'regret', 'constrained', 'bandits'],\n",
      " ['neural', 'networks'],\n",
      " ['mcmc', 'random', 'trees'],\n",
      " ['response', 'model'],\n",
      " ['spectral', 'learning', 'large', 'structured'],\n",
      " ['individual', 'planning', 'inference', 'structure'],\n",
      " ['estimating', 'mixture', 'models', 'mixtures', 'polynomials'],\n",
      " ['global', 'linear', 'convergence', 'frankwolfe', 'optimization', 'variants'],\n",
      " ['deep', 'knowledge'],\n",
      " ['lda', 'matching', 'discrete', 'ica'],\n",
      " ['efficient', 'compressive', 'constrained', 'vectors'],\n",
      " ['frankwolfe', 'marginal', 'inference'],\n",
      " ['learning', 'theory', 'algorithms', 'time', 'series'],\n",
      " ['compressive', 'spectral', 'embedding'],\n",
      " ['nonconvex', 'optimization', 'framework', 'rank', 'matrix', 'estimation'],\n",
      " ['variational', 'inference'],\n",
      " ['models'],\n",
      " ['estimators', 'highdimensional', 'generalized', 'linear', 'models'],\n",
      " ['online', 'optimization'],\n",
      " ['online', 'rank', 'elicitation', 'dueling', 'bandits', 'approach'],\n",
      " ['submodular', 'beyond'],\n",
      " ['tractable', 'bayesian', 'network', 'structure', 'learning', 'cover'],\n",
      " ['learning', 'largescale', 'poisson', 'models', 'based', 'scoring'],\n",
      " ['training', 'machine', 'energy'],\n",
      " ['convolutional', 'networks', 'text', 'classification'],\n",
      " ['robust', 'linear', 'analysis', 'brain'],\n",
      " ['blackbox', 'optimization', 'noisy', 'functions', 'unknown', 'smoothness'],\n",
      " ['general', 'stochastic', 'block', 'model', 'without'],\n",
      " ['deep', 'learning', 'sgd'],\n",
      " ['function', 'constraints'],\n",
      " ['active', 'learning', 'strong'],\n",
      " ['chain', 'multilabel', 'classification'],\n",
      " ['robust', 'regression'],\n",
      " ['sparse', 'local', 'embeddings', 'multilabel', 'classification'],\n",
      " ['solving',\n",
      "  'random',\n",
      "  'systems',\n",
      "  'is',\n",
      "  'nearly',\n",
      "  'easy',\n",
      "  'solving',\n",
      "  'linear',\n",
      "  'systems'],\n",
      " ['framework', 'disease', 'trajectories', 'structure'],\n",
      " ['subspace', 'clustering', 'features', 'robust'],\n",
      " ['sparse', 'pca', 'bipartite'],\n",
      " ['fast',\n",
      "  'randomized',\n",
      "  'kernel',\n",
      "  'ridge',\n",
      "  'regression',\n",
      "  'statistical',\n",
      "  'guarantees'],\n",
      " ['online', 'learning', 'memory'],\n",
      " ['convolutional', 'covariance', 'analysis', 'neural', 'models'],\n",
      " ['convolutional', 'lstm', 'network', 'machine', 'learning', 'approach'],\n",
      " ['gap', 'screening', 'rules', 'sparse', 'multiclass', 'models'],\n",
      " ['empirical', 'divergences', 'discrete', 'sample'],\n",
      " ['statistical', 'model', 'kernel', 'sample'],\n",
      " ['analysis'],\n",
      " ['generalization', 'submodular', 'cover', 'return'],\n",
      " ['bidirectional', 'recurrent', 'neural', 'networks', 'generative', 'models'],\n",
      " ['randomized', 'dual', 'coordinate', 'arbitrary', 'sampling'],\n",
      " ['maximum', 'likelihood', 'learning', 'arbitrary', 'sets'],\n",
      " ['optimization',\n",
      "  'learning',\n",
      "  'deep',\n",
      "  'multidimensional',\n",
      "  'recurrent',\n",
      "  'neural',\n",
      "  'networks'],\n",
      " ['largescale', 'probabilistic', 'without', 'guarantees'],\n",
      " ['convolutional', 'neural', 'networks'],\n",
      " ['matrix', 'manifold', 'optimization', 'gaussian', 'mixtures'],\n",
      " ['semisupervised',\n",
      "  'convolutional',\n",
      "  'neural',\n",
      "  'networks',\n",
      "  'text',\n",
      "  'region',\n",
      "  'embedding'],\n",
      " ['parallel',\n",
      "  'recursive',\n",
      "  'search',\n",
      "  'exact',\n",
      "  'map',\n",
      "  'inference',\n",
      "  'graphical',\n",
      "  'models'],\n",
      " ['convolutional', 'neural', 'networks', 'recurrent', 'connections'],\n",
      " ['bounding', 'lifted', 'inference'],\n",
      " ['hamiltonian',\n",
      "  'monte',\n",
      "  'carlo',\n",
      "  'efficient',\n",
      "  'kernel',\n",
      "  'exponential',\n",
      "  'families'],\n",
      " ['linear', 'semibandit'],\n",
      " ['learning', 'synthesis'],\n",
      " ['local', 'supervised', 'learning', 'recurrent', 'networks'],\n",
      " ['fast', 'tensor', 'decomposition', 'sketching'],\n",
      " ['differentially', 'private', 'subspace', 'clustering'],\n",
      " ['online', 'algorithms', 'general', 'prediction', 'problems'],\n",
      " ['functions', 'embeddings', 'applications', 'clustering'],\n",
      " ['sgd',\n",
      "  'algorithms',\n",
      "  'based',\n",
      "  'incomplete',\n",
      "  'ustatistics',\n",
      "  'largescale',\n",
      "  'minimization',\n",
      "  'empirical',\n",
      "  'risk'],\n",
      " ['topk', 'selection', 'bandits', 'hidden', 'bipartite', 'graphs'],\n",
      " ['brain', 'information', 'when', 'making', 'perceptual'],\n",
      " ['fast',\n",
      "  'classification',\n",
      "  'rates',\n",
      "  'highdimensional',\n",
      "  'gaussian',\n",
      "  'generative',\n",
      "  'models'],\n",
      " ['fast', 'distributed', 'clustering', 'massive', 'data'],\n",
      " ['human', 'memory', 'search', 'random', 'walk'],\n",
      " ['nonconvex',\n",
      "  'statistical',\n",
      "  'optimization',\n",
      "  'sparse',\n",
      "  'tensor',\n",
      "  'graphical',\n",
      "  'model'],\n",
      " ['convergence',\n",
      "  'rates',\n",
      "  'active',\n",
      "  'learning',\n",
      "  'maximum',\n",
      "  'likelihood',\n",
      "  'estimation'],\n",
      " ['recurrent', '3d', 'view', 'synthesis'],\n",
      " ['efficient',\n",
      "  'exact',\n",
      "  'gradient',\n",
      "  'training',\n",
      "  'deep',\n",
      "  'networks',\n",
      "  'very',\n",
      "  'large',\n",
      "  'sparse'],\n",
      " ['backpropagation'],\n",
      " ['minimization', 'regression', 'problems'],\n",
      " ['learning', 'weights', 'connections', 'efficient', 'neural', 'network'],\n",
      " ['optimal', 'rates', 'random', 'features'],\n",
      " ['population', 'bayesian', 'modeling'],\n",
      " ['frankwolfe', 'bayesian', 'probabilistic', 'guarantees'],\n",
      " ['sampling', 'sequence', 'prediction', 'recurrent', 'neural', 'networks'],\n",
      " ['unified',\n",
      "  'view',\n",
      "  'matrix',\n",
      "  'completion',\n",
      "  'under',\n",
      "  'general',\n",
      "  'structural',\n",
      "  'constraints'],\n",
      " ['efficient', 'output', 'kernel', 'learning', 'tasks'],\n",
      " ['scalable', 'complexity', 'nonparametric', 'hidden', 'markov', 'models'],\n",
      " ['variational', 'monte', 'carlo'],\n",
      " ['method', 'second', 'order', 'method', 'steins'],\n",
      " ['practical', 'optimal', 'distance'],\n",
      " ['learning', 'under', 'uncertainty'],\n",
      " ['analysis', 'langevin', 'monte', 'carlo'],\n",
      " ['deep', 'visual'],\n",
      " ['matrix', 'completion', 'spectral', 'rank', 'estimation'],\n",
      " ['online', 'learning', 'adversarial'],\n",
      " ['feature', 'reduction', 'structured', 'group', 'lasso', 'hierarchical'],\n",
      " ['matching', 'belief', 'propagation'],\n",
      " ['efficient', 'sampling', 'online', 'recommendation'],\n",
      " ['improved',\n",
      "  'iteration',\n",
      "  'complexity',\n",
      "  'bounds',\n",
      "  'cyclic',\n",
      "  'block',\n",
      "  'coordinate',\n",
      "  'descent',\n",
      "  'convex',\n",
      "  'problems'],\n",
      " ['lifted', 'detection', 'map', 'inference'],\n",
      " ['statistical'],\n",
      " ['discriminative', 'robust', 'learning'],\n",
      " ['bandits', 'causal', 'approach'],\n",
      " ['scalable', 'semisupervised', 'classifiers'],\n",
      " ['online', 'learning', 'gaussian', 'side', 'observations'],\n",
      " ['private', 'estimation', 'sparse', 'graphs'],\n",
      " ['search', 'set', 'diverse', 'object', 'proposals'],\n",
      " ['fast',\n",
      "  'second',\n",
      "  'order',\n",
      "  'stochastic',\n",
      "  'backpropagation',\n",
      "  'variational',\n",
      "  'inference'],\n",
      " ['randomized',\n",
      "  'block',\n",
      "  'methods',\n",
      "  'faster',\n",
      "  'approximate',\n",
      "  'value',\n",
      "  'decomposition'],\n",
      " ['matching', 'data', 'kernel', 'embeddings', 'latent', 'distributions'],\n",
      " ['scalable', 'inference', 'gaussian', 'process', 'models', 'blackbox'],\n",
      " ['fast', 'bidirectional', 'probability', 'estimation', 'markov', 'models'],\n",
      " ['probabilistic', 'variational', 'bounds', 'graphical', 'models'],\n",
      " ['linear', 'response', 'methods', 'accurate', 'covariance', 'variational'],\n",
      " ['combinatorial', 'bandits'],\n",
      " ['mixing', 'time', 'estimation', 'markov', 'single', 'sample', 'path'],\n",
      " ['policy', 'gradient', 'risk', 'measures'],\n",
      " ['fast', 'rates', 'empirical', 'risk', 'minimization'],\n",
      " ['deep', 'generative', 'image', 'models', 'adversarial', 'networks'],\n",
      " ['deep', 'neural', 'network', 'semisupervised', 'segmentation'],\n",
      " ['adaptive', 'learning', 'rates', 'nonconvex', 'optimization'],\n",
      " ['learning', 'causal', 'cyclic', 'graphs', 'unknown', 'interventions'],\n",
      " ['robust', 'optimization', 'approach'],\n",
      " ['asynchronous',\n",
      "  'stochastic',\n",
      "  'convex',\n",
      "  'optimization',\n",
      "  'noise',\n",
      "  'is',\n",
      "  'noise',\n",
      "  'sgd'],\n",
      " ['learning', 'tasks'],\n",
      " ['optimal', 'linear', 'estimation', 'under', 'unknown', 'nonlinear'],\n",
      " ['learning', 'group', 'features', 'kernel', 'perspective'],\n",
      " ['regularized',\n",
      "  'em',\n",
      "  'algorithms',\n",
      "  'unified',\n",
      "  'framework',\n",
      "  'statistical',\n",
      "  'guarantees'],\n",
      " ['adaptive', 'stochastic', 'optimization', 'sets'],\n",
      " ['beyond', 'stochastic', 'optimization'],\n",
      " ['tractable',\n",
      "  'approximation',\n",
      "  'optimal',\n",
      "  'point',\n",
      "  'process',\n",
      "  'filtering',\n",
      "  'application',\n",
      "  'neural'],\n",
      " ['lower', 'bounds', 'sparse', 'pca'],\n",
      " ['maxmargin', 'voting', 'learning'],\n",
      " ['learning', 'iterative', 'regularization'],\n",
      " ['random', 'walk', 'kernels'],\n",
      " ['mcmc', 'sparse', 'gaussian', 'processes'],\n",
      " ['is', 'more', 'regularization'],\n",
      " ['infinite', 'dynamical', 'model'],\n",
      " ['regularization', 'path', 'lower', 'bounds'],\n",
      " ['network', 'dynamics', 'path', 'planning'],\n",
      " [],\n",
      " ['principal',\n",
      "  'differences',\n",
      "  'analysis',\n",
      "  'interpretable',\n",
      "  'differences',\n",
      "  'distributions'],\n",
      " ['when', 'restless', 'bandits'],\n",
      " ['graphs', 'chain', 'graph', 'models'],\n",
      " ['efficient', 'optimization', 'decision', 'trees'],\n",
      " ['probabilistic', 'learning', 'gaussian', 'process'],\n",
      " ['inverse',\n",
      "  'reinforcement',\n",
      "  'learning',\n",
      "  'locally',\n",
      "  'consistent',\n",
      "  'functions'],\n",
      " ['complexity', 'distributed', 'convex', 'learning', 'optimization'],\n",
      " ['endtoend', 'learning', 'lda', 'propagation', 'deep'],\n",
      " ['selection', 'pareto', 'optimization'],\n",
      " ['selfnormalized', 'models'],\n",
      " ['regret', 'lower', 'optimal', 'algorithm', 'stochastic'],\n",
      " ['is', 'approval', 'voting', 'optimal', 'approval'],\n",
      " ['metric', 'learning'],\n",
      " ['analysis', 'robust', 'pca', 'local'],\n",
      " ['learning', 'memory'],\n",
      " ['maxmargin', 'deep', 'generative', 'models'],\n",
      " ['random', 'features', 'kernels'],\n",
      " ['factor', 'networks'],\n",
      " ['learning', 'bayesian', 'networks'],\n",
      " ['matrix', 'completion', 'under', 'single', 'index', 'models'],\n",
      " ['answering', 'visual'],\n",
      " ['mcmc', 'inference', 'random', 'measure', 'mixture', 'models'],\n",
      " ['streaming', 'partitioning'],\n",
      " ['learning', 'data'],\n",
      " ['dynamic'],\n",
      " ['generative', 'image', 'modeling', 'spatial'],\n",
      " ['robust', 'pca', 'data'],\n",
      " ['sampling', 'probabilistic', 'submodular', 'models'],\n",
      " ['joint', 'point', 'process', 'model', 'information', 'diffusion', 'network'],\n",
      " ['supervised', 'learning', 'dynamical', 'system', 'learning'],\n",
      " ['games'],\n",
      " ['fast', 'testing', 'representations', 'probability', 'measures'],\n",
      " ['learning', 'object'],\n",
      " ['kernels', 'analysis'],\n",
      " [],\n",
      " ['spatial', 'networks'],\n",
      " ['influence', 'bounds', 'continuoustime', 'diffusion', 'networks'],\n",
      " ['multiclass', 'generalization', 'bounds', 'algorithms'],\n",
      " ['highdimensional',\n",
      "  'neural',\n",
      "  'analysis',\n",
      "  'generalized',\n",
      "  'linear',\n",
      "  'dynamical',\n",
      "  'systems'],\n",
      " ['learning'],\n",
      " ['marginal', 'regression'],\n",
      " ['natural', 'neural', 'networks'],\n",
      " ['optimization', 'monte', 'carlo', 'efficient', 'parallel', 'inference'],\n",
      " ['adaptive', 'primaldual', 'methods', 'statistical', 'learning', 'image'],\n",
      " ['variational', 'inference', 'models'],\n",
      " ['filtering', 'graph', 'information', 'consistency', 'scalable', 'methods'],\n",
      " ['combinatorial', 'bandits'],\n",
      " ['variational', 'information', 'reinforcement', 'learning'],\n",
      " ['structural', 'framework', 'robust', 'graph'],\n",
      " ['estimation', 'is'],\n",
      " ['efficient', 'learning', 'graph', 'constrained', 'prediction'],\n",
      " ['hybrid', 'mixture', 'models'],\n",
      " ['an',\n",
      "  'active',\n",
      "  'learning',\n",
      "  'framework',\n",
      "  'sparse',\n",
      "  'polynomials',\n",
      "  'graph',\n",
      "  'sketching'],\n",
      " ['local', 'smoothness', 'variance', 'reduced', 'optimization'],\n",
      " ['scale', 'information', 'towards', 'theory'],\n",
      " ['bandits', 'smoothness'],\n",
      " ['beyond', 'measurements', 'highdimensional', 'structured', 'estimation'],\n",
      " ['spectral', 'norm', 'regularization', 'representations', 'graph'],\n",
      " ['convolutional', 'networks', 'graphs', 'learning'],\n",
      " ['submodular',\n",
      "  'partitioning',\n",
      "  'fast',\n",
      "  'algorithms',\n",
      "  'guarantees',\n",
      "  'applications'],\n",
      " ['tractable', 'learning', 'complex', 'probability'],\n",
      " ['gradients', 'practical'],\n",
      " ['gap', 'generative', 'approach', 'interpretable', 'feature', 'selection'],\n",
      " ['theory', 'adaptive', 'reduction', 'neural', 'networks'],\n",
      " ['convergence', 'stochastic', 'gradient', 'mcmc', 'algorithms'],\n",
      " ['learning',\n",
      "  'structured',\n",
      "  'infinite',\n",
      "  'dimensional',\n",
      "  'exponential',\n",
      "  'families'],\n",
      " ['machine', 'methods', 'image', 'question'],\n",
      " ['variance', 'reduced', 'stochastic', 'gradient', 'descent', 'neighbors'],\n",
      " ['sample', 'efficient', 'path', 'control', 'under', 'uncertainty'],\n",
      " ['stochastic', 'expectation', 'propagation'],\n",
      " ['approximate', 'map', 'inference', 'continuous'],\n",
      " ['scale', 'nonlinear', 'analysis', 'stochastic', 'gradients'],\n",
      " ['generalization', 'adaptive', 'data', 'analysis'],\n",
      " ['market', 'scoring', 'rules'],\n",
      " ['sparse',\n",
      "  'linear',\n",
      "  'programming',\n",
      "  'dual',\n",
      "  'augmented',\n",
      "  'coordinate',\n",
      "  'descent'],\n",
      " ['training', 'very', 'deep', 'networks'],\n",
      " ['bayesian',\n",
      "  'active',\n",
      "  'model',\n",
      "  'selection',\n",
      "  'an',\n",
      "  'application',\n",
      "  'automated'],\n",
      " ['particle', 'gibbs', 'infinite', 'hidden', 'markov', 'models'],\n",
      " ['learning', 'trajectories', 'data'],\n",
      " ['bayesian', 'framework', 'modeling', 'perceptual', 'decision', 'making'],\n",
      " ['optimization', 'deep', 'neural', 'networks'],\n",
      " ['consistency', 'theory', 'high', 'dimensional', 'variable', 'screening'],\n",
      " ['endtoend', 'memory', 'networks'],\n",
      " ['spectral', 'representations', 'convolutional', 'neural', 'networks'],\n",
      " ['online', 'gradient'],\n",
      " ['deep', 'belief', 'networks', 'sequence', 'modeling'],\n",
      " ['dark'],\n",
      " ['theory', 'decision', 'making', 'under', 'dynamic'],\n",
      " ['gaussian', 'process', 'model', 'spectral', 'energy', 'distributions'],\n",
      " ['hidden', 'machine', 'learning', 'systems'],\n",
      " ['local', 'causal'],\n",
      " ['high', 'dimensional', 'em', 'algorithm', 'statistical', 'optimization'],\n",
      " ['optimization'],\n",
      " ['deep', 'convolutional', 'inverse', 'network'],\n",
      " ['sparse', 'lowrank', 'tensor', 'decomposition'],\n",
      " ['time', 'series', 'prediction'],\n",
      " ['differentially',\n",
      "  'private',\n",
      "  'learning',\n",
      "  'structured',\n",
      "  'discrete',\n",
      "  'distributions'],\n",
      " ['sample', 'complexity', 'learning', 'distance', 'metrics'],\n",
      " ['learning', 'recurrent', 'models'],\n",
      " ['robust', 'gaussian', 'graphical', 'modeling', 'graphical', 'lasso'],\n",
      " ['testing', 'samples'],\n",
      " ['estimating', 'index', 'observations', 'matrix', 'approach'],\n",
      " ['neural', 'adaptive', 'sequential', 'monte', 'carlo'],\n",
      " ['local', 'expectation', 'gradients', 'variational', 'inference'],\n",
      " ['variance',\n",
      "  'reduction',\n",
      "  'stochastic',\n",
      "  'gradient',\n",
      "  'descent',\n",
      "  'asynchronous',\n",
      "  'variants'],\n",
      " ['system', 'evaluation', 'application', 'active', 'learning'],\n",
      " ['superresolution'],\n",
      " ['unified', 'analysis', 'algorithms'],\n",
      " ['return',\n",
      "  'network',\n",
      "  'generative',\n",
      "  'models',\n",
      "  'discriminative',\n",
      "  'training',\n",
      "  'natural',\n",
      "  'image'],\n",
      " ['networks'],\n",
      " ['memory', 'sparse', 'recovery', 'model'],\n",
      " ['robust', 'spectral', 'inference', 'joint', 'stochastic', 'matrix'],\n",
      " ['fast', 'algorithms', 'regression'],\n",
      " ['adversarial', 'prediction', 'games'],\n",
      " ['asynchronous',\n",
      "  'parallel',\n",
      "  'stochastic',\n",
      "  'gradient',\n",
      "  'nonconvex',\n",
      "  'optimization'],\n",
      " ['control', 'locally', 'linear', 'latent', 'dynamics', 'model', 'control'],\n",
      " ['efficient', 'active', 'learning'],\n",
      " ['probabilistic', 'inference'],\n",
      " [],\n",
      " ['estimation', 'regression', 'symmetric', 'semidefinite'],\n",
      " [],\n",
      " ['deep', 'poisson', 'factor', 'modeling'],\n",
      " ['bayesian', 'optimization', 'exponential', 'convergence'],\n",
      " ['sample', 'complexity', 'reinforcement', 'learning'],\n",
      " ['learning'],\n",
      " ['subsampled', 'iteration', 'unified', 'algorithm', 'block', 'models'],\n",
      " ['accelerated', 'descent', 'continuous', 'discrete', 'time'],\n",
      " ['human', 'kernel'],\n",
      " ['prediction', 'deep', 'networks', 'games'],\n",
      " ['iteration', 'optimal', 'recovery', 'noisy', 'ica'],\n",
      " ['distributed', 'submodular', 'cover', 'massive', 'data'],\n",
      " ['detection', 'measure', 'embedding'],\n",
      " ['linear', 'value', 'function', 'approximation'],\n",
      " ['structured', 'estimation', 'general', 'bounds', 'applications'],\n",
      " ['stochastic', 'gradient', 'mcmc'],\n",
      " ['smooth', 'convex', 'optimization'],\n",
      " ['online', 'prediction'],\n",
      " ['learning', 'continuous', 'control', 'stochastic', 'value', 'gradients'],\n",
      " ['models', 'data', 'image', 'question', 'answering'],\n",
      " ['efficient', 'robust', 'automated', 'machine', 'learning'],\n",
      " ['spectral', 'descent', 'deep', 'learning'],\n",
      " ['recurrent', 'latent', 'variable', 'model', 'sequential', 'data'],\n",
      " ['fast', 'convergence', 'regularized', 'learning', 'games'],\n",
      " ['parallel',\n",
      "  'multidimensional',\n",
      "  'lstm',\n",
      "  'application',\n",
      "  'fast',\n",
      "  'image',\n",
      "  'segmentation'],\n",
      " ['hamiltonian', 'monte', 'carlo'],\n",
      " ['consistency', 'neighbors', 'prediction', 'stochastic'],\n",
      " ['nearly', 'optimal', 'private', 'lasso'],\n",
      " ['convergence',\n",
      "  'analysis',\n",
      "  'prediction',\n",
      "  'randomized',\n",
      "  'subspace',\n",
      "  'descent'],\n",
      " ['poisson', 'belief', 'network'],\n",
      " ['convergence', 'rates', 'subsampled', 'methods'],\n",
      " ['learning', 'bayesian', 'games'],\n",
      " ['statistical', 'data', 'analysis', 'kernel', 'perspective'],\n",
      " ['semisupervised', 'sequence', 'learning'],\n",
      " ['structured', 'deep', 'learning'],\n",
      " ['mixing', 'gibbs', 'sampling', 'class', 'factor', 'graphs'],\n",
      " ['convex', 'nonconvex', 'tensor', 'subspace', 'norm'],\n",
      " ['sample',\n",
      "  'complexity',\n",
      "  'bounds',\n",
      "  'iterative',\n",
      "  'stochastic',\n",
      "  'policy',\n",
      "  'optimization'],\n",
      " ['training', 'deep', 'neural', 'networks', 'weights'],\n",
      " ['interactive', 'control', 'diverse', 'complex', 'neural', 'networks'],\n",
      " ['submodular', 'metrics'],\n",
      " ['universal', 'primaldual', 'convex', 'optimization', 'framework'],\n",
      " ['learning', 'small', 'samples', 'an', 'analysis', 'decision'],\n",
      " ['more', 'improved', 'regret', 'bounds', 'bandits'],\n",
      " ['fast', 'memory', 'optimal', 'lowrank', 'matrix', 'approximation'],\n",
      " ['influence', 'networks'],\n",
      " ['learning', 'causal', 'graphs', 'small', 'interventions'],\n",
      " ['lower', 'bounds', 'convex', 'optimization'],\n",
      " ['poisson', 'multinomial'],\n",
      " ['largescale', 'bayesian', 'multilabel', 'learning', 'label', 'embeddings'],\n",
      " ['selfnormalized', 'learning'],\n",
      " ['fast', 'lifted', 'map', 'inference', 'partitioning'],\n",
      " ['data', 'sequential', 'decision', 'making'],\n",
      " ['elicitation', 'complexity'],\n",
      " ['decomposition', 'bounds', 'marginal', 'map'],\n",
      " ['discrete', 'classifiers'],\n",
      " ['class', 'network', 'models', 'spectral', 'clustering'],\n",
      " ['vectors'],\n",
      " ['causal', 'structure', 'learning'],\n",
      " ['principal',\n",
      "  'analysis',\n",
      "  'probability',\n",
      "  'measures',\n",
      "  'under',\n",
      "  'optimal',\n",
      "  'metric'],\n",
      " ['consistent', 'multilabel', 'classification'],\n",
      " ['parallel', 'entropy', 'search', 'global', 'optimization', 'functions'],\n",
      " ['stationary', 'restless', 'mixing', 'bandits'],\n",
      " ['semisupervised', 'logistic', 'regression', 'highdimensional', 'data'],\n",
      " ['gaussian', 'process', 'random'],\n",
      " ['kernel', 'detection'],\n",
      " ['adaptive', 'online', 'learning'],\n",
      " ['universal', 'optimization'],\n",
      " ['inference', 'point', 'processes', 'without', 'spectral', 'knowledge'],\n",
      " ['proximal', 'variational', 'inference'],\n",
      " ['minimization'],\n",
      " ['lasso', 'nonlinear', 'measurements', 'is', 'linear', 'measurements'],\n",
      " ['random', 'graphs'],\n",
      " ['bayesian', 'dark', 'knowledge'],\n",
      " ['matrix', 'completion', 'noisy', 'side', 'information'],\n",
      " ['multinomial', 'models', 'easy'],\n",
      " ['learning', 'bayesian', 'decision', 'theory'],\n",
      " ['structured', 'prediction'],\n",
      " ['learning', 'structured', 'output', 'deep', 'generative', 'models'],\n",
      " ['recommendation', 'recurrent'],\n",
      " ['learning',\n",
      "  'stationary',\n",
      "  'time',\n",
      "  'series',\n",
      "  'gaussian',\n",
      "  'processes',\n",
      "  'nonparametric',\n",
      "  'kernels'],\n",
      " ['market', 'framework', 'private', 'data'],\n",
      " ['lifted', 'inference', 'rules', 'constraints'],\n",
      " ['gradient', 'estimation', 'stochastic', 'graphs'],\n",
      " ['entropy', 'stochastic', 'search'],\n",
      " ['semisupervised', 'learning', 'networks'],\n",
      " ['embedding', 'inference', 'structured', 'multilabel', 'prediction'],\n",
      " ['variational', 'inference'],\n",
      " ['recursive', 'training', 'convolutional', 'networks', 'prediction'],\n",
      " ['dual',\n",
      "  'augmented',\n",
      "  'block',\n",
      "  'minimization',\n",
      "  'framework',\n",
      "  'learning',\n",
      "  'memory'],\n",
      " ['optimal', 'testing', 'properties', 'distributions'],\n",
      " ['efficient',\n",
      "  'learning',\n",
      "  'continuoustime',\n",
      "  'hidden',\n",
      "  'markov',\n",
      "  'models',\n",
      "  'disease'],\n",
      " ['expectation', 'particle', 'belief', 'propagation'],\n",
      " ['latent', 'bayesian', 'integrating', 'individual', 'population', 'models'],\n",
      " ['robust', 'logistic', 'regression'],\n",
      " ['variational', 'local']]\n"
     ]
    }
   ],
   "source": [
    "# remove words that appear only once\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'3d': 127,\n",
      " u'accelerated': 112,\n",
      " u'accurate': 68,\n",
      " u'active': 183,\n",
      " u'adaptive': 11,\n",
      " u'adversarial': 260,\n",
      " u'algorithm': 47,\n",
      " u'algorithmic': 4,\n",
      " u'algorithms': 90,\n",
      " u'an': 25,\n",
      " u'analysis': 171,\n",
      " u'answering': 321,\n",
      " u'application': 293,\n",
      " u'applications': 233,\n",
      " u'approach': 109,\n",
      " u'approval': 317,\n",
      " u'approximate': 279,\n",
      " u'approximation': 292,\n",
      " u'arbitrary': 210,\n",
      " u'asynchronous': 289,\n",
      " u'augmented': 349,\n",
      " u'automated': 350,\n",
      " u'backpropagation': 243,\n",
      " u'bandits': 75,\n",
      " u'based': 163,\n",
      " u'bayesian': 12,\n",
      " u'belief': 266,\n",
      " u'beyond': 160,\n",
      " u'bidirectional': 83,\n",
      " u'bipartite': 194,\n",
      " u'blackbox': 174,\n",
      " u'block': 178,\n",
      " u'bounding': 86,\n",
      " u'bounds': 271,\n",
      " u'brain': 170,\n",
      " u'carlo': 222,\n",
      " u'causal': 274,\n",
      " u'chain': 185,\n",
      " u'class': 126,\n",
      " u'classification': 169,\n",
      " u'classifiers': 275,\n",
      " u'clustering': 28,\n",
      " u'combinatorial': 283,\n",
      " u'completion': 246,\n",
      " u'complex': 340,\n",
      " u'complexity': 252,\n",
      " u'compressive': 150,\n",
      " u'connections': 220,\n",
      " u'consistency': 335,\n",
      " u'consistent': 312,\n",
      " u'constrained': 128,\n",
      " u'constraints': 182,\n",
      " u'continuous': 347,\n",
      " u'continuoustime': 332,\n",
      " u'control': 345,\n",
      " u'convergence': 98,\n",
      " u'convex': 269,\n",
      " u'convolutional': 84,\n",
      " u'coordinate': 208,\n",
      " u'covariance': 200,\n",
      " u'cover': 52,\n",
      " u'cyclic': 272,\n",
      " u'dark': 354,\n",
      " u'data': 115,\n",
      " u'decision': 311,\n",
      " u'decomposition': 228,\n",
      " u'deep': 55,\n",
      " u'descent': 41,\n",
      " u'detection': 35,\n",
      " u'differences': 307,\n",
      " u'differentially': 231,\n",
      " u'diffusion': 326,\n",
      " u'dimensional': 342,\n",
      " u'discrete': 144,\n",
      " u'discriminative': 273,\n",
      " u'disease': 192,\n",
      " u'distance': 258,\n",
      " u'distributed': 92,\n",
      " u'distributions': 281,\n",
      " u'divergences': 118,\n",
      " u'diverse': 278,\n",
      " u'dual': 209,\n",
      " u'dueling': 101,\n",
      " u'dynamic': 324,\n",
      " u'dynamical': 305,\n",
      " u'dynamics': 59,\n",
      " u'easy': 190,\n",
      " u'efficient': 148,\n",
      " u'elicitation': 159,\n",
      " u'em': 291,\n",
      " u'embedding': 154,\n",
      " u'embeddings': 38,\n",
      " u'empirical': 205,\n",
      " u'endtoend': 315,\n",
      " u'energy': 167,\n",
      " u'entropy': 362,\n",
      " u'estimating': 137,\n",
      " u'estimation': 91,\n",
      " u'estimators': 120,\n",
      " u'evaluation': 106,\n",
      " u'exact': 216,\n",
      " u'expectation': 346,\n",
      " u'exponential': 225,\n",
      " u'factor': 319,\n",
      " u'families': 224,\n",
      " u'fast': 69,\n",
      " u'faster': 32,\n",
      " u'feature': 263,\n",
      " u'features': 193,\n",
      " u'filtering': 294,\n",
      " u'framework': 155,\n",
      " u'frankwolfe': 140,\n",
      " u'function': 181,\n",
      " u'functions': 173,\n",
      " u'games': 329,\n",
      " u'gap': 204,\n",
      " u'gaussian': 79,\n",
      " u'general': 179,\n",
      " u'generalization': 5,\n",
      " u'generalized': 158,\n",
      " u'generative': 207,\n",
      " u'gibbs': 351,\n",
      " u'global': 142,\n",
      " u'gradient': 43,\n",
      " u'gradients': 341,\n",
      " u'graph': 310,\n",
      " u'graphical': 218,\n",
      " u'graphs': 29,\n",
      " u'group': 262,\n",
      " u'guarantees': 196,\n",
      " u'hamiltonian': 226,\n",
      " u'hidden': 77,\n",
      " u'hierarchical': 60,\n",
      " u'high': 353,\n",
      " u'highdimensional': 157,\n",
      " u'human': 97,\n",
      " u'hybrid': 122,\n",
      " u'ica': 147,\n",
      " u'image': 23,\n",
      " u'improved': 270,\n",
      " u'incomplete': 117,\n",
      " u'index': 320,\n",
      " u'individual': 135,\n",
      " u'inference': 7,\n",
      " u'infinite': 304,\n",
      " u'influence': 333,\n",
      " u'information': 237,\n",
      " u'integrating': 56,\n",
      " u'interactive': 53,\n",
      " u'interpretable': 306,\n",
      " u'interventions': 288,\n",
      " u'inverse': 313,\n",
      " u'is': 187,\n",
      " u'iteration': 268,\n",
      " u'iterative': 300,\n",
      " u'joint': 327,\n",
      " u'kernel': 195,\n",
      " u'kernels': 301,\n",
      " u'knowledge': 143,\n",
      " u'label': 3,\n",
      " u'langevin': 14,\n",
      " u'large': 132,\n",
      " u'largescale': 13,\n",
      " u'lasso': 264,\n",
      " u'latent': 67,\n",
      " u'lda': 145,\n",
      " u'learning': 2,\n",
      " u'lifted': 221,\n",
      " u'likelihood': 211,\n",
      " u'linear': 48,\n",
      " u'local': 37,\n",
      " u'locally': 66,\n",
      " u'logarithmic': 21,\n",
      " u'logistic': 364,\n",
      " u'lower': 296,\n",
      " u'lowrank': 355,\n",
      " u'lstm': 201,\n",
      " u'machine': 165,\n",
      " u'making': 234,\n",
      " u'manifold': 64,\n",
      " u'map': 99,\n",
      " u'marginal': 151,\n",
      " u'market': 348,\n",
      " u'markov': 253,\n",
      " u'massive': 239,\n",
      " u'matching': 146,\n",
      " u'matrix': 156,\n",
      " u'maximum': 212,\n",
      " u'maxmargin': 297,\n",
      " u'mcmc': 129,\n",
      " u'measure': 322,\n",
      " u'measurements': 44,\n",
      " u'measures': 287,\n",
      " u'memory': 199,\n",
      " u'method': 82,\n",
      " u'methods': 78,\n",
      " u'metric': 318,\n",
      " u'metrics': 356,\n",
      " u'minimization': 42,\n",
      " u'mixing': 284,\n",
      " u'mixture': 6,\n",
      " u'mixtures': 139,\n",
      " u'model': 65,\n",
      " u'modeling': 245,\n",
      " u'models': 9,\n",
      " u'monte': 223,\n",
      " u'more': 303,\n",
      " u'multiclass': 19,\n",
      " u'multidimensional': 214,\n",
      " u'multilabel': 184,\n",
      " u'multinomial': 361,\n",
      " u'natural': 26,\n",
      " u'nearly': 58,\n",
      " u'neighbors': 344,\n",
      " u'network': 110,\n",
      " u'networks': 36,\n",
      " u'neural': 61,\n",
      " u'noise': 1,\n",
      " u'noisy': 176,\n",
      " u'nonconvex': 111,\n",
      " u'nonlinear': 88,\n",
      " u'nonparametric': 119,\n",
      " u'norm': 339,\n",
      " u'object': 34,\n",
      " u'observations': 276,\n",
      " u'online': 22,\n",
      " u'optimal': 57,\n",
      " u'optimization': 17,\n",
      " u'order': 256,\n",
      " u'output': 250,\n",
      " u'parallel': 30,\n",
      " u'pareto': 73,\n",
      " u'particle': 352,\n",
      " u'partitioning': 323,\n",
      " u'path': 285,\n",
      " u'pca': 114,\n",
      " u'perceptual': 235,\n",
      " u'perspective': 290,\n",
      " u'planning': 134,\n",
      " u'point': 295,\n",
      " u'poisson': 164,\n",
      " u'policy': 105,\n",
      " u'polynomials': 138,\n",
      " u'population': 62,\n",
      " u'practical': 257,\n",
      " u'prediction': 20,\n",
      " u'primaldual': 334,\n",
      " u'principal': 308,\n",
      " u'private': 232,\n",
      " u'probabilistic': 71,\n",
      " u'probability': 282,\n",
      " u'problems': 123,\n",
      " u'process': 8,\n",
      " u'processes': 302,\n",
      " u'programming': 45,\n",
      " u'propagation': 265,\n",
      " u'properties': 54,\n",
      " u'proposals': 125,\n",
      " u'proximal': 113,\n",
      " u'question': 343,\n",
      " u'random': 40,\n",
      " u'randomized': 198,\n",
      " u'rank': 46,\n",
      " u'rates': 238,\n",
      " u'recommendation': 267,\n",
      " u'recovery': 358,\n",
      " u'recurrent': 72,\n",
      " u'recursive': 219,\n",
      " u'reduced': 337,\n",
      " u'reduction': 261,\n",
      " u'region': 33,\n",
      " u'regression': 186,\n",
      " u'regret': 74,\n",
      " u'regularization': 299,\n",
      " u'regularized': 124,\n",
      " u'reinforcement': 314,\n",
      " u'representations': 330,\n",
      " u'response': 131,\n",
      " u'restless': 309,\n",
      " u'return': 206,\n",
      " u'ridge': 102,\n",
      " u'risk': 103,\n",
      " u'robust': 16,\n",
      " u'rules': 202,\n",
      " u'sample': 80,\n",
      " u'samples': 357,\n",
      " u'sampling': 15,\n",
      " u'scalable': 254,\n",
      " u'scale': 338,\n",
      " u'scoring': 162,\n",
      " u'screening': 203,\n",
      " u'search': 217,\n",
      " u'second': 255,\n",
      " u'segmentation': 24,\n",
      " u'selection': 121,\n",
      " u'selfnormalized': 316,\n",
      " u'semibandit': 108,\n",
      " u'semidefinite': 39,\n",
      " u'semisupervised': 215,\n",
      " u'sequence': 27,\n",
      " u'sequential': 10,\n",
      " u'series': 152,\n",
      " u'set': 51,\n",
      " u'sets': 213,\n",
      " u'sgd': 180,\n",
      " u'side': 277,\n",
      " u'single': 286,\n",
      " u'sketching': 230,\n",
      " u'small': 360,\n",
      " u'smooth': 49,\n",
      " u'smoothness': 175,\n",
      " u'solving': 188,\n",
      " u'sparse': 116,\n",
      " u'spatial': 325,\n",
      " u'spectral': 76,\n",
      " u'stationary': 363,\n",
      " u'statistical': 197,\n",
      " u'steins': 81,\n",
      " u'stochastic': 70,\n",
      " u'streaming': 94,\n",
      " u'strong': 100,\n",
      " u'structural': 248,\n",
      " u'structure': 136,\n",
      " u'structured': 133,\n",
      " u'submodular': 50,\n",
      " u'subsampled': 359,\n",
      " u'subspace': 107,\n",
      " u'superresolution': 85,\n",
      " u'supervised': 227,\n",
      " u'symmetric': 0,\n",
      " u'synthesis': 89,\n",
      " u'system': 328,\n",
      " u'systems': 189,\n",
      " u'tasks': 251,\n",
      " u'tensor': 229,\n",
      " u'testing': 331,\n",
      " u'text': 168,\n",
      " u'theory': 153,\n",
      " u'time': 18,\n",
      " u'topk': 104,\n",
      " u'towards': 31,\n",
      " u'tractable': 161,\n",
      " u'training': 166,\n",
      " u'trajectories': 191,\n",
      " u'trees': 130,\n",
      " u'uncertainty': 259,\n",
      " u'under': 249,\n",
      " u'unified': 247,\n",
      " u'universal': 87,\n",
      " u'unknown': 172,\n",
      " u'ustatistics': 93,\n",
      " u'value': 280,\n",
      " u'variable': 63,\n",
      " u'variance': 336,\n",
      " u'variants': 141,\n",
      " u'variational': 95,\n",
      " u'vectors': 149,\n",
      " u'very': 242,\n",
      " u'view': 241,\n",
      " u'visual': 96,\n",
      " u'voting': 298,\n",
      " u'walk': 240,\n",
      " u'weights': 244,\n",
      " u'when': 236,\n",
      " u'without': 177}\n"
     ]
    }
   ],
   "source": [
    "#ASSOCIATE EVERY WORD WITH AN INTEGER (FOR COMPUTATIONAL EFFICIENCY)\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[],\n",
      " [(0, 1), (1, 1), (2, 1), (3, 1)],\n",
      " [(4, 1), (5, 1)],\n",
      " [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)],\n",
      " [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)],\n",
      " [(16, 1), (17, 1)],\n",
      " [(18, 1), (19, 1), (20, 1), (21, 1), (22, 1)],\n",
      " [(23, 1), (24, 1)],\n",
      " [(23, 1), (25, 1), (26, 1), (27, 1)],\n",
      " [(28, 1), (29, 1), (30, 1)],\n",
      " [(31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1)],\n",
      " [(37, 1), (38, 1)],\n",
      " [(39, 1),\n",
      "  (40, 1),\n",
      "  (41, 1),\n",
      "  (42, 1),\n",
      "  (43, 1),\n",
      "  (44, 1),\n",
      "  (45, 1),\n",
      "  (46, 1),\n",
      "  (47, 1),\n",
      "  (48, 1)],\n",
      " [(49, 1), (50, 1), (51, 1), (52, 1), (53, 1)],\n",
      " [(2, 1), (34, 1), (54, 1), (55, 1), (56, 1)],\n",
      " [(57, 1), (58, 1)],\n",
      " [(9, 1), (59, 1), (60, 1), (61, 1), (62, 1)],\n",
      " [(2, 1), (12, 1), (48, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1)],\n",
      " [(2, 1)],\n",
      " [(7, 1), (9, 1), (68, 1), (69, 1)],\n",
      " [(17, 1), (70, 1), (71, 1)],\n",
      " [(4, 1), (72, 1)],\n",
      " [],\n",
      " [(73, 1), (74, 1), (75, 1)],\n",
      " [(76, 1), (77, 1), (78, 1), (79, 2)],\n",
      " [(80, 1), (81, 1), (82, 1)],\n",
      " [(36, 1), (72, 1), (83, 1), (84, 1), (85, 1)],\n",
      " [(86, 1)],\n",
      " [(38, 1), (47, 1), (69, 1), (87, 1), (88, 1)],\n",
      " [(36, 1), (61, 1), (84, 1), (89, 1)],\n",
      " [(90, 1), (91, 1), (92, 1), (93, 1)],\n",
      " [(7, 1), (12, 1), (92, 1), (94, 1), (95, 1)],\n",
      " [(2, 1), (96, 1), (97, 1)],\n",
      " [(7, 1), (48, 1), (49, 1), (98, 1), (99, 1), (100, 1)],\n",
      " [(75, 1), (101, 1)],\n",
      " [(35, 1), (57, 1), (102, 1), (103, 1)],\n",
      " [(19, 1), (104, 1)],\n",
      " [(105, 1), (106, 1)],\n",
      " [(107, 1)],\n",
      " [(2, 1), (22, 1), (70, 1), (108, 1)],\n",
      " [(2, 1), (7, 1)],\n",
      " [(12, 1), (15, 1), (61, 1), (109, 1), (110, 1)],\n",
      " [(43, 1), (45, 1), (78, 1), (111, 1), (112, 1), (113, 1)],\n",
      " [(114, 1), (115, 1), (116, 1), (117, 1)],\n",
      " [(118, 1), (119, 1), (120, 1)],\n",
      " [(11, 1), (15, 1), (121, 1)],\n",
      " [(17, 1), (111, 1), (122, 1), (123, 1), (124, 1)],\n",
      " [(34, 2), (35, 1), (68, 1), (125, 1), (126, 1), (127, 1)],\n",
      " [(21, 1), (74, 1), (75, 1), (90, 1), (128, 1)],\n",
      " [(36, 1), (61, 1)],\n",
      " [(40, 1), (129, 1), (130, 1)],\n",
      " [(65, 1), (131, 1)],\n",
      " [(2, 1), (76, 1), (132, 1), (133, 1)],\n",
      " [(7, 1), (134, 1), (135, 1), (136, 1)],\n",
      " [(6, 1), (9, 1), (137, 1), (138, 1), (139, 1)],\n",
      " [(17, 1), (48, 1), (98, 1), (140, 1), (141, 1), (142, 1)],\n",
      " [(55, 1), (143, 1)],\n",
      " [(144, 1), (145, 1), (146, 1), (147, 1)],\n",
      " [(128, 1), (148, 1), (149, 1), (150, 1)],\n",
      " [(7, 1), (140, 1), (151, 1)],\n",
      " [(2, 1), (18, 1), (90, 1), (152, 1), (153, 1)],\n",
      " [(76, 1), (150, 1), (154, 1)],\n",
      " [(17, 1), (46, 1), (91, 1), (111, 1), (155, 1), (156, 1)],\n",
      " [(7, 1), (95, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (48, 1), (120, 1), (157, 1), (158, 1)],\n",
      " [(17, 1), (22, 1)],\n",
      " [(22, 1), (46, 1), (75, 1), (101, 1), (109, 1), (159, 1)],\n",
      " [(50, 1), (160, 1)],\n",
      " [(2, 1), (12, 1), (52, 1), (110, 1), (136, 1), (161, 1)],\n",
      " [(2, 1), (9, 1), (13, 1), (162, 1), (163, 1), (164, 1)],\n",
      " [(165, 1), (166, 1), (167, 1)],\n",
      " [(36, 1), (84, 1), (168, 1), (169, 1)],\n",
      " [(16, 1), (48, 1), (170, 1), (171, 1)],\n",
      " [(17, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1)],\n",
      " [(65, 1), (70, 1), (177, 1), (178, 1), (179, 1)],\n",
      " [(2, 1), (55, 1), (180, 1)],\n",
      " [(181, 1), (182, 1)],\n",
      " [(2, 1), (100, 1), (183, 1)],\n",
      " [(169, 1), (184, 1), (185, 1)],\n",
      " [(16, 1), (186, 1)],\n",
      " [(37, 1), (38, 1), (116, 1), (169, 1), (184, 1)],\n",
      " [(40, 1), (48, 1), (58, 1), (187, 1), (188, 2), (189, 2), (190, 1)],\n",
      " [(136, 1), (155, 1), (191, 1), (192, 1)],\n",
      " [(16, 1), (28, 1), (107, 1), (193, 1)],\n",
      " [(114, 1), (116, 1), (194, 1)],\n",
      " [(69, 1), (102, 1), (186, 1), (195, 1), (196, 1), (197, 1), (198, 1)],\n",
      " [(2, 1), (22, 1), (199, 1)],\n",
      " [(9, 1), (61, 1), (84, 1), (171, 1), (200, 1)],\n",
      " [(2, 1), (84, 1), (109, 1), (110, 1), (165, 1), (201, 1)],\n",
      " [(9, 1), (19, 1), (116, 1), (202, 1), (203, 1), (204, 1)],\n",
      " [(80, 1), (118, 1), (144, 1), (205, 1)],\n",
      " [(65, 1), (80, 1), (195, 1), (197, 1)],\n",
      " [(171, 1)],\n",
      " [(5, 1), (50, 1), (52, 1), (206, 1)],\n",
      " [(9, 1), (36, 1), (61, 1), (72, 1), (83, 1), (207, 1)],\n",
      " [(15, 1), (198, 1), (208, 1), (209, 1), (210, 1)],\n",
      " [(2, 1), (210, 1), (211, 1), (212, 1), (213, 1)],\n",
      " [(2, 1), (17, 1), (36, 1), (55, 1), (61, 1), (72, 1), (214, 1)],\n",
      " [(13, 1), (71, 1), (177, 1), (196, 1)],\n",
      " [(36, 1), (61, 1), (84, 1)],\n",
      " [(17, 1), (64, 1), (79, 1), (139, 1), (156, 1)],\n",
      " [(33, 1), (36, 1), (61, 1), (84, 1), (154, 1), (168, 1), (215, 1)],\n",
      " [(7, 1), (9, 1), (30, 1), (99, 1), (216, 1), (217, 1), (218, 1), (219, 1)],\n",
      " [(36, 1), (61, 1), (72, 1), (84, 1), (220, 1)],\n",
      " [(7, 1), (86, 1), (221, 1)],\n",
      " [(148, 1), (195, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 1)],\n",
      " [(48, 1), (108, 1)],\n",
      " [(2, 1), (89, 1)],\n",
      " [(2, 1), (36, 1), (37, 1), (72, 1), (227, 1)],\n",
      " [(69, 1), (228, 1), (229, 1), (230, 1)],\n",
      " [(28, 1), (107, 1), (231, 1), (232, 1)],\n",
      " [(20, 1), (22, 1), (90, 1), (123, 1), (179, 1)],\n",
      " [(28, 1), (38, 1), (173, 1), (233, 1)],\n",
      " [(13, 1),\n",
      "  (42, 1),\n",
      "  (90, 1),\n",
      "  (93, 1),\n",
      "  (103, 1),\n",
      "  (117, 1),\n",
      "  (163, 1),\n",
      "  (180, 1),\n",
      "  (205, 1)],\n",
      " [(29, 1), (75, 1), (77, 1), (104, 1), (121, 1), (194, 1)],\n",
      " [(170, 1), (234, 1), (235, 1), (236, 1), (237, 1)],\n",
      " [(9, 1), (69, 1), (79, 1), (157, 1), (169, 1), (207, 1), (238, 1)],\n",
      " [(28, 1), (69, 1), (92, 1), (115, 1), (239, 1)],\n",
      " [(40, 1), (97, 1), (199, 1), (217, 1), (240, 1)],\n",
      " [(17, 1), (65, 1), (111, 1), (116, 1), (197, 1), (218, 1), (229, 1)],\n",
      " [(2, 1), (91, 1), (98, 1), (183, 1), (211, 1), (212, 1), (238, 1)],\n",
      " [(72, 1), (89, 1), (127, 1), (241, 1)],\n",
      " [(36, 1),\n",
      "  (43, 1),\n",
      "  (55, 1),\n",
      "  (116, 1),\n",
      "  (132, 1),\n",
      "  (148, 1),\n",
      "  (166, 1),\n",
      "  (216, 1),\n",
      "  (242, 1)],\n",
      " [(243, 1)],\n",
      " [(42, 1), (123, 1), (186, 1)],\n",
      " [(2, 1), (61, 1), (110, 1), (148, 1), (220, 1), (244, 1)],\n",
      " [(40, 1), (57, 1), (193, 1), (238, 1)],\n",
      " [(12, 1), (62, 1), (245, 1)],\n",
      " [(12, 1), (71, 1), (140, 1), (196, 1)],\n",
      " [(15, 1), (20, 1), (27, 1), (36, 1), (61, 1), (72, 1)],\n",
      " [(156, 1),\n",
      "  (179, 1),\n",
      "  (182, 1),\n",
      "  (241, 1),\n",
      "  (246, 1),\n",
      "  (247, 1),\n",
      "  (248, 1),\n",
      "  (249, 1)],\n",
      " [(2, 1), (148, 1), (195, 1), (250, 1), (251, 1)],\n",
      " [(9, 1), (77, 1), (119, 1), (252, 1), (253, 1), (254, 1)],\n",
      " [(95, 1), (222, 1), (223, 1)],\n",
      " [(81, 1), (82, 2), (255, 1), (256, 1)],\n",
      " [(57, 1), (257, 1), (258, 1)],\n",
      " [(2, 1), (249, 1), (259, 1)],\n",
      " [(14, 1), (171, 1), (222, 1), (223, 1)],\n",
      " [(55, 1), (96, 1)],\n",
      " [(46, 1), (76, 1), (91, 1), (156, 1), (246, 1)],\n",
      " [(2, 1), (22, 1), (260, 1)],\n",
      " [(60, 1), (133, 1), (261, 1), (262, 1), (263, 1), (264, 1)],\n",
      " [(146, 1), (265, 1), (266, 1)],\n",
      " [(15, 1), (22, 1), (148, 1), (267, 1)],\n",
      " [(41, 1),\n",
      "  (123, 1),\n",
      "  (178, 1),\n",
      "  (208, 1),\n",
      "  (252, 1),\n",
      "  (268, 1),\n",
      "  (269, 1),\n",
      "  (270, 1),\n",
      "  (271, 1),\n",
      "  (272, 1)],\n",
      " [(7, 1), (35, 1), (99, 1), (221, 1)],\n",
      " [(197, 1)],\n",
      " [(2, 1), (16, 1), (273, 1)],\n",
      " [(75, 1), (109, 1), (274, 1)],\n",
      " [(215, 1), (254, 1), (275, 1)],\n",
      " [(2, 1), (22, 1), (79, 1), (276, 1), (277, 1)],\n",
      " [(29, 1), (91, 1), (116, 1), (232, 1)],\n",
      " [(34, 1), (51, 1), (125, 1), (217, 1), (278, 1)],\n",
      " [(7, 1), (69, 1), (70, 1), (95, 1), (243, 1), (255, 1), (256, 1)],\n",
      " [(32, 1), (78, 1), (178, 1), (198, 1), (228, 1), (279, 1), (280, 1)],\n",
      " [(38, 1), (67, 1), (115, 1), (146, 1), (195, 1), (281, 1)],\n",
      " [(7, 1), (8, 1), (9, 1), (79, 1), (174, 1), (254, 1)],\n",
      " [(9, 1), (69, 1), (83, 1), (91, 1), (253, 1), (282, 1)],\n",
      " [(9, 1), (71, 1), (95, 1), (218, 1), (271, 1)],\n",
      " [(48, 1), (68, 1), (78, 1), (95, 1), (131, 1), (200, 1)],\n",
      " [(75, 1), (283, 1)],\n",
      " [(18, 1), (80, 1), (91, 1), (253, 1), (284, 1), (285, 1), (286, 1)],\n",
      " [(43, 1), (103, 1), (105, 1), (287, 1)],\n",
      " [(42, 1), (69, 1), (103, 1), (205, 1), (238, 1)],\n",
      " [(9, 1), (23, 1), (36, 1), (55, 1), (207, 1), (260, 1)],\n",
      " [(24, 1), (55, 1), (61, 1), (110, 1), (215, 1)],\n",
      " [(2, 1), (11, 1), (17, 1), (111, 1), (238, 1)],\n",
      " [(2, 1), (29, 1), (172, 1), (272, 1), (274, 1), (288, 1)],\n",
      " [(16, 1), (17, 1), (109, 1)],\n",
      " [(1, 2), (17, 1), (70, 1), (180, 1), (187, 1), (269, 1), (289, 1)],\n",
      " [(2, 1), (251, 1)],\n",
      " [(48, 1), (57, 1), (88, 1), (91, 1), (172, 1), (249, 1)],\n",
      " [(2, 1), (193, 1), (195, 1), (262, 1), (290, 1)],\n",
      " [(90, 1), (124, 1), (155, 1), (196, 1), (197, 1), (247, 1), (291, 1)],\n",
      " [(11, 1), (17, 1), (70, 1), (213, 1)],\n",
      " [(17, 1), (70, 1), (160, 1)],\n",
      " [(8, 1), (57, 1), (61, 1), (161, 1), (292, 1), (293, 1), (294, 1), (295, 1)],\n",
      " [(114, 1), (116, 1), (271, 1), (296, 1)],\n",
      " [(2, 1), (297, 1), (298, 1)],\n",
      " [(2, 1), (299, 1), (300, 1)],\n",
      " [(40, 1), (240, 1), (301, 1)],\n",
      " [(79, 1), (116, 1), (129, 1), (302, 1)],\n",
      " [(187, 1), (299, 1), (303, 1)],\n",
      " [(65, 1), (304, 1), (305, 1)],\n",
      " [(271, 1), (285, 1), (296, 1), (299, 1)],\n",
      " [(59, 1), (110, 1), (134, 1), (285, 1)],\n",
      " [],\n",
      " [(171, 1), (281, 1), (306, 1), (307, 2), (308, 1)],\n",
      " [(75, 1), (236, 1), (309, 1)],\n",
      " [(9, 1), (29, 1), (185, 1), (310, 1)],\n",
      " [(17, 1), (130, 1), (148, 1), (311, 1)],\n",
      " [(2, 1), (8, 1), (71, 1), (79, 1)],\n",
      " [(2, 1), (66, 1), (173, 1), (312, 1), (313, 1), (314, 1)],\n",
      " [(2, 1), (17, 1), (92, 1), (252, 1), (269, 1)],\n",
      " [(2, 1), (55, 1), (145, 1), (265, 1), (315, 1)],\n",
      " [(17, 1), (73, 1), (121, 1)],\n",
      " [(9, 1), (316, 1)],\n",
      " [(47, 1), (57, 1), (70, 1), (74, 1), (296, 1)],\n",
      " [(57, 1), (187, 1), (298, 1), (317, 2)],\n",
      " [(2, 1), (318, 1)],\n",
      " [(16, 1), (37, 1), (114, 1), (171, 1)],\n",
      " [(2, 1), (199, 1)],\n",
      " [(9, 1), (55, 1), (207, 1), (297, 1)],\n",
      " [(40, 1), (193, 1), (301, 1)],\n",
      " [(36, 1), (319, 1)],\n",
      " [(2, 1), (12, 1), (36, 1)],\n",
      " [(9, 1), (156, 1), (246, 1), (249, 1), (286, 1), (320, 1)],\n",
      " [(96, 1), (321, 1)],\n",
      " [(6, 1), (7, 1), (9, 1), (40, 1), (129, 1), (322, 1)],\n",
      " [(94, 1), (323, 1)],\n",
      " [(2, 1), (115, 1)],\n",
      " [(324, 1)],\n",
      " [(23, 1), (207, 1), (245, 1), (325, 1)],\n",
      " [(16, 1), (114, 1), (115, 1)],\n",
      " [(9, 1), (15, 1), (50, 1), (71, 1)],\n",
      " [(8, 1), (65, 1), (110, 1), (237, 1), (295, 1), (326, 1), (327, 1)],\n",
      " [(2, 2), (227, 1), (305, 1), (328, 1)],\n",
      " [(329, 1)],\n",
      " [(69, 1), (282, 1), (287, 1), (330, 1), (331, 1)],\n",
      " [(2, 1), (34, 1)],\n",
      " [(171, 1), (301, 1)],\n",
      " [],\n",
      " [(36, 1), (325, 1)],\n",
      " [(36, 1), (271, 1), (326, 1), (332, 1), (333, 1)],\n",
      " [(5, 1), (19, 1), (90, 1), (271, 1)],\n",
      " [(48, 1), (61, 1), (157, 1), (158, 1), (171, 1), (189, 1), (305, 1)],\n",
      " [(2, 1)],\n",
      " [(151, 1), (186, 1)],\n",
      " [(26, 1), (36, 1), (61, 1)],\n",
      " [(7, 1), (17, 1), (30, 1), (148, 1), (222, 1), (223, 1)],\n",
      " [(2, 1), (11, 1), (23, 1), (78, 1), (197, 1), (334, 1)],\n",
      " [(7, 1), (9, 1), (95, 1)],\n",
      " [(78, 1), (237, 1), (254, 1), (294, 1), (310, 1), (335, 1)],\n",
      " [(75, 1), (283, 1)],\n",
      " [(2, 1), (95, 1), (237, 1), (314, 1)],\n",
      " [(16, 1), (155, 1), (248, 1), (310, 1)],\n",
      " [(91, 1), (187, 1)],\n",
      " [(2, 1), (20, 1), (128, 1), (148, 1), (310, 1)],\n",
      " [(6, 1), (9, 1), (122, 1)],\n",
      " [(2, 1), (25, 1), (116, 1), (138, 1), (155, 1), (183, 1), (230, 1), (310, 1)],\n",
      " [(17, 1), (37, 1), (175, 1), (336, 1), (337, 1)],\n",
      " [(31, 1), (153, 1), (237, 1), (338, 1)],\n",
      " [(75, 1), (175, 1)],\n",
      " [(44, 1), (91, 1), (133, 1), (157, 1), (160, 1)],\n",
      " [(76, 1), (299, 1), (310, 1), (330, 1), (339, 1)],\n",
      " [(2, 1), (29, 1), (36, 1), (84, 1)],\n",
      " [(50, 1), (69, 1), (90, 1), (196, 1), (233, 1), (323, 1)],\n",
      " [(2, 1), (161, 1), (282, 1), (340, 1)],\n",
      " [(257, 1), (341, 1)],\n",
      " [(109, 1), (121, 1), (204, 1), (207, 1), (263, 1), (306, 1)],\n",
      " [(11, 1), (36, 1), (61, 1), (153, 1), (261, 1)],\n",
      " [(43, 1), (70, 1), (90, 1), (98, 1), (129, 1)],\n",
      " [(2, 1), (133, 1), (224, 1), (225, 1), (304, 1), (342, 1)],\n",
      " [(23, 1), (78, 1), (165, 1), (343, 1)],\n",
      " [(41, 1), (43, 1), (70, 1), (336, 1), (337, 1), (344, 1)],\n",
      " [(80, 1), (148, 1), (249, 1), (259, 1), (285, 1), (345, 1)],\n",
      " [(70, 1), (265, 1), (346, 1)],\n",
      " [(7, 1), (99, 1), (279, 1), (347, 1)],\n",
      " [(70, 1), (88, 1), (171, 1), (338, 1), (341, 1)],\n",
      " [(5, 1), (11, 1), (115, 1), (171, 1)],\n",
      " [(162, 1), (202, 1), (348, 1)],\n",
      " [(41, 1), (45, 1), (48, 1), (116, 1), (208, 1), (209, 1), (349, 1)],\n",
      " [(36, 1), (55, 1), (166, 1), (242, 1)],\n",
      " [(12, 1), (25, 1), (65, 1), (121, 1), (183, 1), (293, 1), (350, 1)],\n",
      " [(9, 1), (77, 1), (253, 1), (304, 1), (351, 1), (352, 1)],\n",
      " [(2, 1), (115, 1), (191, 1)],\n",
      " [(12, 1), (155, 1), (234, 1), (235, 1), (245, 1), (311, 1)],\n",
      " [(17, 1), (36, 1), (55, 1), (61, 1)],\n",
      " [(63, 1), (153, 1), (203, 1), (335, 1), (342, 1), (353, 1)],\n",
      " [(36, 1), (199, 1), (315, 1)],\n",
      " [(36, 1), (61, 1), (76, 1), (84, 1), (330, 1)],\n",
      " [(22, 1), (43, 1)],\n",
      " [(27, 1), (36, 1), (55, 1), (245, 1), (266, 1)],\n",
      " [(354, 1)],\n",
      " [(153, 1), (234, 1), (249, 1), (311, 1), (324, 1)],\n",
      " [(8, 1), (65, 1), (76, 1), (79, 1), (167, 1), (281, 1)],\n",
      " [(2, 1), (77, 1), (165, 1), (189, 1)],\n",
      " [(37, 1), (274, 1)],\n",
      " [(17, 1), (47, 1), (197, 1), (291, 1), (342, 1), (353, 1)],\n",
      " [(17, 1)],\n",
      " [(55, 1), (84, 1), (110, 1), (313, 1)],\n",
      " [(116, 1), (228, 1), (229, 1), (355, 1)],\n",
      " [(18, 1), (20, 1), (152, 1)],\n",
      " [(2, 1), (133, 1), (144, 1), (231, 1), (232, 1), (281, 1)],\n",
      " [(2, 1), (80, 1), (252, 1), (258, 1), (356, 1)],\n",
      " [(2, 1), (9, 1), (72, 1)],\n",
      " [(16, 1), (79, 1), (218, 2), (245, 1), (264, 1)],\n",
      " [(331, 1), (357, 1)],\n",
      " [(109, 1), (137, 1), (156, 1), (276, 1), (320, 1)],\n",
      " [(10, 1), (11, 1), (61, 1), (222, 1), (223, 1)],\n",
      " [(7, 1), (37, 1), (95, 1), (341, 1), (346, 1)],\n",
      " [(41, 1), (43, 1), (70, 1), (141, 1), (261, 1), (289, 1), (336, 1)],\n",
      " [(2, 1), (106, 1), (183, 1), (293, 1), (328, 1)],\n",
      " [(85, 1)],\n",
      " [(90, 1), (171, 1), (247, 1)],\n",
      " [(9, 1), (23, 1), (26, 1), (110, 1), (166, 1), (206, 1), (207, 1), (273, 1)],\n",
      " [(36, 1)],\n",
      " [(65, 1), (116, 1), (199, 1), (358, 1)],\n",
      " [(7, 1), (16, 1), (70, 1), (76, 1), (156, 1), (327, 1)],\n",
      " [(69, 1), (90, 1), (186, 1)],\n",
      " [(20, 1), (260, 1), (329, 1)],\n",
      " [(17, 1), (30, 1), (43, 1), (70, 1), (111, 1), (289, 1)],\n",
      " [(48, 1), (59, 1), (65, 1), (66, 1), (67, 1), (345, 2)],\n",
      " [(2, 1), (148, 1), (183, 1)],\n",
      " [(7, 1), (71, 1)],\n",
      " [],\n",
      " [(0, 1), (39, 1), (91, 1), (186, 1)],\n",
      " [],\n",
      " [(55, 1), (164, 1), (245, 1), (319, 1)],\n",
      " [(12, 1), (17, 1), (98, 1), (225, 1)],\n",
      " [(2, 1), (80, 1), (252, 1), (314, 1)],\n",
      " [(2, 1)],\n",
      " [(9, 1), (47, 1), (178, 1), (247, 1), (268, 1), (359, 1)],\n",
      " [(18, 1), (41, 1), (112, 1), (144, 1), (347, 1)],\n",
      " [(97, 1), (195, 1)],\n",
      " [(20, 1), (36, 1), (55, 1), (329, 1)],\n",
      " [(57, 1), (147, 1), (176, 1), (268, 1), (358, 1)],\n",
      " [(50, 1), (52, 1), (92, 1), (115, 1), (239, 1)],\n",
      " [(35, 1), (154, 1), (322, 1)],\n",
      " [(48, 1), (181, 1), (280, 1), (292, 1)],\n",
      " [(91, 1), (133, 1), (179, 1), (233, 1), (271, 1)],\n",
      " [(43, 1), (70, 1), (129, 1)],\n",
      " [(17, 1), (49, 1), (269, 1)],\n",
      " [(20, 1), (22, 1)],\n",
      " [(2, 1), (70, 1), (280, 1), (341, 1), (345, 1), (347, 1)],\n",
      " [(9, 1), (23, 1), (115, 1), (321, 1), (343, 1)],\n",
      " [(2, 1), (16, 1), (148, 1), (165, 1), (350, 1)],\n",
      " [(2, 1), (41, 1), (55, 1), (76, 1)],\n",
      " [(10, 1), (63, 1), (65, 1), (67, 1), (72, 1), (115, 1)],\n",
      " [(2, 1), (69, 1), (98, 1), (124, 1), (329, 1)],\n",
      " [(23, 1), (24, 1), (30, 1), (69, 1), (201, 1), (214, 1), (293, 1)],\n",
      " [(222, 1), (223, 1), (226, 1)],\n",
      " [(20, 1), (70, 1), (335, 1), (344, 1)],\n",
      " [(57, 1), (58, 1), (232, 1), (264, 1)],\n",
      " [(20, 1), (41, 1), (98, 1), (107, 1), (171, 1), (198, 1)],\n",
      " [(110, 1), (164, 1), (266, 1)],\n",
      " [(78, 1), (98, 1), (238, 1), (359, 1)],\n",
      " [(2, 1), (12, 1), (329, 1)],\n",
      " [(115, 1), (171, 1), (195, 1), (197, 1), (290, 1)],\n",
      " [(2, 1), (27, 1), (215, 1)],\n",
      " [(2, 1), (55, 1), (133, 1)],\n",
      " [(15, 1), (29, 1), (126, 1), (284, 1), (319, 1), (351, 1)],\n",
      " [(107, 1), (111, 1), (229, 1), (269, 1), (339, 1)],\n",
      " [(17, 1), (70, 1), (80, 1), (105, 1), (252, 1), (271, 1), (300, 1)],\n",
      " [(36, 1), (55, 1), (61, 1), (166, 1), (244, 1)],\n",
      " [(36, 1), (53, 1), (61, 1), (278, 1), (340, 1), (345, 1)],\n",
      " [(50, 1), (356, 1)],\n",
      " [(17, 1), (87, 1), (155, 1), (269, 1), (334, 1)],\n",
      " [(2, 1), (25, 1), (171, 1), (311, 1), (357, 1), (360, 1)],\n",
      " [(74, 1), (75, 1), (270, 1), (271, 1), (303, 1)],\n",
      " [(57, 1), (69, 1), (156, 1), (199, 1), (292, 1), (355, 1)],\n",
      " [(36, 1), (333, 1)],\n",
      " [(2, 1), (29, 1), (274, 1), (288, 1), (360, 1)],\n",
      " [(17, 1), (269, 1), (271, 1), (296, 1)],\n",
      " [(164, 1), (361, 1)],\n",
      " [(2, 1), (3, 1), (12, 1), (13, 1), (38, 1), (184, 1)],\n",
      " [(2, 1), (316, 1)],\n",
      " [(7, 1), (69, 1), (99, 1), (221, 1), (323, 1)],\n",
      " [(10, 1), (115, 1), (234, 1), (311, 1)],\n",
      " [(159, 1), (252, 1)],\n",
      " [(99, 1), (151, 1), (228, 1), (271, 1)],\n",
      " [(144, 1), (275, 1)],\n",
      " [(9, 1), (28, 1), (76, 1), (110, 1), (126, 1)],\n",
      " [(149, 1)],\n",
      " [(2, 1), (136, 1), (274, 1)],\n",
      " [(57, 1), (171, 1), (249, 1), (282, 1), (287, 1), (308, 1), (318, 1)],\n",
      " [(169, 1), (184, 1), (312, 1)],\n",
      " [(17, 1), (30, 1), (142, 1), (173, 1), (217, 1), (362, 1)],\n",
      " [(75, 1), (284, 1), (309, 1), (363, 1)],\n",
      " [(115, 1), (157, 1), (186, 1), (215, 1), (364, 1)],\n",
      " [(8, 1), (40, 1), (79, 1)],\n",
      " [(35, 1), (195, 1)],\n",
      " [(2, 1), (11, 1), (22, 1)],\n",
      " [(17, 1), (87, 1)],\n",
      " [(7, 1), (76, 1), (143, 1), (177, 1), (295, 1), (302, 1)],\n",
      " [(7, 1), (95, 1), (113, 1)],\n",
      " [(42, 1)],\n",
      " [(44, 2), (48, 1), (88, 1), (187, 1), (264, 1)],\n",
      " [(29, 1), (40, 1)],\n",
      " [(12, 1), (143, 1), (354, 1)],\n",
      " [(156, 1), (176, 1), (237, 1), (246, 1), (277, 1)],\n",
      " [(9, 1), (190, 1), (361, 1)],\n",
      " [(2, 1), (12, 1), (153, 1), (311, 1)],\n",
      " [(20, 1), (133, 1)],\n",
      " [(2, 1), (9, 1), (55, 1), (133, 1), (207, 1), (250, 1)],\n",
      " [(72, 1), (267, 1)],\n",
      " [(2, 1), (18, 1), (79, 1), (119, 1), (152, 1), (301, 1), (302, 1), (363, 1)],\n",
      " [(115, 1), (155, 1), (232, 1), (348, 1)],\n",
      " [(7, 1), (182, 1), (202, 1), (221, 1)],\n",
      " [(29, 1), (43, 1), (70, 1), (91, 1)],\n",
      " [(70, 1), (217, 1), (362, 1)],\n",
      " [(2, 1), (36, 1), (215, 1)],\n",
      " [(7, 1), (20, 1), (133, 1), (154, 1), (184, 1)],\n",
      " [(7, 1), (95, 1)],\n",
      " [(20, 1), (36, 1), (84, 1), (166, 1), (219, 1)],\n",
      " [(2, 1), (42, 1), (155, 1), (178, 1), (199, 1), (209, 1), (349, 1)],\n",
      " [(54, 1), (57, 1), (281, 1), (331, 1)],\n",
      " [(2, 1), (9, 1), (77, 1), (148, 1), (192, 1), (253, 1), (332, 1)],\n",
      " [(265, 1), (266, 1), (346, 1), (352, 1)],\n",
      " [(9, 1), (12, 1), (56, 1), (62, 1), (67, 1), (135, 1)],\n",
      " [(16, 1), (186, 1), (364, 1)],\n",
      " [(37, 1), (95, 1)]]\n"
     ]
    }
   ],
   "source": [
    "#REPRESENT EVERY DOCUMENT AS A SPARSE ARRAY OF WORD IDS AND COUNTS (FOR COMPUTATIONAL EFFICIENCY)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "pprint(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[(0, 0.5683469488014847), (1, 0.5683469488014847), (2, 0.17591258443933855), (3, 0.5683469488014847)]\n",
      "[(4, 0.7546821231853805), (5, 0.6560906133640428)]\n",
      "[(6, 0.49702915811192194), (7, 0.2953355908175191), (8, 0.43672849921975226), (9, 0.26330564615147284), (10, 0.49702915811192194), (11, 0.39829546249435677)]\n",
      "[(11, 0.3966236981061267), (12, 0.34619164933573066), (13, 0.47099936297314193), (14, 0.5693186439367763), (15, 0.42056731420274585)]\n",
      "[(16, 0.801178923907056), (17, 0.598424875725543)]\n",
      "[(18, 0.44038186537774215), (19, 0.48282350182877126), (20, 0.35944902186593214), (21, 0.5553779585652389), (22, 0.3678274086412745)]\n",
      "[(23, 0.6129705569714845), (24, 0.7901057500651846)]\n",
      "[(23, 0.4223625752357862), (25, 0.5124552955270989), (26, 0.5444161966847262), (27, 0.5124552955270989)]\n",
      "[(28, 0.6006350597610682), (29, 0.5277073525843999), (30, 0.6006350597610682)]\n",
      "[(31, 0.46954919580744725), (32, 0.46954919580744725), (33, 0.46954919580744725), (34, 0.38845973948890844), (35, 0.37232473407929145), (36, 0.22145875764740738)]\n",
      "[(37, 0.6816484183759429), (38, 0.731679871067652)]\n",
      "[(39, 0.38774367154869976), (40, 0.2701269852388166), (41, 0.2864341722848229), (42, 0.30745779289865854), (43, 0.2631617786697544), (44, 0.358112542530597), (45, 0.358112542530597), (46, 0.3370889219167613), (47, 0.32078173487075506), (48, 0.24049585622071382)]\n",
      "[(49, 0.4510742003963782), (50, 0.3730805445569429), (51, 0.48839720990124824), (52, 0.42459310372545533), (53, 0.48839720990124824)]\n",
      "[(2, 0.17577595816753022), (34, 0.4698302887159343), (54, 0.5679055299856799), (55, 0.32144817678555376), (56, 0.5679055299856799)]\n",
      "[(57, 0.5738840150354976), (58, 0.8189365892953718)]\n",
      "[(9, 0.2567158230217166), (59, 0.5148128576692008), (60, 0.5574097633737001), (61, 0.30549432901921514), (62, 0.5148128576692008)]\n",
      "[(2, 0.14308397354411817), (12, 0.2811051470084175), (48, 0.2867282575263866), (63, 0.4269553202775649), (64, 0.46228267321167504), (65, 0.30617030280872654), (66, 0.4269553202775649), (67, 0.3824481191949159)]\n",
      "[(2, 1.0)]\n",
      "[(7, 0.39599483000764035), (9, 0.35304811824128884), (68, 0.7079957460633681), (69, 0.4661406916046696)]\n",
      "[(17, 0.4553156886617637), (70, 0.5244513298672124), (71, 0.7194709349646975)]\n",
      "[(4, 0.8274261948633913), (72, 0.5615744759636864)]\n",
      "[]\n",
      "[(73, 0.675054981665273), (74, 0.5868659444400648), (75, 0.4470896274634076)]\n",
      "[(76, 0.36394533033952203), (77, 0.42520547082327215), (78, 0.39613039545361856), (79, 0.7278906606790441)]\n",
      "[(80, 0.4629944954430872), (81, 0.6267519833193195), (82, 0.6267519833193195)]\n",
      "[(36, 0.27345662841559615), (72, 0.39350911035912917), (83, 0.5354901792045733), (84, 0.38400081350802306), (85, 0.5797979782998313)]\n",
      "[(86, 1.0)]\n",
      "[(38, 0.4370470893526411), (47, 0.45598689244791557), (69, 0.3351572566373513), (87, 0.5090521300439419), (88, 0.4791673380199531)]\n",
      "[(36, 0.35013706885247603), (61, 0.40686929360199015), (84, 0.49167913777655997), (89, 0.6856478953621259)]\n",
      "[(90, 0.42386062498158483), (91, 0.4041975211609111), (92, 0.5166660117296973), (93, 0.6245180276028253)]\n",
      "[(7, 0.31095950627855207), (12, 0.36604235291388565), (92, 0.4980065676755654), (94, 0.6019634973409271), (95, 0.3986810601838864)]\n",
      "[(2, 0.23058431909762458), (96, 0.6880519136614203), (97, 0.6880519136614203)]\n",
      "[(7, 0.2693194726928022), (48, 0.32336793525266905), (49, 0.4815139657122656), (98, 0.38513606152342844), (99, 0.4134041776409566), (100, 0.5213556376660465)]\n",
      "[(75, 0.5521778433646571), (101, 0.8337263515669611)]\n",
      "[(35, 0.47358282572501303), (57, 0.386548350108251), (102, 0.5972486236169353), (103, 0.5192241922277672)]\n",
      "[(19, 0.6560906133640428), (104, 0.7546821231853805)]\n",
      "[(105, 0.6784798696991756), (106, 0.7346189940458862)]\n",
      "[(107, 1.0)]\n",
      "[(2, 0.2278998036102879), (22, 0.4876587513827783), (70, 0.409998189799501), (108, 0.7363097894740461)]\n",
      "[(2, 0.5139718268134246), (7, 0.8578070652787088)]\n",
      "[(12, 0.4044893406675764), (15, 0.49138965643630395), (61, 0.36456458793385765), (109, 0.5081305745407894), (110, 0.4514649037025852)]\n",
      "[(43, 0.32199299018861927), (45, 0.4381705009608554), (78, 0.3504680508408206), (111, 0.3624079784736387), (112, 0.47442582604427813), (113, 0.47442582604427813)]\n",
      "[(114, 0.5227811032951496), (115, 0.4001553508193298), (116, 0.4089814721008095), (117, 0.6319096206946212)]\n",
      "[(118, 0.5920372396072239), (119, 0.5467941238130831), (120, 0.5920372396072239)]\n",
      "[(11, 0.5319005943128805), (15, 0.5640106868076146), (121, 0.631643651861085)]\n",
      "[(17, 0.2611766539055677), (111, 0.4127004978209312), (122, 0.5402634219373903), (123, 0.46968352500644683), (124, 0.4989768289280241)]\n",
      "[(34, 0.6221698598702367), (35, 0.2981637529967891), (68, 0.3472872769607751), (125, 0.376022696382984), (126, 0.3472872769607751), (127, 0.376022696382984)]\n",
      "[(21, 0.5339076803806143), (74, 0.46415809615599535), (75, 0.3536076207190823), (90, 0.3623633475517811), (128, 0.49310678916832024)]\n",
      "[(36, 0.6522849813544956), (61, 0.7579738142570397)]\n",
      "[(40, 0.4729510909766936), (129, 0.5616398203918471), (130, 0.6788799435056017)]\n",
      "[(65, 0.5521778433646571), (131, 0.8337263515669611)]\n",
      "[(2, 0.21661046771857395), (76, 0.47497872625698867), (132, 0.6998356530243979), (133, 0.4875501755039618)]\n",
      "[(7, 0.29712600527912664), (134, 0.5751842464289967), (135, 0.5751842464289967), (136, 0.5000423005173492)]\n",
      "[(6, 0.4364351195204949), (9, 0.23120541174097925), (137, 0.5020187393683014), (138, 0.5020187393683014), (139, 0.5020187393683014)]\n",
      "[(17, 0.24119696569008925), (48, 0.3094610045058131), (98, 0.3685726983948668), (140, 0.4608057239703251), (141, 0.4989339441179854), (142, 0.4989339441179854)]\n",
      "[(55, 0.5225341983868529), (143, 0.8526183269882306)]\n",
      "[(144, 0.4398664496794635), (145, 0.531686856355847), (146, 0.4910556791273855), (147, 0.531686856355847)]\n",
      "[(128, 0.5105938259410978), (148, 0.3578074770545232), (149, 0.5528416383896516), (150, 0.5528416383896516)]\n",
      "[(7, 0.36777853623392803), (140, 0.6575480774383076), (151, 0.6575480774383076)]\n",
      "[(2, 0.18952925889251593), (18, 0.48554968308807717), (90, 0.4155956400692403), (152, 0.5655456961954779), (153, 0.48554968308807717)]\n",
      "[(76, 0.4558814572282164), (150, 0.6716976565144188), (154, 0.5839472195228914)]\n",
      "[(17, 0.27823932335022716), (46, 0.5003679472583876), (91, 0.3725103863707037), (111, 0.4396622192024717), (155, 0.4124002554254434), (156, 0.4124002554254434)]\n",
      "[(7, 0.6150173860108614), (95, 0.7885135476986854)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.2542488153338916), (48, 0.3424078802493122), (120, 0.552053123664759), (157, 0.45671553591754477), (158, 0.552053123664759)]\n",
      "[(17, 0.5895676458524386), (22, 0.8077190049540827)]\n",
      "[(22, 0.32253319103507), (46, 0.42336868078102813), (75, 0.32253319103507), (101, 0.4869887914777027), (109, 0.3720046712682183), (159, 0.4869887914777027)]\n",
      "[(50, 0.6373423549975985), (160, 0.7705807696316559)]\n",
      "[(2, 0.17062178186397525), (12, 0.3352063818588475), (52, 0.4792375713243133), (110, 0.37413573533639327), (136, 0.4792375713243133), (161, 0.5091267436712654)]\n",
      "[(2, 0.15987342184866107), (9, 0.2378871375576807), (13, 0.4273245142749899), (162, 0.5165268408268564), (163, 0.5165268408268564), (164, 0.44904788573326093)]\n",
      "[(165, 0.5439507706226666), (166, 0.5213573130918826), (167, 0.6574983735528325)]\n",
      "[(36, 0.3079585353728943), (84, 0.4324500334663899), (168, 0.6529508435905138), (169, 0.5401885827193469)]\n",
      "[(16, 0.4375179293793943), (48, 0.4192857167723299), (170, 0.6760007669322532), (171, 0.4192857167723299)]\n",
      "[(17, 0.22667058334863685), (172, 0.43305313548990637), (173, 0.40762999682563056), (174, 0.4688850369327301), (175, 0.43305313548990637), (176, 0.43305313548990637)]\n",
      "[(65, 0.37973580096993415), (70, 0.3192621696181352), (177, 0.5295426076162041), (178, 0.47434137648109903), (179, 0.4984548864134956)]\n",
      "[(2, 0.2747394162946437), (55, 0.5024264147368145), (180, 0.8198084842869029)]\n",
      "[(181, 0.7346189940458862), (182, 0.6784798696991756)]\n",
      "[(2, 0.23569197451968682), (100, 0.761485114906342), (183, 0.6038126472036559)]\n",
      "[(169, 0.5375204455151174), (184, 0.5375204455151174), (185, 0.6497257431458752)]\n",
      "[(16, 0.6589858560865646), (186, 0.7521553306849973)]\n",
      "[(37, 0.4292121138557621), (38, 0.46071531255788184), (116, 0.3760456128581488), (169, 0.4806807980064841), (184, 0.4806807980064841)]\n",
      "[(40, 0.20214658187279808), (48, 0.17997244979656066), (58, 0.2679896136046417), (187, 0.23008268444439534), (188, 0.6561413496822509), (189, 0.5359792272092834), (190, 0.2901637456808791)]\n",
      "[(136, 0.48081683781407264), (155, 0.3962863485038182), (191, 0.5530697507837443), (192, 0.5530697507837443)]\n",
      "[(16, 0.4103322076401895), (28, 0.5027218104172572), (107, 0.5245076827705225), (193, 0.5511714355975106)]\n",
      "[(114, 0.570443156271959), (116, 0.44626839097935406), (194, 0.6895209414333867)]\n",
      "[(69, 0.2894047805955203), (102, 0.4759315759164285), (186, 0.351580379035823), (195, 0.34101519038117795), (196, 0.3937399055217574), (197, 0.351580379035823), (198, 0.41375597747612575)]\n",
      "[(2, 0.29273109460364855), (22, 0.6263843927195251), (199, 0.7224618320780286)]\n",
      "[(9, 0.30134086064931814), (61, 0.35859855830681975), (84, 0.43334661211541514), (171, 0.40582877521743244), (200, 0.6543045763686888)]\n",
      "[(2, 0.1713454869875344), (84, 0.36664413446490185), (109, 0.4228815343820096), (110, 0.37572266020384876), (165, 0.45798811430627395), (201, 0.5535913478313506)]\n",
      "[(9, 0.22366922570583483), (19, 0.4222094306483777), (116, 0.3143234873495298), (202, 0.44854186198151685), (203, 0.4856553394611805), (204, 0.4856553394611805)]\n",
      "[(80, 0.4207106125924743), (118, 0.5695126258325715), (144, 0.4711598448936286), (205, 0.5259908269439365)]\n",
      "[(65, 0.46331058566252625), (80, 0.5167695149827699), (195, 0.5012402996387985), (197, 0.5167695149827699)]\n",
      "[(171, 1.0)]\n",
      "[(5, 0.49415423429517785), (50, 0.4342023674156801), (52, 0.49415423429517785), (206, 0.5684113735552023)]\n",
      "[(9, 0.28593726443698914), (36, 0.29282250375265695), (61, 0.3402681288304477), (72, 0.4213769606261588), (83, 0.573412961017474), (207, 0.45864092250384264)]\n",
      "[(15, 0.3690145462493299), (198, 0.4342733081550707), (208, 0.46135814150246274), (209, 0.46135814150246274), (210, 0.4995320700608114)]\n",
      "[(2, 0.15293750150082566), (210, 0.4941179314279853), (211, 0.4941179314279853), (212, 0.4941179314279853), (213, 0.4941179314279853)]\n",
      "[(2, 0.19073504243498096), (17, 0.29790370363122315), (36, 0.2906423337944642), (55, 0.3488044228517565), (61, 0.33773470895082935), (72, 0.4182396560171943), (214, 0.6162360682889343)]\n",
      "[(13, 0.49393347303838386), (71, 0.45607154080296486), (177, 0.5514147242267745), (196, 0.49393347303838386)]\n",
      "[(36, 0.4810015482084989), (61, 0.5589375634589329), (84, 0.6754452685270795)]\n",
      "[(17, 0.2699165026780484), (64, 0.5583424521050061), (79, 0.37894723649752954), (139, 0.5583424521050061), (156, 0.40006435218308856)]\n",
      "[(33, 0.47968384598333913), (36, 0.22623867646567852), (61, 0.26289581614628543), (84, 0.31769511791744215), (154, 0.4170180517906941), (168, 0.47968384598333913), (215, 0.38036091211008716)]\n",
      "[(7, 0.22704280925831843), (9, 0.2024194017061249), (30, 0.3485096896717495), (99, 0.3485096896717495), (216, 0.4395153733773248), (217, 0.3636126501077867), (218, 0.3820971797833642), (219, 0.4395153733773248)]\n",
      "[(36, 0.30305145394187205), (61, 0.35215446166408837), (72, 0.4360966078045349), (84, 0.4255592761555125), (220, 0.6425465761586219)]\n",
      "[(7, 0.3632241177115687), (86, 0.7031386910563453), (221, 0.6112808040927031)]\n",
      "[(148, 0.28821243155766946), (195, 0.31907534592307607), (222, 0.3531057827770678), (223, 0.3531057827770678), (224, 0.44531163568258114), (225, 0.4112811988285893), (226, 0.44531163568258114)]\n",
      "[(48, 0.527089709065139), (108, 0.8498096484493614)]\n",
      "[(2, 0.31775742506800553), (89, 0.948172040725812)]\n",
      "[(2, 0.20300671271296528), (36, 0.3093419227300702), (37, 0.48451489551456467), (72, 0.4451487079160879), (227, 0.6558839785360568)]\n",
      "[(69, 0.35823184972453004), (228, 0.5121566023921715), (229, 0.5121566023921715), (230, 0.5891189787259922)]\n",
      "[(28, 0.4579864900890016), (107, 0.4778337197214649), (231, 0.5775796460145164), (232, 0.4778337197214649)]\n",
      "[(20, 0.3847472274431068), (22, 0.3937152893549884), (90, 0.4034641276759472), (123, 0.5168048662607808), (179, 0.5168048662607808)]\n",
      "[(28, 0.4683602863704852), (38, 0.4683602863704852), (173, 0.5134983326094816), (233, 0.5455242860855934)]\n",
      "[(13, 0.30755821634533004), (42, 0.29478352445310696), (90, 0.25231351557639153), (93, 0.3717597950792169), (103, 0.32319317562223554), (117, 0.3717597950792169), (163, 0.3717597950792169), (180, 0.3433501439100883), (205, 0.3433501439100883)]\n",
      "[(29, 0.338442839778881), (75, 0.3217492363830609), (77, 0.38521471095815696), (104, 0.48580510825015627), (121, 0.40190831435397717), (194, 0.48580510825015627)]\n",
      "[(170, 0.477570909788267), (234, 0.41518114912447923), (235, 0.477570909788267), (236, 0.477570909788267), (237, 0.37868547870719543)]\n",
      "[(9, 0.24321102381043888), (69, 0.32111927238030274), (79, 0.35841261266106494), (157, 0.43688798681231256), (169, 0.43688798681231256), (207, 0.3901083985788246), (238, 0.41874147299369174)]\n",
      "[(28, 0.4515304435007455), (69, 0.3462640718100046), (92, 0.47109789492593007), (115, 0.36059517497116533), (239, 0.5694377440506101)]\n",
      "[(40, 0.3668533625937986), (97, 0.486344562348027), (199, 0.40225243209136297), (217, 0.43564643492381655), (240, 0.5265859299704857)]\n",
      "[(17, 0.25023950950552076), (65, 0.3428329370512741), (111, 0.39541807662768474), (116, 0.33502387530529776), (197, 0.3823905951701095), (218, 0.4500148583836792), (229, 0.4500148583836792)]\n",
      "[(2, 0.148952153903054), (91, 0.31146702713769125), (98, 0.3555032063745071), (183, 0.3815963379244251), (211, 0.48124187623087733), (212, 0.48124187623087733), (238, 0.3815963379244251)]\n",
      "[(72, 0.3728427781244618), (89, 0.5073672776998445), (127, 0.5493481174628171), (241, 0.5493481174628171)]\n",
      "[(36, 0.20165935636872345), (43, 0.29019151731677345), (55, 0.24201455614718512), (116, 0.2767293894460224), (132, 0.427569402158118), (148, 0.2767293894460224), (166, 0.33903724121006795), (216, 0.427569402158118), (242, 0.427569402158118)]\n",
      "[(243, 1.0)]\n",
      "[(42, 0.5707343080454158), (123, 0.6257386120746377), (186, 0.5317081332637545)]\n",
      "[(2, 0.17101406243094486), (61, 0.30281475215181203), (110, 0.3749959196386434), (148, 0.35759967364267714), (220, 0.552520565225911), (244, 0.552520565225911)]\n",
      "[(40, 0.46049206595080855), (57, 0.427806683299539), (193, 0.574643714049323), (238, 0.5241308050708372)]\n",
      "[(12, 0.4468864256076107), (62, 0.6787514885569856), (245, 0.5827426013166789)]\n",
      "[(12, 0.3852998534236776), (71, 0.48402421573713683), (140, 0.5852109933671157), (196, 0.5242067099666083)]\n",
      "[(15, 0.4492522006138507), (20, 0.3936033342350631), (27, 0.5287006741048672), (36, 0.2868282086168107), (61, 0.33330258634859417), (72, 0.4127510598396106)]\n",
      "[(156, 0.28736154244188916), (179, 0.34865765290159195), (182, 0.3704027941451345), (241, 0.4010508493749859), (246, 0.34865765290159195), (247, 0.34865765290159195), (248, 0.4010508493749859), (249, 0.30635774784646624)]\n",
      "[(2, 0.17786845603338264), (148, 0.3719325821790144), (195, 0.411760577701125), (250, 0.574666074043521), (251, 0.574666074043521)]\n",
      "[(9, 0.23886161073725354), (77, 0.4112529981359765), (119, 0.47900837183967165), (252, 0.3961847165194558), (253, 0.4290749926001746), (254, 0.4508873509749865)]\n",
      "[(95, 0.5085374369144453), (222, 0.6088471381457281), (223, 0.6088471381457281)]\n",
      "[(81, 0.3779644730092272), (82, 0.7559289460184544), (255, 0.3779644730092272), (256, 0.3779644730092272)]\n",
      "[(57, 0.416141316847166), (257, 0.6429721628549353), (258, 0.6429721628549353)]\n",
      "[(2, 0.23884495398202962), (249, 0.5894705626338981), (259, 0.7716719145760054)]\n",
      "[(14, 0.6151996006658691), (171, 0.38157413148185154), (222, 0.48781688855779987), (223, 0.48781688855779987)]\n",
      "[(55, 0.5225341983868529), (96, 0.8526183269882306)]\n",
      "[(46, 0.5101103604735509), (76, 0.3982377973644532), (91, 0.3797633491770711), (156, 0.42042989385533813), (246, 0.5101103604735509)]\n",
      "[(2, 0.2627699047840009), (22, 0.5622736028639247), (260, 0.7840920689958728)]\n",
      "[(60, 0.44306254536283624), (133, 0.3086656428796606), (261, 0.40920398258527113), (262, 0.44306254536283624), (263, 0.44306254536283624), (264, 0.3851809499856567)]\n",
      "[(146, 0.6006178254589211), (265, 0.5653575097851788), (266, 0.5653575097851788)]\n",
      "[(15, 0.4765206435384997), (22, 0.42722539166318146), (148, 0.41749403535989915), (267, 0.6450622229202408)]\n",
      "[(41, 0.2730461792919217), (123, 0.32133331535415244), (178, 0.30578832963407165), (208, 0.34137428755275795), (252, 0.28234845838224526), (268, 0.34137428755275795), (269, 0.28234845838224526), (270, 0.36962045141638317), (271, 0.25086154221149165), (272, 0.36962045141638317)]\n",
      "[(7, 0.342099309539499), (35, 0.5251208994198189), (99, 0.5251208994198189), (221, 0.5757292283683701)]\n",
      "[(197, 1.0)]\n",
      "[(2, 0.2514906397899317), (16, 0.5258805587954639), (273, 0.8125282124203576)]\n",
      "[(75, 0.5069788213051206), (109, 0.5847413382613801), (274, 0.6332851191022578)]\n",
      "[(215, 0.513497419998626), (254, 0.5629855404947535), (275, 0.6475860412706446)]\n",
      "[(2, 0.17884605613406987), (22, 0.38269381123827206), (79, 0.3921697452267781), (276, 0.5778245520804645), (277, 0.5778245520804645)]\n",
      "[(29, 0.49168893789794244), (91, 0.456789224593637), (116, 0.456789224593637), (232, 0.5838914256427138)]\n",
      "[(34, 0.39580449845567267), (51, 0.47842714457707036), (125, 0.47842714457707036), (217, 0.39580449845567267), (278, 0.47842714457707036)]\n",
      "[(7, 0.24667969209340931), (69, 0.29037611999891555), (70, 0.26590167982007784), (95, 0.31626793580492113), (243, 0.4775289617372721), (255, 0.4775289617372721), (256, 0.4775289617372721)]\n",
      "[(32, 0.42277668503361254), (78, 0.3123137751165449), (178, 0.3497646730024208), (198, 0.3675452300750788), (228, 0.3675452300750788), (279, 0.42277668503361254), (280, 0.3904683550226007)]\n",
      "[(38, 0.40865505882084685), (67, 0.4263644694004231), (115, 0.3263546114318908), (146, 0.47598241291142324), (195, 0.3692710814050846), (281, 0.4263644694004231)]\n",
      "[(7, 0.2852787232169622), (8, 0.42185687239724456), (9, 0.25433947307868315), (79, 0.37481226640458226), (174, 0.552249969778401), (254, 0.48010415282247687)]\n",
      "[(9, 0.25375686931594715), (69, 0.3350432885795164), (83, 0.5088790301588912), (91, 0.3566058041761561), (253, 0.45583183705376457), (282, 0.479004400265181)]\n",
      "[(9, 0.2941936956193214), (71, 0.48796056235659335), (95, 0.4230685511007054), (218, 0.5553350146217076), (271, 0.4335442095647696)]\n",
      "[(48, 0.3018567844331591), (68, 0.4494825908944639), (78, 0.3595159582222489), (95, 0.32232464247533665), (131, 0.48667390664137616), (200, 0.48667390664137616)]\n",
      "[(75, 0.5521778433646571), (283, 0.8337263515669611)]\n",
      "[(18, 0.3587060658862197), (80, 0.3341781455821827), (91, 0.2927835014495468), (253, 0.3742508948588337), (284, 0.41780414822011963), (285, 0.39327622791608263), (286, 0.45237431024998254)]\n",
      "[(43, 0.39701462134112686), (103, 0.5085435710670431), (105, 0.5402605051119974), (287, 0.5402605051119974)]\n",
      "[(42, 0.44079072919947837), (69, 0.33802813277740923), (103, 0.48327176974737057), (205, 0.513412547684459), (238, 0.44079072919947837)]\n",
      "[(9, 0.2820087798403527), (23, 0.4387466047920168), (36, 0.28879942303316464), (55, 0.3465927167452505), (207, 0.4523396668665575), (260, 0.5655348553452235)]\n",
      "[(24, 0.5768837361992756), (55, 0.35354797230562246), (61, 0.34232771634763115), (110, 0.42392748668076546), (215, 0.4952839658661412)]\n",
      "[(2, 0.21739207198396565), (11, 0.48930942241746495), (17, 0.3395385690920708), (111, 0.5365247406239055), (238, 0.5569306410760364)]\n",
      "[(2, 0.1525140956397146), (29, 0.3432810744519471), (172, 0.4550943263441002), (272, 0.49274997114228947), (274, 0.40765382441757136), (288, 0.49274997114228947)]\n",
      "[(16, 0.5821299321015534), (17, 0.4348105296818865), (109, 0.6870695346391064)]\n",
      "[(1, 0.7321519784884002), (17, 0.17697015578643197), (70, 0.20384150131481324), (180, 0.33810069096428447), (187, 0.2902766026758439), (269, 0.2796408878831695), (289, 0.33810069096428447)]\n",
      "[(2, 0.2956770890538167), (251, 0.9552879455999963)]\n",
      "[(48, 0.3356455108174794), (57, 0.3502407142051348), (88, 0.470454606435418), (91, 0.3502407142051348), (172, 0.49979600129790447), (249, 0.4133780299732299)]\n",
      "[(2, 0.16872966997052105), (193, 0.4739230616609706), (195, 0.3906045396230487), (262, 0.5451400387622418), (290, 0.5451400387622418)]\n",
      "[(90, 0.30936541201208956), (124, 0.4209867968923562), (155, 0.3266050289438587), (196, 0.37710177395776096), (197, 0.33672376806070164), (247, 0.39627203365408575), (291, 0.45582029924746975)]\n",
      "[(11, 0.48907084409260315), (17, 0.3393730162959694), (70, 0.39090379301576444), (213, 0.7020184398394481)]\n",
      "[(17, 0.40904222423902026), (70, 0.4711516510764409), (160, 0.7814733395820608)]\n",
      "[(8, 0.3222961189202866), (57, 0.2730702037635971), (61, 0.2312353510572388), (161, 0.3896731315900478), (292, 0.3896731315900478), (293, 0.3667966916193521), (294, 0.421915662821822), (295, 0.3896731315900478)]\n",
      "[(114, 0.5431818512984394), (116, 0.42494136027915586), (271, 0.44561359513321613), (296, 0.5707949198937834)]\n",
      "[(2, 0.21380035889538399), (297, 0.6907566165214074), (298, 0.6907566165214074)]\n",
      "[(2, 0.22746326099078268), (299, 0.6388922889671629), (300, 0.7348993863092678)]\n",
      "[(40, 0.4653609699367136), (240, 0.6679849883165118), (301, 0.5807195734977004)]\n",
      "[(79, 0.4365599558304783), (116, 0.4163077237770377), (129, 0.5321458941123054), (302, 0.5940740959411438)]\n",
      "[(187, 0.513497419998626), (299, 0.5629855404947535), (303, 0.6475860412706446)]\n",
      "[(65, 0.45224912723744387), (304, 0.630662638386392), (305, 0.630662638386392)]\n",
      "[(271, 0.41091902490936505), (285, 0.5263539857572204), (296, 0.5263539857572204), (299, 0.5263539857572204)]\n",
      "[(59, 0.5271648838712364), (110, 0.3873912025294005), (134, 0.5707838271716297), (285, 0.4962167510827885)]\n",
      "[]\n",
      "[(171, 0.2168299900010093), (281, 0.2892153796641652), (306, 0.3495879627451889), (307, 0.7905161176150753), (308, 0.3495879627451889)]\n",
      "[(75, 0.42411278222328475), (236, 0.6403625332398923), (309, 0.6403625332398923)]\n",
      "[(9, 0.3019633550278153), (29, 0.456771952690915), (185, 0.6556562049521368), (310, 0.5198965823729135)]\n",
      "[(17, 0.32006161368102276), (130, 0.6620713607144738), (148, 0.4285026068176404), (311, 0.5249834213764006)]\n",
      "[(2, 0.23576530459732056), (8, 0.5818699722707616), (71, 0.5818699722707616), (79, 0.5169810362937687)]\n",
      "[(2, 0.14498255836402638), (66, 0.4326206010898012), (173, 0.4072228551109253), (312, 0.4684167135526476), (313, 0.4684167135526476), (314, 0.4326206010898012)]\n",
      "[(2, 0.20958375065444737), (17, 0.32734297140058144), (92, 0.5601948930143099), (252, 0.517253763822429), (269, 0.517253763822429)]\n",
      "[(2, 0.17378760391765483), (55, 0.3178119977819549), (145, 0.5614814581965304), (265, 0.48812964158448613), (315, 0.5614814581965304)]\n",
      "[(17, 0.3490516520900653), (73, 0.7220394211010221), (121, 0.5973458115273738)]\n",
      "[(9, 0.41831890093293184), (316, 0.9083002241121951)]\n",
      "[(47, 0.48373289747854553), (57, 0.3784333275770128), (70, 0.3255832656015857), (74, 0.5083238326284769), (296, 0.5083238326284769)]\n",
      "[(57, 0.2418582930733176), (187, 0.29631463651730433), (298, 0.3736907235742383), (317, 0.8450191982267865)]\n",
      "[(2, 0.2956770890538167), (318, 0.9552879455999963)]\n",
      "[(16, 0.4538392457703651), (37, 0.5180044530961798), (114, 0.5801206113415044), (171, 0.43492689255537303)]\n",
      "[(2, 0.3755300998126777), (199, 0.9268101985491314)]\n",
      "[(9, 0.3194732789834306), (55, 0.3926371077987855), (207, 0.5124324025299504), (297, 0.6936756867819162)]\n",
      "[(40, 0.4929960971636513), (193, 0.6152051886084056), (301, 0.6152051886084056)]\n",
      "[(36, 0.45479672597261706), (319, 0.8905952717394071)]\n",
      "[(2, 0.37315279408544133), (12, 0.733102166789117), (36, 0.5686107678503195)]\n",
      "[(9, 0.22843219040429963), (156, 0.35539265132747805), (246, 0.43120024557694514), (249, 0.37888609358327063), (286, 0.4959972147467719), (320, 0.4959972147467719)]\n",
      "[(96, 0.6784798696991756), (321, 0.7346189940458862)]\n",
      "[(6, 0.47116393161558934), (7, 0.2799664282155874), (9, 0.2496033108572174), (40, 0.3775683035706981), (129, 0.4483706629477913), (322, 0.5419662909926826)]\n",
      "[(94, 0.7346189940458862), (323, 0.6784798696991756)]\n",
      "[(2, 0.4391281442180561), (115, 0.8984244392020989)]\n",
      "[(324, 1.0)]\n",
      "[(23, 0.43704423276348914), (207, 0.4505845526665691), (245, 0.48365644005400404), (325, 0.609952742022469)]\n",
      "[(16, 0.5276893742691926), (114, 0.6745196349862461), (115, 0.5163014490599548)]\n",
      "[(9, 0.3319541219252101), (15, 0.5324515676839374), (50, 0.5505914043134801), (71, 0.5505914043134801)]\n",
      "[(8, 0.342838162236664), (65, 0.297245424628186), (110, 0.30460555938722034), (237, 0.35587748900034233), (295, 0.4145095533724987), (326, 0.44880711237007914), (327, 0.44880711237007914)]\n",
      "[(2, 0.34410861294028955), (227, 0.555881436351604), (305, 0.5134013244428429), (328, 0.555881436351604)]\n",
      "[(329, 1.0)]\n",
      "[(69, 0.3167880503802036), (282, 0.45290526703844713), (287, 0.481152141643764), (330, 0.481152141643764), (331, 0.481152141643764)]\n",
      "[(2, 0.3504060783723106), (34, 0.9365978754191887)]\n",
      "[(171, 0.5807872478743746), (301, 0.8140553867560301)]\n",
      "[]\n",
      "[(36, 0.4265765867752408), (325, 0.9044514445868198)]\n",
      "[(36, 0.24575717287530377), (271, 0.3536490851322876), (326, 0.521068049479603), (332, 0.521068049479603), (333, 0.521068049479603)]\n",
      "[(5, 0.5573689083041546), (19, 0.5573689083041546), (90, 0.4351320489872413), (271, 0.4351320489872413)]\n",
      "[(48, 0.293687329048792), (61, 0.2595080960768539), (157, 0.3917303707527064), (158, 0.4735025585979376), (171, 0.293687329048792), (189, 0.43731778903567), (305, 0.43731778903567)]\n",
      "[(2, 1.0)]\n",
      "[(151, 0.7809282285834495), (186, 0.6246207663867059)]\n",
      "[(26, 0.7873982990039072), (36, 0.4020975405853811), (61, 0.4672488486673258)]\n",
      "[(7, 0.3083989393015137), (17, 0.2886077459093704), (30, 0.4733909828819182), (148, 0.3863917639094908), (222, 0.4733909828819182), (223, 0.4733909828819182)]\n",
      "[(2, 0.1734056526764057), (11, 0.3903041126599772), (23, 0.4014293108232563), (78, 0.41386622425110436), (197, 0.41386622425110436), (334, 0.5602474315164793)]\n",
      "[(7, 0.5392707924858804), (9, 0.4807854145619912), (95, 0.6913988700570061)]\n",
      "[(78, 0.3516916318186241), (237, 0.3775050024141664), (254, 0.41388690487321506), (294, 0.4760821779278059), (310, 0.3775050024141664), (335, 0.43970027546875723)]\n",
      "[(75, 0.5521778433646571), (283, 0.8337263515669611)]\n",
      "[(2, 0.21797999851604083), (95, 0.4664324067805109), (237, 0.5584368335393862), (314, 0.6504412602982615)]\n",
      "[(16, 0.40442687612756417), (155, 0.4477344877303495), (248, 0.6248723996706789), (310, 0.4954868459327137)]\n",
      "[(91, 0.6323275282950476), (187, 0.7747011662313901)]\n",
      "[(2, 0.1991567380765401), (20, 0.4164475337732458), (128, 0.5942736058045384), (148, 0.4164475337732458), (310, 0.5102140515030406)]\n",
      "[(6, 0.6197247338720315), (9, 0.3283047258396062), (122, 0.7128515001166937)]\n",
      "[(2, 0.13785048663531707), (25, 0.38719049642589914), (116, 0.28825283916156863), (138, 0.44537406871535873), (155, 0.31912008053123425), (183, 0.3531552884785667), (230, 0.44537406871535873), (310, 0.3531552884785667)]\n",
      "[(17, 0.25894162947385957), (37, 0.3956882908078216), (175, 0.49470682474949745), (336, 0.49470682474949745), (337, 0.535640106914659)]\n",
      "[(31, 0.554060382649905), (153, 0.4393371056237351), (237, 0.4393371056237351), (338, 0.554060382649905)]\n",
      "[(75, 0.5827523798826677), (175, 0.8126497792660051)]\n",
      "[(44, 0.5088261394124157), (91, 0.3565687400684555), (133, 0.3838113853229203), (157, 0.4557844598094683), (160, 0.5088261394124157)]\n",
      "[(76, 0.35292644737407497), (299, 0.4520701914335886), (310, 0.4123318633644305), (330, 0.4802649838980847), (339, 0.5200033119672429)]\n",
      "[(2, 0.2777042752325495), (29, 0.625061058008233), (36, 0.42316617663900047), (84, 0.5942300869426951)]\n",
      "[(50, 0.3916417333890055), (69, 0.3117602333998287), (90, 0.34796665721241216), (196, 0.4241548622372332), (233, 0.47351566386312544), (323, 0.47351566386312544)]\n",
      "[(2, 0.18820569784843647), (161, 0.561596257166881), (282, 0.5286267706323015), (340, 0.6080641385613835)]\n",
      "[(257, 0.7546821231853805), (341, 0.6560906133640428)]\n",
      "[(109, 0.3481701808362443), (121, 0.37707440882206944), (204, 0.45578722174637787), (207, 0.33669933303482835), (263, 0.45578722174637787), (306, 0.45578722174637787)]\n",
      "[(11, 0.44150049476074316), (36, 0.29889572219946225), (61, 0.3473253824581216), (153, 0.5025146508883406), (261, 0.5853057019368336)]\n",
      "[(43, 0.4325947334206626), (70, 0.35491511700630296), (90, 0.4325947334206626), (98, 0.47085072546806844), (129, 0.5273124759381361)]\n",
      "[(2, 0.15211626011884088), (133, 0.34238562013680435), (224, 0.49146462475769703), (225, 0.45390720532680434), (304, 0.45390720532680434), (342, 0.45390720532680434)]\n",
      "[(23, 0.43258692506126967), (78, 0.4459891505389573), (165, 0.49946964184552917), (343, 0.6037319825404924)]\n",
      "[(41, 0.36177893181636855), (43, 0.33238487720441584), (70, 0.2726996157611941), (336, 0.4523118595568076), (337, 0.48973727608156015), (344, 0.48973727608156015)]\n",
      "[(80, 0.36663461753285853), (148, 0.32121959048781373), (249, 0.37912531615866996), (259, 0.4963103793973413), (285, 0.43147249846509994), (345, 0.43147249846509994)]\n",
      "[(70, 0.40197707061669335), (265, 0.6275952935943839), (346, 0.6667372662125622)]\n",
      "[(7, 0.3115856163382449), (99, 0.478282517781364), (279, 0.6031755374736234), (347, 0.5570812399185964)]\n",
      "[(70, 0.3109688523567814), (88, 0.48550676757297145), (171, 0.3463844646821275), (338, 0.5584644418156554), (341, 0.48550676757297145)]\n",
      "[(5, 0.6106476923966558), (11, 0.48934393706026685), (115, 0.44479983719241745), (171, 0.4356661702113162)]\n",
      "[(162, 0.5920372396072239), (202, 0.5467941238130831), (348, 0.5920372396072239)]\n",
      "[(41, 0.3334375246126933), (45, 0.41687819145902627), (48, 0.2799608103257518), (116, 0.2921346211934298), (208, 0.41687819145902627), (209, 0.41687819145902627), (349, 0.45137173750649645)]\n",
      "[(36, 0.3200541472892287), (55, 0.38410200148433027), (166, 0.5380869853436185), (242, 0.6785966338426037)]\n",
      "[(12, 0.28250248488250496), (25, 0.4038879092856443), (65, 0.307692236379095), (121, 0.38434922007304595), (183, 0.3683849485806646), (293, 0.4038879092856443), (350, 0.4645806214872139)]\n",
      "[(9, 0.22010279079549022), (77, 0.37895554808223325), (253, 0.3953779053919924), (304, 0.4413898036226895), (351, 0.47791150187429854), (352, 0.47791150187429854)]\n",
      "[(2, 0.25298865967608386), (115, 0.5175965095535101), (191, 0.8173680880566121)]\n",
      "[(12, 0.30805334172020243), (155, 0.3629894045222935), (234, 0.44041743628405805), (235, 0.5065994835659858), (245, 0.40170342040317575), (311, 0.40170342040317575)]\n",
      "[(17, 0.4658449707584427), (36, 0.45449005110474594), (55, 0.5454406379752091), (61, 0.5281304451658396)]\n",
      "[(63, 0.4054944649448458), (153, 0.34813757804618867), (203, 0.43904609293866487), (335, 0.4054944649448458), (342, 0.4054944649448458), (353, 0.43904609293866487)]\n",
      "[(36, 0.3509591242524689), (199, 0.5684263570185291), (315, 0.7441230877688071)]\n",
      "[(36, 0.3126886314548427), (61, 0.36335313771364264), (76, 0.4499646832338076), (84, 0.43909225952601505), (330, 0.612315350566345)]\n",
      "[(22, 0.6984072336468584), (43, 0.7157005910223511)]\n",
      "[(27, 0.5307336482274102), (36, 0.2879311281973855), (55, 0.3455506624955991), (245, 0.48408056596219673), (266, 0.5307336482274102)]\n",
      "[(354, 1.0)]\n",
      "[(153, 0.4181011201548098), (234, 0.4583954581747408), (249, 0.40278192381768707), (311, 0.4181011201548098), (324, 0.5272790839973449)]\n",
      "[(8, 0.4010548540673281), (65, 0.34772010099081463), (76, 0.35633004613940267), (79, 0.35633004613940267), (167, 0.5250181887035904), (281, 0.4343494369317811)]\n",
      "[(2, 0.20579662150371592), (77, 0.5272245822920378), (165, 0.5500723028669311), (189, 0.6140866811968109)]\n",
      "[(37, 0.6660441157518326), (274, 0.7459123513338276)]\n",
      "[(17, 0.23267291376048627), (47, 0.3981823024141823), (197, 0.3555471082430051), (291, 0.4813013058427267), (342, 0.4445205608906689), (353, 0.4813013058427267)]\n",
      "[(17, 1.0)]\n",
      "[(55, 0.37991954044031845), (84, 0.4445413968276445), (110, 0.45554874737210965), (313, 0.6712074404130557)]\n",
      "[(116, 0.37807732699439583), (228, 0.5078456411813006), (229, 0.5078456411813006), (355, 0.5841602042925212)]\n",
      "[(18, 0.5751623140255213), (20, 0.4694596836163656), (152, 0.6699223223506063)]\n",
      "[(2, 0.1623547332432475), (133, 0.36543053306863216), (144, 0.43395679357567546), (231, 0.5245435825775062), (232, 0.43395679357567546), (281, 0.43395679357567546)]\n",
      "[(2, 0.17235200505767012), (80, 0.4113514898412001), (252, 0.4253656261043272), (258, 0.5568432554529594), (356, 0.5568432554529594)]\n",
      "[(2, 0.3530608500834794), (9, 0.5253445759705164), (72, 0.7741841593811444)]\n",
      "[(16, 0.2814720482679247), (79, 0.29516489351797404), (218, 0.7561646394593614), (245, 0.3448477478794074), (264, 0.3780823197296807)]\n",
      "[(331, 0.6784798696991756), (357, 0.7346189940458862)]\n",
      "[(109, 0.3773986174909041), (137, 0.494049969885292), (156, 0.3539974085851202), (276, 0.494049969885292), (320, 0.494049969885292)]\n",
      "[(10, 0.5196341185753084), (11, 0.4164099997111124), (61, 0.32758686371897683), (222, 0.4739567183149841), (223, 0.4739567183149841)]\n",
      "[(7, 0.3054579963992224), (37, 0.43681538701148), (95, 0.39162757654046143), (341, 0.5140644592431358), (346, 0.546125721003773)]\n",
      "[(41, 0.33456062801605124), (43, 0.3073780242045485), (70, 0.25218316127681456), (141, 0.4528920736928989), (261, 0.4182823445043597), (289, 0.4182823445043597), (336, 0.4182823445043597)]\n",
      "[(2, 0.16590992821308675), (106, 0.5360298797059491), (183, 0.42503998346089106), (293, 0.466003051819133), (328, 0.5360298797059491)]\n",
      "[(85, 1.0)]\n",
      "[(90, 0.5363713041549882), (171, 0.4901740545189607), (247, 0.687048193619283)]\n",
      "[(9, 0.20170136518625897), (23, 0.3138050851022581), (26, 0.40448794696434803), (110, 0.2972411042110328), (166, 0.3472734312765451), (206, 0.4379562931386351), (207, 0.32352726176302915), (273, 0.4379562931386351)]\n",
      "[(36, 1.0)]\n",
      "[(65, 0.42390323171535366), (116, 0.41424754769836253), (199, 0.48892326975011685), (358, 0.6400461355746369)]\n",
      "[(7, 0.299755869825623), (16, 0.37556289767556184), (70, 0.32311370525134236), (76, 0.3938330053867068), (156, 0.4157796415791486), (327, 0.5802752065957645)]\n",
      "[(69, 0.5183648298824742), (90, 0.5785653773210974), (186, 0.6297301067173222)]\n",
      "[(20, 0.46273114935613596), (260, 0.6603206558924655), (329, 0.591486698766163)]\n",
      "[(17, 0.27591976022983866), (30, 0.45257942083357533), (43, 0.3873754646223853), (70, 0.3178157238870767), (111, 0.4359969419267412), (289, 0.5271434676081155)]\n",
      "[(48, 0.2483576306581575), (59, 0.36981918927635704), (65, 0.26519789726852533), (66, 0.36981918927635704), (67, 0.33126804296286316), (345, 0.696216619146462)]\n",
      "[(2, 0.28945172226539595), (148, 0.6052592397728267), (183, 0.741538234447298)]\n",
      "[(7, 0.560181322928512), (71, 0.8283700172278461)]\n",
      "[]\n",
      "[(0, 0.5807875410621303), (39, 0.5807875410621303), (91, 0.37589448829770167), (186, 0.42903962283385644)]\n",
      "[]\n",
      "[(55, 0.3539083575514794), (164, 0.5435702897657144), (245, 0.4957888281416724), (319, 0.5774717763052483)]\n",
      "[(12, 0.4297448209703335), (17, 0.34164750894324414), (98, 0.522071013251847), (225, 0.6527160374958176)]\n",
      "[(2, 0.2147123890164122), (80, 0.512452762471265), (252, 0.5299112694149511), (314, 0.6406908792744512)]\n",
      "[(2, 1.0)]\n",
      "[(9, 0.22500054828515406), (47, 0.40417590877206094), (178, 0.40417590877206094), (247, 0.4247225030053364), (268, 0.45121167007309243), (359, 0.4885460541634434)]\n",
      "[(18, 0.4115692488788014), (41, 0.3834266031414034), (112, 0.5190415573867321), (144, 0.42940494833485), (347, 0.4793767260014657)]\n",
      "[(97, 0.7901057500651846), (195, 0.6129705569714845)]\n",
      "[(20, 0.5044455740185507), (36, 0.3676016124751883), (55, 0.4411644601280218), (329, 0.6448082168200768)]\n",
      "[(57, 0.318670962696351), (147, 0.49237254228036303), (176, 0.45474574035988785), (268, 0.45474574035988785), (358, 0.49237254228036303)]\n",
      "[(50, 0.4127768063904349), (52, 0.46977036977184433), (92, 0.44704451676848556), (115, 0.3421838591942289), (239, 0.5403633169680502)]\n",
      "[(35, 0.513497419998626), (154, 0.5629855404947535), (322, 0.6475860412706446)]\n",
      "[(48, 0.3528044598968517), (181, 0.5688151919491532), (280, 0.5253466905219536), (292, 0.5253466905219536)]\n",
      "[(91, 0.37532172758116195), (133, 0.40399714281476734), (179, 0.5041442313084115), (233, 0.5355867865647929), (271, 0.39358010302687635)]\n",
      "[(43, 0.5626368807690659), (70, 0.4616060227807582), (129, 0.6858276898252111)]\n",
      "[(17, 0.3740600543683365), (49, 0.7146400605348208), (269, 0.5910741574495918)]\n",
      "[(20, 0.6989143764110495), (22, 0.7152053512425323)]\n",
      "[(2, 0.16260097625776618), (70, 0.29252375328640395), (280, 0.48519306653287375), (341, 0.4567089623929605), (345, 0.4567089623929605), (347, 0.48519306653287375)]\n",
      "[(9, 0.26046413270608637), (23, 0.4052276456058786), (115, 0.35813232468667805), (321, 0.5655485075680505), (343, 0.5655485075680505)]\n",
      "[(2, 0.19129253630956483), (16, 0.40000306163244687), (148, 0.40000306163244687), (165, 0.5113044383343213), (350, 0.618037246661733)]\n",
      "[(2, 0.25950954517739516), (41, 0.6193698646036325), (55, 0.47457497046448377), (76, 0.5690468910303736)]\n",
      "[(10, 0.4586073218086443), (63, 0.4872098231593502), (65, 0.34937889751815027), (67, 0.4364214981545277), (72, 0.3580299163552375), (115, 0.33405262087395504)]\n",
      "[(2, 0.19387588444767687), (69, 0.3808917773886599), (98, 0.46272240282407495), (124, 0.5785158064046284), (329, 0.5182094509096256)]\n",
      "[(23, 0.3166388311561577), (24, 0.40814058415213195), (30, 0.3504094056830256), (69, 0.26871762327168064), (201, 0.4419111586789998), (214, 0.4419111586789998), (293, 0.38417998020989347)]\n",
      "[(222, 0.5277471680542717), (223, 0.5277471680542717), (226, 0.665556799395358)]\n",
      "[(20, 0.4027866341010512), (70, 0.34653551395946647), (335, 0.5747794043785563), (344, 0.6223380924036599)]\n",
      "[(57, 0.3930024463626329), (58, 0.5608172985773863), (232, 0.5023558926808983), (264, 0.5278935421636891)]\n",
      "[(20, 0.354393797162209), (41, 0.40449909696125236), (98, 0.40449909696125236), (107, 0.453004336186041), (171, 0.3396255268294902), (198, 0.47603316120880623)]\n",
      "[(110, 0.48328317157432693), (164, 0.6190465960140075), (266, 0.6190465960140075)]\n",
      "[(78, 0.4479009807351611), (98, 0.4479009807351611), (238, 0.48077590001042625), (359, 0.6063200119426428)]\n",
      "[(2, 0.288626143222074), (12, 0.5670397069025828), (329, 0.7714667330770552)]\n",
      "[(115, 0.37544516086409785), (171, 0.36773564574681783), (195, 0.42481716422605326), (197, 0.4379786702538719), (290, 0.5928882587912222)]\n",
      "[(2, 0.25439129876182687), (27, 0.714526989771148), (215, 0.6517179205782612)]\n",
      "[(2, 0.32598284267747796), (55, 0.5961372165707458), (133, 0.7337272008718189)]\n",
      "[(15, 0.3448032213816489), (29, 0.3251729845971045), (126, 0.43108808315978037), (284, 0.43108808315978037), (319, 0.43108808315978037), (351, 0.46675739124938104)]\n",
      "[(107, 0.4355878747982709), (111, 0.4021983608284398), (229, 0.45773132056574795), (269, 0.4021983608284398), (339, 0.5265151456470124)]\n",
      "[(17, 0.242117462364463), (70, 0.27888084747162095), (80, 0.36997930789414785), (105, 0.46256432874892783), (252, 0.3825839551688366), (271, 0.3399189836875047), (300, 0.500838060262145)]\n",
      "[(36, 0.29997947745989945), (55, 0.3600100753965411), (61, 0.3485847370837163), (166, 0.504336700706717), (244, 0.6360332004765178)]\n",
      "[(36, 0.22801360444881902), (53, 0.4834471471774453), (61, 0.26495833325440415), (278, 0.4834471471774453), (340, 0.4834471471774453), (345, 0.4202897161283801)]\n",
      "[(50, 0.6070396773261466), (356, 0.7946715234307618)]\n",
      "[(17, 0.27093658211618044), (87, 0.5176231281139289), (155, 0.40157629167379233), (269, 0.4281227308995156), (334, 0.5604525626361428)]\n",
      "[(2, 0.15743672754896487), (25, 0.4422037686135771), (171, 0.31548997027855336), (311, 0.40333272875390536), (357, 0.5086542501602582), (360, 0.5086542501602582)]\n",
      "[(74, 0.45472859368281787), (75, 0.34642398229562504), (270, 0.523061195455992), (271, 0.35500183407129665), (303, 0.523061195455992)]\n",
      "[(57, 0.3347306270293502), (69, 0.3144908152218341), (156, 0.370574891697783), (199, 0.39507196496879504), (292, 0.4776629960936643), (355, 0.5171860291726178)]\n",
      "[(36, 0.4265765867752408), (333, 0.9044514445868198)]\n",
      "[(2, 0.17127885271664664), (29, 0.3855170785680258), (274, 0.45781000804500843), (288, 0.5533760625823425), (360, 0.5533760625823425)]\n",
      "[(17, 0.3389933828993583), (269, 0.5356632600521433), (271, 0.4759272010644205), (296, 0.6096241936371503)]\n",
      "[(164, 0.6560906133640428), (361, 0.7546821231853805)]\n",
      "[(2, 0.16632052343625142), (3, 0.5373564507583946), (12, 0.3267560582293203), (13, 0.44455692553223125), (38, 0.4260918758264816), (184, 0.44455692553223125)]\n",
      "[(2, 0.2956770890538167), (316, 0.9552879455999963)]\n",
      "[(7, 0.3047042667404378), (69, 0.3586790707104458), (99, 0.4677196771404786), (221, 0.512795984906153), (323, 0.5447781342383321)]\n",
      "[(10, 0.5453421126065661), (115, 0.3972308188859996), (234, 0.5453421126065661), (311, 0.4974049024313823)]\n",
      "[(159, 0.7946715234307618), (252, 0.6070396773261466)]\n",
      "[(99, 0.482731355606481), (151, 0.5622630393774382), (228, 0.5292544082317043), (271, 0.4131833542529067)]\n",
      "[(144, 0.6374384688708215), (275, 0.7705012643750968)]\n",
      "[(9, 0.28479426020130827), (28, 0.49033619508050436), (76, 0.4196925504050484), (110, 0.4196925504050484), (126, 0.5711208028249146)]\n",
      "[(149, 1.0)]\n",
      "[(2, 0.24973828462924766), (136, 0.7014577370188076), (274, 0.6675236567844576)]\n",
      "[(57, 0.28956206279238594), (171, 0.27749545537526926), (249, 0.34176093817006703), (282, 0.3889490877689129), (287, 0.41320713224232425), (308, 0.44739692565238465), (318, 0.44739692565238465)]\n",
      "[(169, 0.5375204455151174), (184, 0.5375204455151174), (312, 0.6497257431458752)]\n",
      "[(17, 0.23305564040127327), (30, 0.3822712322848699), (142, 0.48209300449391224), (173, 0.41911247835299503), (217, 0.3988372775574458), (362, 0.48209300449391224)]\n",
      "[(75, 0.36504720489797354), (284, 0.5090593204299559), (309, 0.5511801640478222), (363, 0.5511801640478222)]\n",
      "[(115, 0.3507290352534735), (157, 0.45820832242285536), (186, 0.40914586867013436), (215, 0.4391762503457041), (364, 0.5538575235338496)]\n",
      "[(8, 0.6176676401091511), (40, 0.5633115741807435), (79, 0.5487866222458462)]\n",
      "[(35, 0.7419551577624572), (195, 0.670449508814562)]\n",
      "[(2, 0.3064992385964161), (11, 0.6898732048520463), (22, 0.6558453918165346)]\n",
      "[(17, 0.463739236982673), (87, 0.8859717377449058)]\n",
      "[(7, 0.25389860540375747), (76, 0.3335836288637973), (143, 0.4539431298984729), (177, 0.4539431298984729), (295, 0.4539431298984729), (302, 0.4539431298984729)]\n",
      "[(7, 0.3955566936832887), (95, 0.5071430807431904), (113, 0.7657289322839061)]\n",
      "[(42, 1.0)]\n",
      "[(44, 0.75808851502575), (48, 0.25455286377833325), (88, 0.356791804109563), (187, 0.325428732549523), (264, 0.356791804109563)]\n",
      "[(29, 0.7071067811865475), (40, 0.7071067811865475)]\n",
      "[(12, 0.4078632525770093), (143, 0.6194813132619442), (354, 0.6707387492284606)]\n",
      "[(156, 0.36996340531018235), (176, 0.4768748034057203), (237, 0.4094212213513226), (246, 0.4488790373924628), (277, 0.5163326194468606)]\n",
      "[(9, 0.30965275419080895), (190, 0.6723522781332887), (361, 0.6723522781332887)]\n",
      "[(2, 0.2357933623208322), (12, 0.4632435494836822), (153, 0.6040723897626824), (311, 0.6040723897626824)]\n",
      "[(20, 0.6806278469656141), (133, 0.7326293291528485)]\n",
      "[(2, 0.18980012509764727), (9, 0.2824172269879214), (55, 0.3470947039762963), (133, 0.4272050435820048), (207, 0.45299481259205016), (250, 0.6132154918034672)]\n",
      "[(72, 0.5615744759636864), (267, 0.8274261948633913)]\n",
      "[(2, 0.13197838419676491), (18, 0.33811171423171826), (79, 0.28939933272605917), (119, 0.39381680490632165), (152, 0.39381680490632165), (301, 0.37069710337563305), (302, 0.39381680490632165), (363, 0.4264021940502365)]\n",
      "[(115, 0.3928114534233468), (155, 0.444467168879624), (232, 0.5131867025822829), (348, 0.620312426233985)]\n",
      "[(7, 0.3127235523454214), (182, 0.5591157459053482), (202, 0.5591157459053482), (221, 0.5262918821052371)]\n",
      "[(29, 0.5383022855261677), (43, 0.5244221964563056), (70, 0.4302534239014893), (91, 0.5000939916478638)]\n",
      "[(70, 0.3942807116850997), (217, 0.5857996132907127), (362, 0.7080829989920268)]\n",
      "[(2, 0.3180593605172416), (36, 0.48465931401891205), (215, 0.8148273390860674)]\n",
      "[(7, 0.31967273292318665), (20, 0.4005166536166209), (133, 0.43111701726848844), (154, 0.537986867333985), (184, 0.5119609379619227)]\n",
      "[(7, 0.6150173860108614), (95, 0.7885135476986854)]\n",
      "[(20, 0.3932472984501381), (36, 0.28656875678418847), (84, 0.40241348826940726), (166, 0.48179009626244607), (219, 0.6075990433658558)]\n",
      "[(2, 0.14824981956077937), (42, 0.37979704730668085), (155, 0.3431942498841803), (178, 0.39625587169288135), (199, 0.36588130956889087), (209, 0.4423699427954956), (349, 0.47897274021799624)]\n",
      "[(54, 0.5815998559657433), (57, 0.37642023079980796), (281, 0.4811596538819074), (331, 0.5371543585055314)]\n",
      "[(2, 0.1539904110758231), (9, 0.22913338363918256), (77, 0.39450370741370067), (148, 0.32200229596353774), (192, 0.4975197229838363), (253, 0.411599857281301), (332, 0.4975197229838363)]\n",
      "[(265, 0.4739522835763629), (266, 0.4739522835763629), (346, 0.5035118221761837), (352, 0.5451736518893048)]\n",
      "[(9, 0.2269165896532758), (12, 0.2996052112229631), (56, 0.4927063749144109), (62, 0.45505406171270596), (67, 0.4076177571013548), (135, 0.4927063749144109)]\n",
      "[(16, 0.46175554022581905), (186, 0.5270399779393103), (364, 0.7134498459769216)]\n",
      "[(37, 0.7445694506560327), (95, 0.6675450045875362)]\n"
     ]
    }
   ],
   "source": [
    "#COMPUTE AND APPLY MAPPING FROM COUNTS TO NUMBERS THAT ACCOUNT FOR DOCUMENT FREQUENCY\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 topics:\n",
      "[(0,\n",
      "  u'0.580*\"learning\" + 0.423*\"networks\" + 0.274*\"neural\" + 0.231*\"deep\" + 0.180*\"convolutional\" + 0.166*\"optimization\" + 0.161*\"models\" + 0.143*\"recurrent\" + 0.133*\"inference\" + 0.124*\"bayesian\"'),\n",
      " (1,\n",
      "  u'-0.539*\"networks\" + 0.482*\"learning\" + -0.381*\"neural\" + -0.249*\"convolutional\" + 0.232*\"inference\" + 0.156*\"variational\" + -0.130*\"deep\" + 0.123*\"online\" + -0.104*\"recurrent\" + 0.081*\"stochastic\"')]\n",
      "\n",
      "3 topics:\n",
      "[(0,\n",
      "  u'0.581*\"learning\" + 0.423*\"networks\" + 0.275*\"neural\" + 0.231*\"deep\" + 0.180*\"convolutional\" + 0.166*\"optimization\" + 0.161*\"models\" + 0.143*\"recurrent\" + 0.132*\"inference\" + 0.124*\"bayesian\"'),\n",
      " (1,\n",
      "  u'-0.538*\"networks\" + 0.482*\"learning\" + -0.383*\"neural\" + -0.248*\"convolutional\" + 0.230*\"inference\" + 0.156*\"variational\" + -0.130*\"deep\" + 0.125*\"online\" + -0.105*\"recurrent\" + 0.081*\"data\"'),\n",
      " (2,\n",
      "  u'0.517*\"inference\" + -0.463*\"learning\" + 0.421*\"variational\" + 0.276*\"models\" + 0.199*\"optimization\" + 0.137*\"probabilistic\" + 0.125*\"stochastic\" + 0.103*\"fast\" + 0.099*\"map\" + 0.097*\"local\"')]\n",
      "\n",
      "4 topics:\n",
      "[(0,\n",
      "  u'0.581*\"learning\" + 0.423*\"networks\" + 0.274*\"neural\" + 0.231*\"deep\" + 0.179*\"convolutional\" + 0.166*\"optimization\" + 0.161*\"models\" + 0.143*\"recurrent\" + 0.132*\"inference\" + 0.125*\"bayesian\"'),\n",
      " (1,\n",
      "  u'-0.539*\"networks\" + 0.483*\"learning\" + -0.381*\"neural\" + -0.250*\"convolutional\" + 0.232*\"inference\" + 0.155*\"variational\" + -0.131*\"deep\" + 0.124*\"online\" + -0.105*\"recurrent\" + 0.080*\"stochastic\"'),\n",
      " (2,\n",
      "  u'0.517*\"inference\" + -0.463*\"learning\" + 0.420*\"variational\" + 0.276*\"models\" + 0.200*\"optimization\" + 0.138*\"probabilistic\" + 0.126*\"stochastic\" + 0.103*\"fast\" + 0.096*\"local\" + 0.096*\"map\"'),\n",
      " (3,\n",
      "  u'0.633*\"optimization\" + -0.293*\"inference\" + 0.265*\"stochastic\" + -0.246*\"variational\" + 0.218*\"robust\" + 0.191*\"online\" + -0.181*\"models\" + -0.176*\"learning\" + 0.152*\"gradient\" + 0.136*\"convex\"')]\n",
      "\n",
      "5 topics:\n",
      "[(0,\n",
      "  u'-0.581*\"learning\" + -0.423*\"networks\" + -0.273*\"neural\" + -0.230*\"deep\" + -0.180*\"convolutional\" + -0.166*\"optimization\" + -0.161*\"models\" + -0.143*\"recurrent\" + -0.133*\"inference\" + -0.125*\"bayesian\"'),\n",
      " (1,\n",
      "  u'0.540*\"networks\" + -0.482*\"learning\" + 0.381*\"neural\" + 0.248*\"convolutional\" + -0.231*\"inference\" + -0.155*\"variational\" + 0.131*\"deep\" + -0.123*\"online\" + 0.105*\"recurrent\" + -0.082*\"data\"'),\n",
      " (2,\n",
      "  u'0.516*\"inference\" + -0.464*\"learning\" + 0.419*\"variational\" + 0.276*\"models\" + 0.202*\"optimization\" + 0.138*\"probabilistic\" + 0.127*\"stochastic\" + 0.102*\"fast\" + 0.097*\"map\" + 0.095*\"local\"'),\n",
      " (3,\n",
      "  u'0.633*\"optimization\" + -0.295*\"inference\" + 0.263*\"stochastic\" + -0.246*\"variational\" + 0.219*\"robust\" + 0.191*\"online\" + -0.183*\"models\" + -0.176*\"learning\" + 0.151*\"gradient\" + 0.137*\"convex\"'),\n",
      " (4,\n",
      "  u'0.337*\"models\" + 0.316*\"data\" + -0.301*\"online\" + 0.279*\"robust\" + 0.277*\"analysis\" + -0.214*\"inference\" + -0.197*\"variational\" + 0.182*\"regression\" + -0.168*\"prediction\" + 0.166*\"pca\"')]\n",
      "\n",
      "6 topics:\n",
      "[(0,\n",
      "  u'-0.581*\"learning\" + -0.423*\"networks\" + -0.274*\"neural\" + -0.230*\"deep\" + -0.180*\"convolutional\" + -0.166*\"optimization\" + -0.162*\"models\" + -0.143*\"recurrent\" + -0.132*\"inference\" + -0.125*\"bayesian\"'),\n",
      " (1,\n",
      "  u'0.538*\"networks\" + -0.483*\"learning\" + 0.382*\"neural\" + 0.251*\"convolutional\" + -0.231*\"inference\" + -0.155*\"variational\" + 0.131*\"deep\" + -0.124*\"online\" + 0.105*\"recurrent\" + -0.081*\"data\"'),\n",
      " (2,\n",
      "  u'0.517*\"inference\" + -0.464*\"learning\" + 0.420*\"variational\" + 0.273*\"models\" + 0.200*\"optimization\" + 0.140*\"probabilistic\" + 0.127*\"stochastic\" + 0.104*\"fast\" + 0.097*\"local\" + 0.095*\"map\"'),\n",
      " (3,\n",
      "  u'0.634*\"optimization\" + -0.295*\"inference\" + 0.263*\"stochastic\" + -0.247*\"variational\" + 0.216*\"robust\" + 0.190*\"online\" + -0.181*\"models\" + -0.176*\"learning\" + 0.151*\"gradient\" + 0.136*\"convex\"'),\n",
      " (4,\n",
      "  u'0.336*\"models\" + 0.317*\"data\" + -0.300*\"online\" + 0.279*\"analysis\" + 0.278*\"robust\" + -0.215*\"inference\" + -0.194*\"variational\" + 0.183*\"regression\" + -0.168*\"prediction\" + 0.167*\"pca\"'),\n",
      " (5,\n",
      "  u'-0.370*\"models\" + -0.348*\"online\" + -0.309*\"prediction\" + 0.257*\"robust\" + 0.218*\"optimization\" + 0.181*\"variational\" + -0.180*\"gradient\" + -0.163*\"structured\" + -0.150*\"deep\" + 0.145*\"local\"')]\n",
      "\n",
      "7 topics:\n",
      "[(0,\n",
      "  u'-0.581*\"learning\" + -0.423*\"networks\" + -0.273*\"neural\" + -0.231*\"deep\" + -0.180*\"convolutional\" + -0.166*\"optimization\" + -0.161*\"models\" + -0.143*\"recurrent\" + -0.132*\"inference\" + -0.125*\"bayesian\"'),\n",
      " (1,\n",
      "  u'-0.539*\"networks\" + 0.482*\"learning\" + -0.381*\"neural\" + -0.249*\"convolutional\" + 0.231*\"inference\" + 0.155*\"variational\" + -0.131*\"deep\" + 0.124*\"online\" + -0.105*\"recurrent\" + 0.081*\"data\"'),\n",
      " (2,\n",
      "  u'0.516*\"inference\" + -0.463*\"learning\" + 0.420*\"variational\" + 0.276*\"models\" + 0.202*\"optimization\" + 0.137*\"probabilistic\" + 0.126*\"stochastic\" + 0.102*\"fast\" + 0.097*\"map\" + 0.097*\"local\"'),\n",
      " (3,\n",
      "  u'0.635*\"optimization\" + -0.293*\"inference\" + 0.262*\"stochastic\" + -0.247*\"variational\" + 0.218*\"robust\" + 0.191*\"online\" + -0.183*\"models\" + -0.176*\"learning\" + 0.152*\"gradient\" + 0.132*\"convex\"'),\n",
      " (4,\n",
      "  u'-0.337*\"models\" + -0.314*\"data\" + 0.310*\"online\" + -0.281*\"analysis\" + -0.276*\"robust\" + 0.216*\"inference\" + 0.193*\"variational\" + -0.183*\"regression\" + 0.172*\"prediction\" + -0.165*\"pca\"'),\n",
      " (5,\n",
      "  u'-0.373*\"models\" + -0.343*\"online\" + -0.309*\"prediction\" + 0.255*\"robust\" + 0.218*\"optimization\" + 0.184*\"variational\" + -0.178*\"gradient\" + -0.165*\"structured\" + -0.149*\"deep\" + 0.144*\"learning\"'),\n",
      " (6,\n",
      "  u'0.335*\"optimization\" + 0.331*\"models\" + -0.320*\"analysis\" + -0.312*\"prediction\" + -0.300*\"online\" + 0.238*\"deep\" + -0.172*\"data\" + -0.168*\"algorithms\" + -0.147*\"regression\" + 0.143*\"generative\"')]\n",
      "\n",
      "8 topics:\n",
      "[(0,\n",
      "  u'-0.581*\"learning\" + -0.423*\"networks\" + -0.274*\"neural\" + -0.231*\"deep\" + -0.180*\"convolutional\" + -0.166*\"optimization\" + -0.161*\"models\" + -0.143*\"recurrent\" + -0.132*\"inference\" + -0.125*\"bayesian\"'),\n",
      " (1,\n",
      "  u'0.540*\"networks\" + -0.482*\"learning\" + 0.381*\"neural\" + 0.249*\"convolutional\" + -0.231*\"inference\" + -0.156*\"variational\" + 0.130*\"deep\" + -0.124*\"online\" + 0.105*\"recurrent\" + -0.080*\"data\"'),\n",
      " (2,\n",
      "  u'0.518*\"inference\" + -0.464*\"learning\" + 0.421*\"variational\" + 0.275*\"models\" + 0.200*\"optimization\" + 0.138*\"probabilistic\" + 0.126*\"stochastic\" + 0.104*\"fast\" + 0.096*\"map\" + 0.094*\"local\"'),\n",
      " (3,\n",
      "  u'-0.634*\"optimization\" + 0.294*\"inference\" + -0.262*\"stochastic\" + 0.247*\"variational\" + -0.217*\"robust\" + -0.191*\"online\" + 0.181*\"models\" + 0.176*\"learning\" + -0.152*\"gradient\" + -0.136*\"convex\"'),\n",
      " (4,\n",
      "  u'0.331*\"models\" + 0.317*\"data\" + -0.303*\"online\" + 0.281*\"robust\" + 0.280*\"analysis\" + -0.214*\"inference\" + -0.191*\"variational\" + 0.185*\"regression\" + -0.172*\"prediction\" + 0.167*\"pca\"'),\n",
      " (5,\n",
      "  u'-0.373*\"models\" + -0.348*\"online\" + -0.312*\"prediction\" + 0.250*\"robust\" + 0.223*\"optimization\" + -0.183*\"gradient\" + 0.181*\"variational\" + -0.160*\"structured\" + 0.145*\"local\" + -0.145*\"deep\"'),\n",
      " (6,\n",
      "  u'-0.340*\"models\" + -0.333*\"optimization\" + 0.315*\"analysis\" + 0.308*\"online\" + 0.305*\"prediction\" + -0.244*\"deep\" + 0.176*\"data\" + 0.168*\"algorithms\" + 0.149*\"regression\" + 0.135*\"local\"'),\n",
      " (7,\n",
      "  u'0.298*\"graphs\" + 0.290*\"stochastic\" + -0.284*\"prediction\" + -0.274*\"deep\" + 0.235*\"gradient\" + -0.226*\"online\" + -0.220*\"robust\" + 0.211*\"random\" + 0.205*\"estimation\" + 0.154*\"mcmc\"')]\n",
      "\n",
      "9 topics:\n",
      "[(0,\n",
      "  u'0.580*\"learning\" + 0.423*\"networks\" + 0.274*\"neural\" + 0.231*\"deep\" + 0.179*\"convolutional\" + 0.165*\"optimization\" + 0.161*\"models\" + 0.143*\"recurrent\" + 0.133*\"inference\" + 0.125*\"bayesian\"'),\n",
      " (1,\n",
      "  u'0.540*\"networks\" + -0.483*\"learning\" + 0.380*\"neural\" + 0.250*\"convolutional\" + -0.231*\"inference\" + -0.156*\"variational\" + 0.130*\"deep\" + -0.124*\"online\" + 0.105*\"recurrent\" + -0.081*\"data\"'),\n",
      " (2,\n",
      "  u'0.516*\"inference\" + -0.463*\"learning\" + 0.422*\"variational\" + 0.275*\"models\" + 0.202*\"optimization\" + 0.134*\"probabilistic\" + 0.126*\"stochastic\" + 0.104*\"fast\" + 0.097*\"map\" + 0.094*\"local\"'),\n",
      " (3,\n",
      "  u'-0.634*\"optimization\" + 0.293*\"inference\" + -0.264*\"stochastic\" + 0.247*\"variational\" + -0.218*\"robust\" + -0.192*\"online\" + 0.183*\"models\" + 0.177*\"learning\" + -0.151*\"gradient\" + -0.136*\"convex\"'),\n",
      " (4,\n",
      "  u'-0.334*\"models\" + -0.315*\"data\" + 0.309*\"online\" + -0.280*\"robust\" + -0.275*\"analysis\" + 0.215*\"inference\" + 0.195*\"variational\" + -0.183*\"regression\" + 0.170*\"prediction\" + -0.169*\"pca\"'),\n",
      " (5,\n",
      "  u'0.375*\"models\" + 0.344*\"online\" + 0.303*\"prediction\" + -0.249*\"robust\" + -0.218*\"optimization\" + -0.180*\"variational\" + 0.179*\"gradient\" + 0.162*\"structured\" + 0.151*\"deep\" + -0.143*\"learning\"'),\n",
      " (6,\n",
      "  u'-0.339*\"optimization\" + -0.332*\"models\" + 0.315*\"analysis\" + 0.309*\"online\" + 0.304*\"prediction\" + -0.242*\"deep\" + 0.172*\"data\" + 0.167*\"algorithms\" + 0.145*\"regression\" + -0.138*\"generative\"'),\n",
      " (7,\n",
      "  u'-0.295*\"graphs\" + -0.290*\"stochastic\" + 0.282*\"prediction\" + 0.259*\"deep\" + -0.237*\"gradient\" + 0.234*\"online\" + 0.220*\"robust\" + -0.211*\"random\" + -0.202*\"estimation\" + -0.157*\"mcmc\"'),\n",
      " (8,\n",
      "  u'0.454*\"deep\" + -0.274*\"adaptive\" + -0.273*\"online\" + 0.261*\"structured\" + -0.237*\"models\" + 0.206*\"robust\" + -0.181*\"sampling\" + -0.164*\"recurrent\" + 0.158*\"regression\" + -0.147*\"neural\"')]\n",
      "\n",
      "10 topics:\n",
      "[(0,\n",
      "  u'-0.581*\"learning\" + -0.423*\"networks\" + -0.274*\"neural\" + -0.231*\"deep\" + -0.179*\"convolutional\" + -0.166*\"optimization\" + -0.161*\"models\" + -0.143*\"recurrent\" + -0.133*\"inference\" + -0.125*\"bayesian\"'),\n",
      " (1,\n",
      "  u'-0.539*\"networks\" + 0.482*\"learning\" + -0.381*\"neural\" + -0.249*\"convolutional\" + 0.231*\"inference\" + 0.155*\"variational\" + -0.130*\"deep\" + 0.124*\"online\" + -0.105*\"recurrent\" + 0.081*\"data\"'),\n",
      " (2,\n",
      "  u'-0.517*\"inference\" + 0.464*\"learning\" + -0.420*\"variational\" + -0.276*\"models\" + -0.201*\"optimization\" + -0.137*\"probabilistic\" + -0.126*\"stochastic\" + -0.103*\"fast\" + -0.096*\"map\" + -0.095*\"local\"'),\n",
      " (3,\n",
      "  u'0.632*\"optimization\" + -0.294*\"inference\" + 0.262*\"stochastic\" + -0.246*\"variational\" + 0.219*\"robust\" + 0.190*\"online\" + -0.182*\"models\" + -0.175*\"learning\" + 0.152*\"gradient\" + 0.134*\"convex\"'),\n",
      " (4,\n",
      "  u'0.334*\"models\" + 0.315*\"data\" + -0.305*\"online\" + 0.285*\"robust\" + 0.280*\"analysis\" + -0.216*\"inference\" + -0.189*\"variational\" + 0.179*\"regression\" + -0.172*\"prediction\" + 0.166*\"pca\"'),\n",
      " (5,\n",
      "  u'-0.377*\"models\" + -0.340*\"online\" + -0.301*\"prediction\" + 0.253*\"robust\" + 0.218*\"optimization\" + 0.182*\"variational\" + -0.179*\"gradient\" + -0.162*\"structured\" + -0.153*\"deep\" + 0.143*\"learning\"'),\n",
      " (6,\n",
      "  u'0.337*\"optimization\" + 0.333*\"models\" + -0.313*\"analysis\" + -0.312*\"online\" + -0.311*\"prediction\" + 0.238*\"deep\" + -0.176*\"data\" + -0.170*\"algorithms\" + -0.145*\"regression\" + 0.136*\"generative\"'),\n",
      " (7,\n",
      "  u'-0.298*\"graphs\" + -0.294*\"stochastic\" + 0.280*\"prediction\" + 0.269*\"deep\" + 0.231*\"online\" + -0.230*\"gradient\" + 0.220*\"robust\" + -0.208*\"random\" + -0.205*\"estimation\" + -0.158*\"mcmc\"'),\n",
      " (8,\n",
      "  u'0.456*\"deep\" + -0.285*\"online\" + -0.273*\"adaptive\" + 0.254*\"structured\" + -0.237*\"models\" + 0.191*\"robust\" + -0.177*\"sampling\" + -0.162*\"recurrent\" + 0.144*\"regression\" + -0.141*\"neural\"'),\n",
      " (9,\n",
      "  u'-0.360*\"robust\" + 0.333*\"bayesian\" + -0.270*\"bandits\" + -0.210*\"online\" + 0.168*\"games\" + 0.164*\"statistical\" + 0.161*\"kernel\" + -0.146*\"local\" + -0.140*\"approach\" + -0.138*\"models\"')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PRODUCE LSI MODELS FOR TOPIC NUMBERS FROM 2 TO 10\n",
    "for N in range(2,11):\n",
    "    print N, 'topics:'\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=N) \n",
    "    corpus_lsi = lsi[corpus_tfidf]\n",
    "    pprint(lsi.print_topics(N))\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 topics:\n",
      "[(0,\n",
      "  u'0.034*models + 0.026*inference + 0.023*learning + 0.016*fast + 0.013*estimation + 0.013*data + 0.013*efficient + 0.012*variational + 0.012*optimal + 0.011*algorithms'),\n",
      " (1,\n",
      "  u'0.054*learning + 0.032*networks + 0.026*optimization + 0.019*stochastic + 0.019*neural + 0.016*deep + 0.013*robust + 0.012*convolutional + 0.012*online + 0.012*bandits')]\n",
      "\n",
      "3 topics:\n",
      "[(0,\n",
      "  u'0.032*learning + 0.029*stochastic + 0.021*analysis + 0.019*optimization + 0.017*spectral + 0.013*model + 0.013*convergence + 0.013*gradient + 0.012*adaptive + 0.012*gaussian'),\n",
      " (1,\n",
      "  u'0.060*learning + 0.022*efficient + 0.018*data + 0.017*robust + 0.015*models + 0.014*bayesian + 0.014*local + 0.013*optimization + 0.012*submodular + 0.012*deep'),\n",
      " (2,\n",
      "  u'0.040*networks + 0.030*learning + 0.026*inference + 0.025*models + 0.022*neural + 0.016*deep + 0.015*optimization + 0.015*prediction + 0.014*bounds + 0.013*optimal')]\n",
      "\n",
      "4 topics:\n",
      "[(0,\n",
      "  u'0.051*networks + 0.032*neural + 0.027*optimization + 0.026*inference + 0.025*deep + 0.024*models + 0.019*variational + 0.019*learning + 0.019*convolutional + 0.018*online'),\n",
      " (1,\n",
      "  u'0.023*algorithms + 0.022*bandits + 0.022*spectral + 0.021*learning + 0.019*network + 0.019*fast + 0.017*model + 0.015*models + 0.014*image + 0.014*largescale'),\n",
      " (2,\n",
      "  u'0.081*learning + 0.020*bayesian + 0.017*data + 0.014*stochastic + 0.014*optimization + 0.013*estimation + 0.011*linear + 0.011*framework + 0.010*sample + 0.010*active'),\n",
      " (3,\n",
      "  u'0.026*stochastic + 0.025*learning + 0.024*models + 0.022*inference + 0.021*gaussian + 0.016*matrix + 0.016*memory + 0.016*random + 0.014*analysis + 0.013*methods')]\n",
      "\n",
      "5 topics:\n",
      "[(0,\n",
      "  u'0.064*networks + 0.060*learning + 0.050*optimization + 0.033*neural + 0.023*convolutional + 0.019*deep + 0.016*prediction + 0.014*structured + 0.014*linear + 0.014*robust'),\n",
      " (1,\n",
      "  u'0.036*optimal + 0.033*models + 0.023*under + 0.021*fast + 0.019*detection + 0.015*object + 0.014*sparse + 0.011*algorithm + 0.011*analysis + 0.011*pca'),\n",
      " (2,\n",
      "  u'0.022*bounds + 0.021*regression + 0.020*gaussian + 0.020*algorithms + 0.017*graphs + 0.017*sparse + 0.015*bandits + 0.015*selection + 0.013*model + 0.012*is'),\n",
      " (3,\n",
      "  u'0.053*inference + 0.046*models + 0.024*gradient + 0.022*learning + 0.021*variational + 0.020*stochastic + 0.018*fast + 0.017*linear + 0.014*map + 0.013*probabilistic'),\n",
      " (4,\n",
      "  u'0.086*learning + 0.024*analysis + 0.020*deep + 0.015*adaptive + 0.015*bayesian + 0.013*monte + 0.013*carlo + 0.013*kernel + 0.011*active + 0.011*games')]\n",
      "\n",
      "6 topics:\n",
      "[(0,\n",
      "  u'0.033*optimization + 0.030*inference + 0.023*linear + 0.023*efficient + 0.022*fast + 0.017*map + 0.014*measurements + 0.014*lifted + 0.013*model + 0.011*sequential'),\n",
      " (1,\n",
      "  u'0.100*learning + 0.035*networks + 0.026*optimization + 0.021*stochastic + 0.020*online + 0.019*prediction + 0.018*convolutional + 0.017*bayesian + 0.015*neural + 0.014*descent'),\n",
      " (2,\n",
      "  u'0.038*gaussian + 0.038*models + 0.023*framework + 0.021*learning + 0.020*probabilistic + 0.019*modeling + 0.018*process + 0.016*bounds + 0.016*graph + 0.012*estimation'),\n",
      " (3,\n",
      "  u'0.042*neural + 0.031*networks + 0.017*kernel + 0.017*methods + 0.017*image + 0.017*object + 0.017*detection + 0.016*sampling + 0.015*learning + 0.013*network'),\n",
      " (4,\n",
      "  u'0.051*learning + 0.046*models + 0.046*inference + 0.025*variational + 0.018*deep + 0.015*network + 0.015*random + 0.014*spectral + 0.013*stochastic + 0.012*structured'),\n",
      " (5,\n",
      "  u'0.031*analysis + 0.029*bandits + 0.023*robust + 0.021*data + 0.018*adaptive + 0.017*graphs + 0.016*optimal + 0.016*estimation + 0.015*clustering + 0.013*optimization')]\n",
      "\n",
      "7 topics:\n",
      "[(0,\n",
      "  u'0.077*models + 0.031*learning + 0.026*image + 0.024*bandits + 0.023*generative + 0.019*inference + 0.017*data + 0.016*network + 0.016*variational + 0.014*deep'),\n",
      " (1,\n",
      "  u'0.057*stochastic + 0.051*optimization + 0.035*analysis + 0.032*gradient + 0.028*matrix + 0.021*decision + 0.020*under + 0.019*convergence + 0.017*nonconvex + 0.016*completion'),\n",
      " (2,\n",
      "  u'0.047*learning + 0.030*inference + 0.024*efficient + 0.019*models + 0.018*graph + 0.018*spectral + 0.018*semisupervised + 0.018*optimization + 0.015*deep + 0.015*is'),\n",
      " (3,\n",
      "  u'0.110*networks + 0.052*neural + 0.040*convolutional + 0.039*learning + 0.032*deep + 0.024*robust + 0.020*gaussian + 0.020*recurrent + 0.016*memory + 0.014*optimization'),\n",
      " (4,\n",
      "  u'0.068*learning + 0.028*fast + 0.024*inference + 0.024*algorithms + 0.019*probabilistic + 0.018*variational + 0.016*structured + 0.016*guarantees + 0.013*optimal + 0.013*sparse'),\n",
      " (5,\n",
      "  u'0.044*learning + 0.034*prediction + 0.033*bayesian + 0.032*linear + 0.018*model + 0.016*efficient + 0.015*graphs + 0.015*online + 0.015*structured + 0.015*games'),\n",
      " (6,\n",
      "  u'0.025*learning + 0.020*optimization + 0.020*models + 0.019*adaptive + 0.017*online + 0.017*stochastic + 0.015*kernel + 0.015*statistical + 0.013*bandits + 0.013*bounds')]\n",
      "\n",
      "8 topics:\n",
      "[(0,\n",
      "  u'0.065*models + 0.058*learning + 0.053*inference + 0.026*fast + 0.018*bayesian + 0.015*stochastic + 0.013*variational + 0.013*deep + 0.012*model + 0.012*search'),\n",
      " (1,\n",
      "  u'0.045*learning + 0.041*bandits + 0.032*analysis + 0.032*efficient + 0.031*online + 0.025*optimization + 0.022*monte + 0.022*carlo + 0.019*kernel + 0.015*active'),\n",
      " (2,\n",
      "  u'0.040*regression + 0.028*probabilistic + 0.020*estimation + 0.020*gaussian + 0.020*process + 0.020*inference + 0.020*variational + 0.016*lasso + 0.016*model + 0.016*robust'),\n",
      " (3,\n",
      "  u'0.029*gradient + 0.024*random + 0.021*sparse + 0.021*linear + 0.021*training + 0.021*descent + 0.018*learning + 0.018*stochastic + 0.018*decision + 0.017*mcmc'),\n",
      " (4,\n",
      "  u'0.096*learning + 0.041*networks + 0.027*network + 0.023*deep + 0.023*graphs + 0.020*convolutional + 0.020*bounds + 0.016*algorithms + 0.016*minimization + 0.016*semisupervised'),\n",
      " (5,\n",
      "  u'0.039*matrix + 0.030*data + 0.030*optimization + 0.029*optimal + 0.024*robust + 0.022*sparse + 0.021*private + 0.020*completion + 0.018*estimation + 0.018*spectral'),\n",
      " (6,\n",
      "  u'0.037*optimization + 0.021*stochastic + 0.021*learning + 0.021*control + 0.016*systems + 0.016*under + 0.016*is + 0.014*linear + 0.011*fast + 0.011*sample'),\n",
      " (7,\n",
      "  u'0.076*networks + 0.061*neural + 0.036*learning + 0.035*recurrent + 0.031*deep + 0.027*models + 0.025*spectral + 0.025*convolutional + 0.018*optimization + 0.018*bayesian')]\n",
      "\n",
      "9 topics:\n",
      "[(0,\n",
      "  u'0.046*stochastic + 0.042*gradient + 0.024*bandits + 0.019*descent + 0.019*discrete + 0.019*framework + 0.019*learning + 0.017*inference + 0.015*algorithms + 0.015*information'),\n",
      " (1,\n",
      "  u'0.040*optimization + 0.034*carlo + 0.034*monte + 0.023*detection + 0.022*inference + 0.021*bayesian + 0.018*linear + 0.018*map + 0.017*online + 0.016*games'),\n",
      " (2,\n",
      "  u'0.041*learning + 0.033*optimization + 0.030*data + 0.024*private + 0.023*subspace + 0.020*clustering + 0.018*distributions + 0.018*smoothness + 0.018*convex + 0.017*distributed'),\n",
      " (3,\n",
      "  u'0.053*optimization + 0.031*analysis + 0.029*sparse + 0.029*robust + 0.024*pca + 0.023*optimal + 0.019*stochastic + 0.018*highdimensional + 0.017*data + 0.017*nonconvex'),\n",
      " (4,\n",
      "  u'0.103*networks + 0.053*neural + 0.044*deep + 0.034*convolutional + 0.025*recurrent + 0.025*learning + 0.024*models + 0.016*graphs + 0.016*semisupervised + 0.015*training'),\n",
      " (5,\n",
      "  u'0.059*models + 0.050*inference + 0.032*probabilistic + 0.032*variational + 0.023*under + 0.023*fast + 0.019*markov + 0.015*learning + 0.014*hidden + 0.014*algorithms'),\n",
      " (6,\n",
      "  u'0.046*learning + 0.043*adaptive + 0.026*statistical + 0.025*models + 0.021*causal + 0.021*graphs + 0.017*selection + 0.017*prediction + 0.017*random + 0.014*optimization'),\n",
      " (7,\n",
      "  u'0.093*learning + 0.030*structured + 0.022*inference + 0.022*deep + 0.019*robust + 0.019*convergence + 0.015*linear + 0.015*is + 0.014*estimation + 0.013*models'),\n",
      " (8,\n",
      "  u'0.086*learning + 0.025*bayesian + 0.022*gaussian + 0.018*model + 0.018*methods + 0.014*network + 0.014*spectral + 0.014*linear + 0.014*latent + 0.014*memory')]\n",
      "\n",
      "10 topics:\n",
      "[(0,\n",
      "  u'0.049*learning + 0.039*bayesian + 0.027*models + 0.026*prediction + 0.023*stochastic + 0.023*model + 0.020*deep + 0.019*probabilistic + 0.016*online + 0.016*network'),\n",
      " (1,\n",
      "  u'0.055*inference + 0.048*variational + 0.037*optimization + 0.029*bandits + 0.024*robust + 0.022*models + 0.019*network + 0.015*gradient + 0.015*online + 0.015*regression'),\n",
      " (2,\n",
      "  u'0.029*sampling + 0.025*bayesian + 0.022*data + 0.021*causal + 0.021*fast + 0.017*learning + 0.017*robust + 0.016*clustering + 0.016*subspace + 0.016*bandits'),\n",
      " (3,\n",
      "  u'0.048*analysis + 0.047*models + 0.029*linear + 0.027*generative + 0.025*random + 0.022*kernels + 0.017*systems + 0.017*local + 0.017*selection + 0.016*highdimensional'),\n",
      " (4,\n",
      "  u'0.046*estimation + 0.036*stochastic + 0.029*fast + 0.026*graphs + 0.021*inference + 0.020*spectral + 0.020*gradient + 0.020*sparse + 0.020*beyond + 0.020*propagation'),\n",
      " (5,\n",
      "  u'0.060*optimal + 0.052*carlo + 0.052*monte + 0.050*kernel + 0.020*analysis + 0.018*learning + 0.018*efficient + 0.018*features + 0.018*approval + 0.018*nearly'),\n",
      " (6,\n",
      "  u'0.032*optimization + 0.025*model + 0.023*statistical + 0.020*stochastic + 0.019*inference + 0.016*estimation + 0.015*sample + 0.015*dimensional + 0.015*machine + 0.015*lifted'),\n",
      " (7,\n",
      "  u'0.056*optimization + 0.037*data + 0.033*learning + 0.029*models + 0.027*adaptive + 0.024*decision + 0.019*bounds + 0.019*sequential + 0.015*recurrent + 0.015*process'),\n",
      " (8,\n",
      "  u'0.167*learning + 0.027*structured + 0.017*efficient + 0.017*prediction + 0.017*deep + 0.017*memory + 0.017*active + 0.017*minimization + 0.016*models + 0.014*online'),\n",
      " (9,\n",
      "  u'0.113*networks + 0.058*neural + 0.045*learning + 0.043*convolutional + 0.036*deep + 0.021*recurrent + 0.016*fast + 0.015*semisupervised + 0.015*sequence + 0.015*object')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PRODUCE LDA MODELS FOR TOPIC NUMBERS FROM 2 TO 10\n",
    "for N in range(2,11):\n",
    "    print N, 'topics:'\n",
    "    lda = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=N,passes=10,alpha='auto')\n",
    "    pprint(lda.print_topics(N))\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
